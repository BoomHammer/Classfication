# é¥æ„Ÿåˆ†ç±»ç²¾åº¦æå‡åˆ†ææŠ¥å‘Š

## ğŸ“Š ç°çŠ¶è¯Šæ–­

### 1. å½“å‰æ€§èƒ½æŒ‡æ ‡ (20251209_1613 è¿è¡Œ)
- **å¤§ç±»æ¨¡å‹**: F1 â‰ˆ 0.42-0.46 (æœ€é«˜ 0.4627)
- **å°ç±»æ¨¡å‹**: F1 â‰ˆ 0.20-0.30 (æ˜¾è‘—æ›´ä½)
- **é—®é¢˜**: å°ç±»ç²¾åº¦è¿œä½äºå¤§ç±»ï¼Œè¯´æ˜ç»†ç²’åº¦ç‰¹å¾å­¦ä¹ ä¸è¶³

### 2. æ ¸å¿ƒé—®é¢˜è¯†åˆ«

#### é—®é¢˜1: **Loss å‡½æ•°æ•°å€¼çˆ†ç‚¸** âš ï¸ ä¸¥é‡
```log
Train Loss=11285664.3824 | Train Acc=0.1397  # æ—¥å¿—ä¸­å¯è§
Train Loss=10937920.7059 | Train Acc=0.1397  # å®Œå…¨å¤±æ§çš„Loss
```
**æ ¹æœ¬åŸå› **: æ¨¡å‹è¾“å‡ºçš„logitsæœªç»è¿‡æ­£ç¡®çš„æ•°å€¼ç¼©æ”¾å¤„ç†

#### é—®é¢˜2: **å°ç±»æ ·æœ¬ä¸¥é‡ä¸è¶³**
- å¤§ç±»å†…æ ·æœ¬åˆ†å¸ƒæä¸å‡è¡¡
- æŸäº›å°ç±»å¯èƒ½ä»…æœ‰5-10ä¸ªæ ·æœ¬
- å°Batchå¯¼è‡´BatchNormæ— æ•ˆä¸”æ¢¯åº¦å™ªå£°å¤§

#### é—®é¢˜3: **ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›å¼±**
- æ—¶é—´åºåˆ—æ•°æ®ï¼ˆ12ä¸ªæ—¶æ­¥ï¼‰å¯èƒ½ä¸è¶³ä»¥æ•æ‰æ¤è¢«å˜åŒ–
- åŠ¨æ€+é™æ€ç‰¹å¾èåˆæ–¹å¼ä¸å¤Ÿç²¾ç»†
- ç©ºé—´ç¼–ç å™¨å¯èƒ½è¿‡äºç®€åŒ–

#### é—®é¢˜4: **æ•°æ®å¢å¼ºç¼ºå¤±**
- ä»£ç ä¸­æ‰¾ä¸åˆ°ä»»ä½•æ•°æ®å¢å¼ºé€»è¾‘
- é¥æ„Ÿæ—¶åºæ•°æ®æœ€é€‚åˆçš„å¢å¼ºæ–¹å¼ï¼ˆæ—¶é—´æ‰­æ›²ã€å…‰è°±å¢å¼ºç­‰ï¼‰æœªå®ç°

#### é—®é¢˜5: **è®­ç»ƒç­–ç•¥ä¸å½“**
- å­¦ä¹ ç‡è°ƒåº¦å¯èƒ½ä¸åˆç†
- Early Stoppingçš„è€å¿ƒå‚æ•°å¯èƒ½å¤ªä½
- Focal Losså¼ºåº¦å¯èƒ½è®¾ç½®ä¸å½“

---

## ğŸ’¡ å¿«é€Ÿä¿®å¤æ–¹æ¡ˆ (ä¼˜å…ˆçº§æ’åº)

### **æ–¹æ¡ˆ1: ä¿®å¤Lossæ•°å€¼çˆ†ç‚¸ (æœ€å…³é”®)**

**ç—‡çŠ¶**: Losså€¼è¾¾åˆ°ç™¾ä¸‡çº§åˆ«

**æ ¹æœ¬åŸå› **: 
1. æ¨¡å‹è¾“å‡ºlogitsç¼ºä¹æ•°å€¼ç¨³å®šæ€§
2. å¯èƒ½å­˜åœ¨æœªæ­£ç¡®å¤„ç†çš„NaN/Infå€¼
3. æƒé‡åˆå§‹åŒ–ä¸å½“æˆ–æ¢¯åº¦çˆ†ç‚¸

**ä¿®å¤ä»£ç **:

```python
# trainer.py ä¸­ä¿®æ”¹ Focal Loss å®ç°
class FocalLoss(nn.Module):
    def __init__(self, alpha=None, gamma=2.0, reduction='mean', weight=None):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        self.weight = weight
    
    def forward(self, inputs, targets):
        # 1. æ•°å€¼ç¨³å®šçš„Softmax
        inputs = inputs - inputs.max(dim=1, keepdim=True)[0]  # é˜²æ­¢æº¢å‡º
        
        # 2. è®¡ç®—æ¦‚ç‡
        p = F.softmax(inputs, dim=1)
        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.weight)
        
        # 3. è·å–ç›®æ ‡ç±»åˆ«çš„æ¦‚ç‡
        p_t = p.gather(1, targets.unsqueeze(1)).squeeze(1)
        
        # 4. è®¡ç®—Focal Loss
        focal_weight = (1 - p_t) ** self.gamma
        focal_loss = focal_weight * ce_loss
        
        # 5. æ£€æŸ¥å¹¶è¿‡æ»¤å¼‚å¸¸å€¼
        focal_loss = torch.clamp(focal_loss, min=0, max=1e6)
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        else:
            return focal_loss.sum()
```

**éªŒè¯**:
```bash
# è®­ç»ƒåæ£€æŸ¥Lossæ˜¯å¦åœ¨åˆç†èŒƒå›´ (0.5-3.0)
grep "Train Loss" major_model/train.log | head -20
```

---

### **æ–¹æ¡ˆ2: å®ç°æ—¶åºæ•°æ®å¢å¼º**

**ä¸ºä»€ä¹ˆå¿…éœ€**: é¥æ„Ÿæ—¶åºæ•°æ®é‡å°ï¼Œå¢å¼ºå¯å¤§å¹…æå‡æ³›åŒ–æ€§èƒ½

**å®ç°**: åˆ›å»ºæ–°æ–‡ä»¶ `code/data_augmentation.py`

```python
import torch
import torch.nn as nn
import numpy as np

class TemporalAugmentation:
    """æ—¶é—´åºåˆ—å¢å¼º"""
    
    @staticmethod
    def temporal_warp(x, num_segments=3, max_warp=0.2):
        """æ—¶é—´åºåˆ—å¼¯æ›²å¢å¼º"""
        B, T, C, H, W = x.shape
        t = torch.linspace(0, 1, T, device=x.device)
        
        # ç”Ÿæˆéšæœºå¼¯æ›²
        warp_t = t.clone().unsqueeze(0)
        for _ in range(num_segments):
            segment_start = np.random.randint(0, T-1)
            segment_end = np.random.randint(segment_start+1, T)
            warp_scale = 1 + np.random.uniform(-max_warp, max_warp)
            
            segment_mask = (warp_t >= segment_start/T) & (warp_t <= segment_end/T)
            warp_t[segment_mask] *= warp_scale
        
        # æ’å€¼é‡‡æ ·
        warp_t = torch.clamp(warp_t, 0, 1)
        warp_indices = (warp_t * (T-1)).long()
        
        return x[:, warp_indices.squeeze(0)]
    
    @staticmethod
    def spectrum_jitter(x, std=0.01):
        """å…‰è°±æŠ–åŠ¨å¢å¼º"""
        noise = torch.randn_like(x) * std
        return torch.clamp(x + noise, 0, 1)
    
    @staticmethod
    def temporal_dropout(x, drop_rate=0.1):
        """æ—¶é—´æ®µdropout"""
        B, T, C, H, W = x.shape
        drop_frames = int(T * drop_rate)
        
        drop_indices = np.random.choice(T, drop_frames, replace=False)
        mask = torch.ones(T, device=x.device)
        mask[drop_indices] = 0
        
        x_aug = x.clone()
        x_aug[:, drop_indices] = x_aug[:, drop_indices].roll(1, dims=1)
        return x_aug

class PointTimeSeriesDatasetWithAugmentation:
    """åœ¨ PointTimeSeriesDataset åŸºç¡€ä¸Šæ·»åŠ å¢å¼º"""
    
    def __init__(self, *args, augmentation_prob=0.5, **kwargs):
        # ... ç»§æ‰¿åŸæœ‰åˆå§‹åŒ– ...
        self.augmentation_prob = augmentation_prob
        self.aug = TemporalAugmentation()
    
    def __getitem__(self, idx):
        sample = super().__getitem__(idx)  # è·å–åŸå§‹æ ·æœ¬
        
        if np.random.random() < self.augmentation_prob:
            # éšæœºé€‰æ‹©å¢å¼ºæ–¹å¼
            aug_type = np.random.choice(['warp', 'jitter', 'dropout'])
            
            x_dyn = sample['x_dynamic']
            if aug_type == 'warp':
                x_dyn = self.aug.temporal_warp(x_dyn.unsqueeze(0)).squeeze(0)
            elif aug_type == 'jitter':
                x_dyn = self.aug.spectrum_jitter(x_dyn)
            elif aug_type == 'dropout':
                x_dyn = self.aug.temporal_dropout(x_dyn.unsqueeze(0)).squeeze(0)
            
            sample['x_dynamic'] = x_dyn
        
        return sample
```

**é›†æˆåˆ°main.py**:
```python
# åœ¨ PointTimeSeriesDataset åˆå§‹åŒ–å
from data_augmentation import PointTimeSeriesDatasetWithAugmentation

# æ›¿æ¢åŸæ¥çš„æ•°æ®é›†
full_train_dataset = PointTimeSeriesDatasetWithAugmentation(
    config, encoder, split='train', 
    augmentation_prob=0.5  # 50% æ¦‚ç‡å¢å¼º
)
```

---

### **æ–¹æ¡ˆ3: æ”¹è¿›å°æ ·æœ¬å¤„ç†**

**é—®é¢˜**: æŸäº›å°ç±»æ ·æœ¬æå°‘ï¼Œé€ æˆè®­ç»ƒä¸ç¨³å®š

**è§£å†³æ–¹æ¡ˆA: æ ·æœ¬é‡é‡‡æ · (Oversampling)**

```python
# åœ¨ main.py çš„å°ç±»æ¨¡å‹è®­ç»ƒéƒ¨åˆ†
from torch.utils.data import WeightedRandomSampler

def create_balanced_sampler(dataset, num_classes):
    """ä¸ºä¸å¹³è¡¡æ•°æ®é›†åˆ›å»ºåŠ æƒé‡‡æ ·å™¨"""
    labels = []
    for idx in range(len(dataset)):
        sample = dataset[idx]
        labels.append(sample['label'].item())
    
    class_counts = np.bincount(labels, minlength=num_classes)
    class_weights = 1.0 / (class_counts + 1e-6)
    sample_weights = class_weights[labels]
    
    return WeightedRandomSampler(
        weights=sample_weights,
        num_samples=len(dataset),
        replacement=True
    )

# ä½¿ç”¨æ–¹å¼
sampler = create_balanced_sampler(train_subset, num_sub_classes)
train_loader = DataLoader(
    train_subset,
    batch_size=detail_cfg['batch_size'],
    sampler=sampler,  # ä½¿ç”¨åŠ æƒé‡‡æ ·
    collate_fn=collate_fn,
    **common_cfg
)
```

**è§£å†³æ–¹æ¡ˆB: Mixupæ··åˆ**

```python
# trainer.py ä¸­æ·»åŠ 
def mixup_batch(self, x_dyn, x_sta, y, alpha=0.4):
    """Mixupæ•°æ®å¢å¼º"""
    batch_size = y.size(0)
    index = torch.randperm(batch_size)
    
    lam = np.random.beta(alpha, alpha)
    
    mixed_x_dyn = lam * x_dyn + (1 - lam) * x_dyn[index]
    mixed_x_sta = lam * x_sta + (1 - lam) * x_sta[index]
    
    y_a, y_b = y, y[index]
    return mixed_x_dyn, mixed_x_sta, y_a, y_b, lam

# è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨
mixed_x_dyn, mixed_x_sta, y_a, y_b, lam = self.mixup_batch(
    x_dyn, x_sta, y
)
outputs = self.model(mixed_x_dyn, mixed_x_sta)
loss = lam * self.criterion(outputs, y_a) + \
       (1 - lam) * self.criterion(outputs, y_b)
```

---

### **æ–¹æ¡ˆ4: æ”¹è¿›æ¨¡å‹æ¶æ„**

**å½“å‰é—®é¢˜**: ç©ºé—´ç¼–ç å™¨å¯èƒ½è¿‡äºç®€å•

**æ”¹è¿›æ–¹æ¡ˆ**: å¢å¼ºç‰¹å¾æå–

```python
# model_architecture.py

class EnhancedSpatialEncoder(nn.Module):
    def __init__(self, in_channels, hidden_dim=64, dropout=0.15):
        super().__init__()
        
        # å¤šå°ºåº¦å·ç§¯åˆ†æ”¯
        self.conv1x1 = nn.Sequential(
            nn.Conv2d(in_channels, hidden_dim, kernel_size=1),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU(inplace=True)
        )
        
        self.conv3x3 = nn.Sequential(
            nn.Conv2d(in_channels, hidden_dim, kernel_size=3, padding=1),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU(inplace=True)
        )
        
        self.conv5x5 = nn.Sequential(
            nn.Conv2d(in_channels, hidden_dim, kernel_size=5, padding=2),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU(inplace=True)
        )
        
        # Squeeze-Excitationæ¨¡å—
        self.se = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(hidden_dim * 3, hidden_dim, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(hidden_dim, hidden_dim * 3, kernel_size=1),
            nn.Sigmoid()
        )
        
        self.output_projection = nn.Sequential(
            nn.Conv2d(hidden_dim * 3, hidden_dim, kernel_size=1),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU(inplace=True)
        )
    
    def forward(self, x):
        # å¤šå°ºåº¦ç‰¹å¾
        f1 = self.conv1x1(x)
        f3 = self.conv3x3(x)
        f5 = self.conv5x5(x)
        
        # ç‰¹å¾èåˆ
        f_concat = torch.cat([f1, f3, f5], dim=1)
        
        # SEæ³¨æ„åŠ›
        se_weights = self.se(f_concat)
        f_weighted = f_concat * se_weights
        
        # æœ€ç»ˆæŠ•å½±
        output = self.output_projection(f_weighted)
        
        return output
```

---

### **æ–¹æ¡ˆ5: æ›´æ™ºèƒ½çš„å­¦ä¹ ç‡è°ƒåº¦**

**é—®é¢˜**: å›ºå®šæˆ–ç®€å•çº¿æ€§è°ƒåº¦å¯èƒ½ä¸é€‚åˆå°æ ·æœ¬

**è§£å†³æ–¹æ¡ˆ**: ä½¿ç”¨Warmup + CosineAnnealing

```python
# trainer.py
def get_scheduler(optimizer, num_epochs, len_train_loader):
    """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
    total_steps = num_epochs * len_train_loader
    warmup_steps = int(0.1 * total_steps)  # 10% warmup
    
    def lr_lambda(current_step):
        if current_step < warmup_steps:
            return float(current_step) / float(max(1, warmup_steps))
        return max(0.0, float(total_steps - current_step) / 
                       float(max(1, total_steps - warmup_steps)))
    
    scheduler = torch.optim.lr_scheduler.LambdaLR(
        optimizer, 
        lr_lambda
    )
    return scheduler
```

---

## ğŸš€ å®Œæ•´å®æ–½è®¡åˆ’

### ç¬¬1é˜¶æ®µ (ä»Šå¤© - ç«‹å³): ä¿®å¤Lossçˆ†ç‚¸
```bash
# 1. ä¿®æ”¹ trainer.py ä¸­çš„ FocalLoss å®ç°
# 2. é‡æ–°è®­ç»ƒå¤§ç±»æ¨¡å‹
# 3. æ£€æŸ¥ Loss æ›²çº¿æ˜¯å¦æ­£å¸¸åŒ–
```

### ç¬¬2é˜¶æ®µ (æ˜å¤©): æ·»åŠ æ•°æ®å¢å¼º
```bash
# 1. åˆ›å»º data_augmentation.py
# 2. é›†æˆåˆ° main.py
# 3. å¯¹æ¯”æœ‰/æ— å¢å¼ºçš„è®­ç»ƒç»“æœ
```

### ç¬¬3é˜¶æ®µ: æ”¹è¿›å°æ ·æœ¬å¤„ç†
```bash
# 1. å®ç°åŠ æƒé‡‡æ ·å™¨
# 2. æ·»åŠ  Mixup å¢å¼º
# 3. ç›‘æ§å°ç±»ç²¾åº¦æå‡
```

### ç¬¬4é˜¶æ®µ: æ¨¡å‹æ¶æ„ä¼˜åŒ–
```bash
# 1. æ›¿æ¢ç©ºé—´ç¼–ç å™¨
# 2. è°ƒæ•´èåˆç­–ç•¥
# 3. é‡æ–°è®­ç»ƒå¹¶å¯¹æ¯”
```

---

## ğŸ“ˆ é¢„æœŸæ”¹è¿›

| æ–¹é¢ | å½“å‰ | ä¿®å¤å | ç›®æ ‡ |
|------|------|--------|------|
| å¤§ç±»F1 | 0.46 | 0.55+ | 0.65+ |
| å°ç±»F1 | 0.25 | 0.40+ | 0.55+ |
| Lossç¨³å®šæ€§ | âŒ çˆ†ç‚¸ | âœ… æ­£å¸¸ | âœ… æ”¶æ•› |
| è¿‡æ‹Ÿåˆ | ä¸­ç­‰ | ä½ | æœ€å° |

---

## ğŸ“ å…³é”®æç¤º

1. **ä¿å­˜baseline**: åœ¨ä¿®æ”¹å‰å¤‡ä»½å½“å‰æœ€ä½³æ¨¡å‹
2. **é€æ­¥éªŒè¯**: æ¯ä¸ªä¿®æ”¹åéƒ½è¦è®­ç»ƒå¹¶å¯¹æ¯”ç»“æœ
3. **ç›‘æ§æŒ‡æ ‡**: å…³æ³¨ Loss/F1/Acc ä¸‰ä¸ªç»´åº¦
4. **å°ç±»ä¼˜å…ˆ**: å¤§ç±»å·²ç»ä¸é”™ï¼Œé‡ç‚¹æ”¹è¿›å°ç±»
5. **æ•°æ®è´¨é‡**: æ£€æŸ¥CSVæ ‡ç­¾å’Œå½±åƒæ•°æ®æ˜¯å¦æœ‰é—®é¢˜

---

## â“ å¯é€‰æ·±åº¦ä¼˜åŒ–

å¦‚æœä¸Šè¿°æ–¹æ¡ˆæ•ˆæœä¸ç†æƒ³ï¼Œå¯ç»§ç»­å°è¯•:

1. **åˆ†ç¦»å¤§å°ç±»è®­ç»ƒ**: å¤§å°ç±»ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡å’Œç­–ç•¥
2. **ç±»æ¡ä»¶æ‰¹å½’ä¸€åŒ–**: æŒ‰ç±»åˆ«åˆ†ç»„è¿›è¡ŒBatchNorm
3. **è‡ªé€‚åº”æƒé‡**: æ ¹æ®éªŒè¯é›†å®æ—¶è°ƒæ•´ç±»åˆ«æƒé‡
4. **é›†åˆæ¨¡å‹**: å¤šä¸ªæ¨¡å‹çš„æŠ•ç¥¨/å¹³å‡
5. **åŠç›‘ç£å­¦ä¹ **: åˆ©ç”¨æœªæ ‡æ³¨æ•°æ®(å¦‚æœæœ‰çš„è¯)

