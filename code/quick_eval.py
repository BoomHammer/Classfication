#!/usr/bin/env python3
"""
quick_eval.py: åˆ†å±‚åˆ†ç±»æ¨¡å‹éªŒè¯è„šæœ¬

ä½¿ç”¨æ–¹å¼ï¼š
1. ç¡®ä¿å·²å®‰è£…æ‰€éœ€çš„ Python åŒ…ã€‚
2. åœ¨ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
   ```
   python code/quick_eval.py --run_dir experiments/outputs/XXXXXXXX_XXXX_EXP_2023_001
   ```ã€‚
"""

import torch
import json
import sys
import logging
import argparse
import numpy as np
import pandas as pd
from pathlib import Path
from tqdm import tqdm
from torch.utils.data import DataLoader, Subset
from sklearn.metrics import classification_report, accuracy_score

# å¯¼å…¥æœ¬åœ°æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent))
from config_manager import ConfigManager
from label_encoder import LabelEncoder
from point_timeseries_dataset import PointTimeSeriesDataset, collate_fn
from model_architecture import DualStreamSpatio_TemporalFusionNetwork

def load_model_weights(model, path, device):
    """å®‰å…¨åŠ è½½æ¨¡å‹æƒé‡"""
    try:
        # print(f"   â³ åŠ è½½æƒé‡: {path.name} ...")
        checkpoint = torch.load(path, map_location=device, weights_only=False)
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        return True
    except Exception as e:
        print(f"   âŒ æƒé‡åŠ è½½å¤±è´¥: {e}")
        return False

def predict_subset(model, dataset, indices, device, batch_size):
    """è¾…åŠ©å‡½æ•°ï¼šå¯¹æŒ‡å®šç´¢å¼•çš„å­é›†è¿›è¡Œé¢„æµ‹ï¼Œè¿”å›å±€éƒ¨é¢„æµ‹ç»“æœ"""
    if len(indices) == 0:
        return []
    
    subset = Subset(dataset, indices)
    dataloader = DataLoader(subset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
    
    local_preds = []
    model.eval()
    with torch.no_grad():
        for batch in dataloader:
            dyn = batch['dynamic'].to(device)
            sta = batch['static'].to(device)
            outputs = model(dyn, sta)
            preds = torch.argmax(outputs['probabilities'], dim=1)
            local_preds.extend(preds.cpu().numpy())
            
    return local_preds

def predict_subset_ensemble(models_list, dataset, indices, device, batch_size, method='voting'):
    """
    é›†åˆé¢„æµ‹ï¼šä½¿ç”¨å¤šä¸ªæ¨¡å‹è¿›è¡ŒæŠ•ç¥¨æˆ–æ¦‚ç‡å¹³å‡
    
    Args:
        models_list: æ¨¡å‹åˆ—è¡¨ (ä¾‹å¦‚ 5 ä¸ª K-Fold æ¨¡å‹)
        method: 'voting' å¤šæ•°æŠ•ç¥¨ï¼Œ'averaging' æ¦‚ç‡å¹³å‡
    
    Returns:
        ensemble_preds: é›†åˆé¢„æµ‹ç»“æœ (N,)
    """
    if len(indices) == 0:
        return []
    
    subset = Subset(dataset, indices)
    dataloader = DataLoader(subset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
    
    ensemble_preds = []
    
    for batch in dataloader:
        dyn = batch['dynamic'].to(device)
        sta = batch['static'].to(device)
        batch_size_actual = dyn.size(0)
        
        if method == 'voting':
            # å¤šæ•°æŠ•ç¥¨ï¼šæ¯ä¸ªæ¨¡å‹æŠ•ä¸€ç¥¨ï¼Œé€‰ä¸¾å‡ºç°æ¬¡æ•°æœ€å¤šçš„ç±»åˆ«
            all_preds = []
            for model in models_list:
                model.eval()
                with torch.no_grad():
                    outputs = model(dyn, sta)
                    preds = torch.argmax(outputs['probabilities'], dim=1)
                    all_preds.append(preds.cpu().numpy())
            
            all_preds = np.array(all_preds)  # (num_models, batch_size)
            # å¯¹æ¯ä¸ªæ ·æœ¬ï¼Œç»Ÿè®¡å„ç±»åˆ«çš„æŠ•ç¥¨æ•°ï¼Œé€‰æ‹©æŠ•ç¥¨æœ€å¤šçš„ç±»åˆ«
            ensemble_batch = []
            for i in range(batch_size_actual):
                votes = all_preds[:, i]
                # å¤šæ•°æŠ•ç¥¨ï¼šè¿”å›å‡ºç°æœ€å¤šçš„ç±»åˆ«
                vote_result = np.bincount(votes.astype(int))
                pred_class = np.argmax(vote_result)
                ensemble_batch.append(pred_class)
            ensemble_preds.extend(ensemble_batch)
            
        elif method == 'averaging':
            # æ¦‚ç‡å¹³å‡ï¼šæ‰€æœ‰æ¨¡å‹çš„æ¦‚ç‡å–å¹³å‡ï¼Œç„¶åé€‰æ‹©æœ€é«˜æ¦‚ç‡çš„ç±»åˆ«
            all_probs = []
            for model in models_list:
                model.eval()
                with torch.no_grad():
                    outputs = model(dyn, sta)
                    probs = torch.softmax(outputs['probabilities'], dim=1)
                    all_probs.append(probs.cpu().numpy())
            
            all_probs = np.array(all_probs)  # (num_models, batch_size, num_classes)
            avg_probs = np.mean(all_probs, axis=0)  # (batch_size, num_classes)
            preds = np.argmax(avg_probs, axis=1)
            ensemble_preds.extend(preds)
    
    return ensemble_preds

def main():
    parser = argparse.ArgumentParser(description='åˆ†å±‚æ¨¡å‹å¿«é€Ÿè¯„ä¼°')
    parser.add_argument('--config', type=str, default='config.yaml')
    parser.add_argument('--run_dir', type=str, help='æŒ‡å®šå®éªŒè¾“å‡ºç›®å½•')
    parser.add_argument('--split', type=str, default='val', help='è¯„ä¼°æ•°æ®é›†: val æˆ– test')
    parser.add_argument('--batch_size', type=int, default=32)
    args = parser.parse_args()

    print("="*60)
    print("ğŸš€ å¯åŠ¨å…¨é“¾è·¯è¯„ä¼°è„šæœ¬")
    print("="*60)
    
    # 1. åˆå§‹åŒ–é…ç½®ä¸è·¯å¾„
    config_path = Path(__file__).parent / args.config
    # [ä¿®å¤] è¯„ä¼°æ¨¡å¼ä¸‹ä¸åˆ›å»ºæ–°çš„å®éªŒæ–‡ä»¶å¤¹
    config = ConfigManager(str(config_path), create_experiment_dir=False)
    
    if args.run_dir:
        output_dir = Path(args.run_dir)
        if not output_dir.exists():
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {output_dir}")
            sys.exit(1)
        print(f"ğŸ“‚ å®éªŒç›®å½•: {output_dir}")
    else:
        # å¦‚æœæœªæŒ‡å®š run_dirï¼Œä½¿ç”¨é»˜è®¤åŸºç¡€ç›®å½•æˆ–ä¸Šæ¬¡è¿è¡Œç›®å½•
        # æ³¨æ„ï¼šç”±äº create_experiment_dir=Falseï¼Œè¿™é‡Œå¾—åˆ°çš„æ˜¯åŸºç¡€ç›®å½•
        output_dir = config.get_experiment_output_dir()
        print(f"âš ï¸ æœªæŒ‡å®š --run_dirï¼Œå°†åœ¨åŸºç¡€ç›®å½•å¯»æ‰¾èµ„æº: {output_dir}")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"âš™ï¸  é…ç½®: Split={args.split}, Device={device}")
    
    # 2. ç¡®å®šé€šé“æ•°
    param_file = output_dir / 'detected_parameters.json'
    if param_file.exists():
        with open(param_file, 'r') as f:
            params = json.load(f)
        dyn_ch = params['dynamic_channels']
        sta_ch = params['static_channels']
    else:
        print("âš ï¸ è‡ªåŠ¨æ¨æ–­é€šé“æ•°...")
        temp_ds = PointTimeSeriesDataset(config, None, split='val', verbose=False)
        dyn_ch = temp_ds.num_channels
        sta_ch = temp_ds.num_static_channels
    print(f"ğŸ“Š é€šé“: Dynamic={dyn_ch}, Static={sta_ch}")

    # 3. åŠ è½½æ˜ å°„
    major_map_file = output_dir / 'major_labels_map.json'
    detailed_map_file = output_dir / 'detailed_labels_map.json'
    
    if not major_map_file.exists():
        print(f"âŒ ç¼ºå°‘æ˜ å°„æ–‡ä»¶ (major_labels_map.json)ï¼Œè¯·æ£€æŸ¥ç›®å½•: {output_dir}")
        print("ğŸ’¡ æç¤ºï¼šè¯„ä¼°è„šæœ¬éœ€è¦æŒ‡å®šåŒ…å«è®­ç»ƒç»“æœçš„æ–‡ä»¶å¤¹ï¼Œè¯·ä½¿ç”¨ --run_dir å‚æ•°ã€‚")
        sys.exit(1)
        
    with open(major_map_file, 'r', encoding='utf-8') as f:
        major_map = json.load(f)
    with open(detailed_map_file, 'r', encoding='utf-8') as f:
        detailed_map = json.load(f)
    
    inverse_detailed_map = {v: k for k, v in detailed_map.items()}
    encoder = LabelEncoder(config=config, output_dir=output_dir)
    
    # 4. åŠ è½½æ•°æ®é›†
    print(f"\nğŸ“¦ åŠ è½½ {args.split} æ•°æ®é›†...")
    dataset = PointTimeSeriesDataset(config, encoder, split=args.split, verbose=True)
    if len(dataset) == 0:
        print("âŒ æ•°æ®é›†ä¸ºç©º")
        sys.exit(1)
        
    # è·å–ç”¨äºç´¢å¼•çš„ DataFrame
    eval_df = dataset.points_df.iloc[dataset.indices].reset_index(drop=True)
    num_samples = len(dataset)
    
    # åˆå§‹åŒ–ç»“æœæ•°ç»„
    true_major_array = np.array(eval_df['major_label'])
    true_detail_array = np.array(eval_df['detail_label'])
    
    pred_major_array = np.full(num_samples, -1)
    
    # [å…³é”®] ä¸¤ä¸ªå°ç±»é¢„æµ‹æ•°ç»„
    # 1. Upper Bound: å‡è®¾å¤§ç±»å·²çŸ¥ï¼Œé€å…¥æ­£ç¡®çš„å°ç±»æ¨¡å‹ (åæ˜ å°ç±»æ¨¡å‹æœ¬èº«èƒ½åŠ›)
    pred_detail_upper = np.full(num_samples, -1) 
    # 2. Pipeline: ä¾æ®é¢„æµ‹çš„å¤§ç±»ï¼Œé€å…¥å¯¹åº”çš„å°ç±»æ¨¡å‹ (åæ˜ çœŸå®ç³»ç»Ÿèƒ½åŠ›)
    pred_detail_pipeline = np.full(num_samples, -1)

    # =========================================================================
    # é˜¶æ®µ A: è¯„ä¼°å¤§ç±»æ¨¡å‹
    # =========================================================================
    print("\n" + "-"*50)
    print("ğŸ—ï¸  Step 1: å¤§ç±»é¢„æµ‹ (Major Prediction - Ensemble)")
    print("-"*50)
    
    major_model_dir = output_dir / 'major_model'
    fold_models = []
    
    # åŠ è½½ 5 ä¸ª K-Fold æ¨¡å‹
    classifier_hidden_dims = config.get('model.classifier.hidden_dims', [128, 64, 32])
    for fold_idx in range(1, 6):
        fold_path = major_model_dir / f'fold_{fold_idx}' / 'best_model.pth'
        if fold_path.exists():
            major_model = DualStreamSpatio_TemporalFusionNetwork(
                in_channels_dynamic=dyn_ch, in_channels_static=sta_ch, num_classes=len(major_map),
                classifier_hidden_dims=classifier_hidden_dims
            ).to(device)
            
            if load_model_weights(major_model, fold_path, device):
                fold_models.append(major_model)
                print(f"   âœ… åŠ è½½å¤§ç±»æ¨¡å‹ fold_{fold_idx}")
    
    if len(fold_models) > 0:
        # å¯¹æ‰€æœ‰æ•°æ®è¿›è¡Œå¤§ç±»é›†åˆé¢„æµ‹
        all_indices = list(range(num_samples))
        preds = predict_subset_ensemble(fold_models, dataset, all_indices, device, args.batch_size, method='voting')
        pred_major_array = np.array(preds)
        
        # è¾“å‡ºæŠ¥å‘Š
        print(f"\nğŸ“Š å¤§ç±»é›†åˆé¢„æµ‹ (ä½¿ç”¨ {len(fold_models)} ä¸ªæ¨¡å‹æŠ•ç¥¨):")
        print("ğŸ“‹ å¤§ç±»åˆ†ç±»æŠ¥å‘Š:")
        major_names = [k for k, v in sorted(major_map.items(), key=lambda x: x[1])]
        print(classification_report(true_major_array, pred_major_array, target_names=major_names, digits=4, zero_division=0))
    else:
        print(f"âŒ å¤§ç±»æ¨¡å‹ç¼ºå¤±: {major_model_dir / 'fold_1' / 'best_model.pth'}")
        print("   ğŸ’¡ è¯·ç¡®ä¿å·²ä½¿ç”¨ K-Fold è®­ç»ƒè¿‡å¤§ç±»æ¨¡å‹")

    # =========================================================================
    # é˜¶æ®µ B: è¯„ä¼°å°ç±»æ¨¡å‹ (åŒè·¯å¾„)
    # =========================================================================
    print("\n" + "-"*50)
    print("ğŸ—ï¸  Step 2: å°ç±»é¢„æµ‹ (Detail Prediction - Ensemble)")
    print("-"*50)

    # éå†æ¯ä¸€ä¸ªå¤§ç±» ID
    for major_name, major_id in major_map.items():
        sub_model_dir = output_dir / f"detail_model_{major_id}_{major_name}"
        mapping_path = sub_model_dir / "class_mapping.json"
        
        # å¦‚æœè¯¥å¤§ç±»æ²¡æœ‰è®­ç»ƒå¥½çš„å°ç±»æ¨¡å‹
        if not (sub_model_dir / 'fold_1' / 'best_model.pth').exists():
            continue 

        # åŠ è½½å±€éƒ¨æ˜ å°„
        try:
            with open(mapping_path, 'r', encoding='utf-8') as f:
                mapping_data = json.load(f)
            local_to_global = {int(k): int(v) for k, v in mapping_data['local_to_global_map'].items()}
        except:
            continue
            
        num_sub_classes = len(local_to_global)
        
        # åŠ è½½ 5 ä¸ª K-Fold å°ç±»æ¨¡å‹
        fold_models = []
        for fold_idx in range(1, 6):
            fold_path = sub_model_dir / f'fold_{fold_idx}' / 'best_model.pth'
            if fold_path.exists():
                sub_model = DualStreamSpatio_TemporalFusionNetwork(
                    in_channels_dynamic=dyn_ch, in_channels_static=sta_ch, num_classes=num_sub_classes,
                    classifier_hidden_dims=classifier_hidden_dims
                ).to(device)
                
                if load_model_weights(sub_model, fold_path, device):
                    fold_models.append(sub_model)
        
        if len(fold_models) == 0:
            continue
        
        # --- è·¯å¾„ 1: Upper Bound (åŸºäºçœŸå®æ ‡ç­¾) ---
        true_indices = np.where(true_major_array == major_id)[0]
        if len(true_indices) > 0:
            local_preds = predict_subset_ensemble(fold_models, dataset, true_indices, device, args.batch_size, method='voting')
            global_preds = [local_to_global[p] for p in local_preds]
            pred_detail_upper[true_indices] = global_preds
            
        # --- è·¯å¾„ 2: Pipeline (åŸºäºå¤§ç±»é¢„æµ‹) ---
        # æ‰¾å‡ºå¤§ç±»æ¨¡å‹é¢„æµ‹ä¸ºå½“å‰ major_id çš„æ‰€æœ‰æ ·æœ¬ (å¯èƒ½åŒ…å«è¯¯åˆ¤è¿›æ¥çš„)
        pred_indices = np.where(pred_major_array == major_id)[0]
        if len(pred_indices) > 0:
            local_preds = predict_subset_ensemble(fold_models, dataset, pred_indices, device, args.batch_size, method='voting')
            global_preds = [local_to_global[p] for p in local_preds]
            pred_detail_pipeline[pred_indices] = global_preds
            
        print(f"ğŸ‘‰ æ¨¡å‹ [{major_name}]: ä½¿ç”¨ {len(fold_models)} ä¸ªæ¨¡å‹æŠ•ç¥¨ | å¤„ç†çœŸå®æ ·æœ¬ {len(true_indices)} ä¸ª, å¤„ç†é¢„æµ‹æ ·æœ¬ {len(pred_indices)} ä¸ª")

    # =========================================================================
    # é˜¶æ®µ C: ç”ŸæˆæŠ¥å‘Š
    # =========================================================================
    print("\n" + "="*60)
    print("ğŸ“Š æœ€ç»ˆè¯„ä¼°æŠ¥å‘Š")
    print("="*60)
    
    # 1. Upper Bound æŠ¥å‘Š
    valid_mask_upper = pred_detail_upper != -1
    if np.sum(valid_mask_upper) > 0:
        y_true = true_detail_array[valid_mask_upper]
        y_pred = pred_detail_upper[valid_mask_upper]
        unique_labels = sorted(list(set(y_true) | set(y_pred)))
        names = [inverse_detailed_map.get(i, str(i)) for i in unique_labels]
        
        print("\nâœ… å°ç±»åˆ†ç±»æŠ¥å‘Š (Upper Bound - å‡è®¾å¤§ç±»æ­£ç¡®):")
        print("   (ä»…åŒ…å«å·²è®­ç»ƒå°ç±»æ¨¡å‹çš„ç±»åˆ«)")
        print(classification_report(y_true, y_pred, target_names=names, digits=4, zero_division=0))
    
    # 2. Pipeline æŠ¥å‘Š
    valid_mask_pipe = pred_detail_pipeline != -1
    
    if np.sum(valid_mask_pipe) > 0:
        y_true = true_detail_array[valid_mask_pipe]
        y_pred = pred_detail_pipeline[valid_mask_pipe]
        
        unique_labels = sorted(list(set(y_true) | set(y_pred)))
        names = [inverse_detailed_map.get(i, str(i)) for i in unique_labels]
        
        print("\nğŸš€ æ€»ä½“å„å°ç±»åˆ†ç±»æŠ¥å‘Š (Pipeline - çœŸå®æµæ°´çº¿):")
        print("   (åŒ…å«å¤§ç±»é”™è¯¯å¯¼è‡´çš„ä¼ é€’è¯¯å·®)")
        print(classification_report(y_true, y_pred, target_names=names, digits=4, zero_division=0))
        
        acc = accuracy_score(y_true, y_pred)
        print(f"ğŸ† æ€»ä½“å°ç±»å‡†ç¡®ç‡ (Pipeline Accuracy): {acc:.2%}")
    else:
        print("\nâŒ æ— æ³•ç”Ÿæˆæµæ°´çº¿æŠ¥å‘Š (å¯èƒ½æ˜¯å¤§ç±»æ¨¡å‹æœªé¢„æµ‹å‡ºä»»ä½•æœ‰æ•ˆç±»åˆ«)")

    # 3. ä¿å­˜è¯¦ç»†ç»“æœ
    id_col = config.get('data_specs.csv_columns.id', 'Index')
    if id_col not in eval_df.columns:
        id_col = 'sample_id_generated'
        eval_df[id_col] = eval_df.index

    results_df = pd.DataFrame({
        'sample_id': eval_df[id_col],
        'true_major': [list(major_map.keys())[list(major_map.values()).index(i)] for i in true_major_array],
        'pred_major': [list(major_map.keys())[list(major_map.values()).index(i)] if i!=-1 else 'N/A' for i in pred_major_array],
        'true_detail': [inverse_detailed_map.get(i, str(i)) for i in true_detail_array],
        'pred_detail_upper': [inverse_detailed_map.get(i, str(i)) if i!=-1 else 'N/A' for i in pred_detail_upper],
        'pred_detail_pipeline': [inverse_detailed_map.get(i, str(i)) if i!=-1 else 'N/A' for i in pred_detail_pipeline]
    })
    
    csv_name = f"eval_full_report_{args.split}.csv"
    save_path = output_dir / csv_name
    results_df.to_csv(save_path, index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ è¯¦ç»†é¢„æµ‹ç»“æœå·²ä¿å­˜: {save_path}")

if __name__ == "__main__":
    main()