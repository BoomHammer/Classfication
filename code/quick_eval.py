#!/usr/bin/env python3
"""
quick_eval.py: åˆ†å±‚åˆ†ç±»æ¨¡å‹éªŒè¯è„šæœ¬ (é€‚é… Segmentation æ¶æ„ + å®Œå–„æŒ‡æ ‡è¾“å‡º)

ä½¿ç”¨æ–¹å¼ï¼š
   python code/quick_eval.py --run_dir experiments/outputs/XXXXXXXX_XXXX_EXP_2023_001
"""

import torch
import json
import sys
import logging
import argparse
import numpy as np
import pandas as pd
from pathlib import Path
from tqdm import tqdm
from torch.utils.data import DataLoader, Subset
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score

# å¯¼å…¥æœ¬åœ°æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent))
from config_manager import ConfigManager
from label_encoder import LabelEncoder
from point_timeseries_dataset import PointTimeSeriesDataset, collate_fn
from model_architecture import DualStreamSpatio_TemporalFusionNetwork

# [é‡è¦] å¿…é¡»ä¸ main.py ä¸­çš„è®¾ç½®ä¿æŒä¸€è‡´
MAX_TEMPORAL_STEPS = 64 

def load_model_weights(model, path, device):
    """å®‰å…¨åŠ è½½æ¨¡å‹æƒé‡"""
    try:
        # ä½¿ç”¨ weights_only=True æ¶ˆé™¤è­¦å‘Š
        checkpoint = torch.load(path, map_location=device, weights_only=True)
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        return True
    except RuntimeError as e:
        print(f"   âŒ æƒé‡åŠ è½½å¤±è´¥ (å°ºå¯¸ä¸åŒ¹é…?): {e}")
        return False
    except Exception as e:
        print(f"   âŒ æƒé‡åŠ è½½å¤±è´¥: {e}")
        return False

def get_center_predictions(outputs_dict):
    """
    ä»åˆ†å‰²æ¨¡å‹çš„è¾“å‡º (B, C, H, W) ä¸­æå–ä¸­å¿ƒåƒç´ çš„é¢„æµ‹
    """
    # (B, C, H, W)
    probs = torch.softmax(outputs_dict['logits'], dim=1)
    B, C, H, W = probs.shape
    
    # å–ä¸­å¿ƒåƒç´ 
    center_h, center_w = H // 2, W // 2
    center_probs = probs[:, :, center_h, center_w] # (B, C)
    
    return center_probs

def predict_subset_ensemble(models_list, dataset, indices, device, batch_size, method='voting'):
    """
    é›†åˆé¢„æµ‹ï¼šä½¿ç”¨å¤šä¸ªæ¨¡å‹è¿›è¡ŒæŠ•ç¥¨æˆ–æ¦‚ç‡å¹³å‡
    é€‚é…ï¼šæå–ä¸­å¿ƒåƒç´ è¿›è¡Œè¯„ä¼°
    """
    if len(indices) == 0:
        return []
    
    subset = Subset(dataset, indices)
    dataloader = DataLoader(subset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
    
    ensemble_preds = []
    
    for batch in dataloader:
        dyn = batch['dynamic'].to(device)
        sta = batch['static'].to(device)
        batch_size_actual = dyn.size(0)
        
        if method == 'voting':
            # å¤šæ•°æŠ•ç¥¨
            all_preds = []
            for model in models_list:
                model.eval()
                with torch.no_grad():
                    outputs = model(dyn, sta)
                    # æå–ä¸­å¿ƒåƒç´ æ¦‚ç‡ -> é¢„æµ‹ç±»åˆ«
                    center_probs = get_center_predictions(outputs)
                    preds = torch.argmax(center_probs, dim=1)
                    all_preds.append(preds.cpu().numpy())
            
            all_preds = np.array(all_preds)  # (num_models, batch_size)
            
            ensemble_batch = []
            for i in range(batch_size_actual):
                votes = all_preds[:, i]
                vote_result = np.bincount(votes.astype(int))
                pred_class = np.argmax(vote_result)
                ensemble_batch.append(pred_class)
            ensemble_preds.extend(ensemble_batch)
            
        elif method == 'averaging':
            # æ¦‚ç‡å¹³å‡
            all_probs = []
            for model in models_list:
                model.eval()
                with torch.no_grad():
                    outputs = model(dyn, sta)
                    # æå–ä¸­å¿ƒåƒç´ æ¦‚ç‡
                    center_probs = get_center_predictions(outputs)
                    all_probs.append(center_probs.cpu().numpy())
            
            all_probs = np.array(all_probs)  # (num_models, batch_size, num_classes)
            avg_probs = np.mean(all_probs, axis=0)  # (batch_size, num_classes)
            preds = np.argmax(avg_probs, axis=1)
            ensemble_preds.extend(preds)
    
    return ensemble_preds

def main():
    parser = argparse.ArgumentParser(description='åˆ†å±‚æ¨¡å‹å¿«é€Ÿè¯„ä¼°')
    parser.add_argument('--config', type=str, default='config.yaml')
    parser.add_argument('--run_dir', type=str, help='æŒ‡å®šå®éªŒè¾“å‡ºç›®å½•')
    parser.add_argument('--split', type=str, default='val', help='è¯„ä¼°æ•°æ®é›†: val æˆ– test')
    parser.add_argument('--batch_size', type=int, default=32)
    args = parser.parse_args()

    print("="*60)
    print("ğŸš€ å¯åŠ¨å…¨é“¾è·¯è¯„ä¼°è„šæœ¬ (é€‚é… Segmentation æ¶æ„)")
    print("="*60)
    
    # 1. åˆå§‹åŒ–é…ç½®ä¸è·¯å¾„
    config_path = Path(__file__).parent / args.config
    config = ConfigManager(str(config_path), create_experiment_dir=False)
    
    if args.run_dir:
        output_dir = Path(args.run_dir)
        if not output_dir.exists():
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {output_dir}")
            sys.exit(1)
        print(f"ğŸ“‚ å®éªŒç›®å½•: {output_dir}")
    else:
        output_dir = config.get_experiment_output_dir()
        print(f"âš ï¸ æœªæŒ‡å®š --run_dirï¼Œå°†åœ¨åŸºç¡€ç›®å½•å¯»æ‰¾èµ„æº: {output_dir}")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # 2. ç¡®å®šå‚æ•°
    param_file = output_dir / 'detected_parameters.json'
    if param_file.exists():
        with open(param_file, 'r') as f:
            params = json.load(f)
        dyn_ch = params['dynamic_channels']
        sta_ch = params['static_channels']
    else:
        print("âš ï¸ è‡ªåŠ¨æ¨æ–­é€šé“æ•°...")
        temp_ds = PointTimeSeriesDataset(config, None, split='val', verbose=False)
        dyn_ch = temp_ds.num_channels
        sta_ch = temp_ds.num_static_channels
    
    # è·å– patch_size
    patch_size = config.get('data_specs.spatial.patch_size', 64)
    print(f"ğŸ“Š å‚æ•°: Dynamic={dyn_ch}, Static={sta_ch}, Patch={patch_size}, T_Steps={MAX_TEMPORAL_STEPS}")

    # 3. åŠ è½½æ˜ å°„
    major_map_file = output_dir / 'major_labels_map.json'
    detailed_map_file = output_dir / 'detailed_labels_map.json'
    
    if not major_map_file.exists():
        print(f"âŒ ç¼ºå°‘æ˜ å°„æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ç›®å½•: {output_dir}")
        sys.exit(1)
        
    with open(major_map_file, 'r', encoding='utf-8') as f:
        major_map = json.load(f)
    with open(detailed_map_file, 'r', encoding='utf-8') as f:
        detailed_map = json.load(f)
    
    inverse_detailed_map = {v: k for k, v in detailed_map.items()}
    encoder = LabelEncoder(config=config, output_dir=output_dir)
    
    # 4. åŠ è½½æ•°æ®é›†
    print(f"\nğŸ“¦ åŠ è½½ {args.split} æ•°æ®é›†...")
    dataset = PointTimeSeriesDataset(config, encoder, split=args.split, verbose=True)
    if len(dataset) == 0:
        print("âŒ æ•°æ®é›†ä¸ºç©º")
        sys.exit(1)
        
    eval_df = dataset.points_df.iloc[dataset.indices].reset_index(drop=True)
    num_samples = len(dataset)
    
    true_major_array = np.array(eval_df['major_label'])
    true_detail_array = np.array(eval_df['detail_label'])
    
    pred_major_array = np.full(num_samples, -1)
    pred_detail_upper = np.full(num_samples, -1) 
    pred_detail_pipeline = np.full(num_samples, -1)

    report_lines = []
    def append_report(s):
        text = str(s)
        print(text)
        report_lines.append(text)

    # =========================================================================
    # é˜¶æ®µ A: è¯„ä¼°å¤§ç±»æ¨¡å‹
    # =========================================================================
    print("\n" + "-"*50)
    print("ğŸ—ï¸  Step 1: å¤§ç±»é¢„æµ‹ (Major Prediction)")
    print("-"*50)
    
    major_model_dir = output_dir / 'major_model'
    fold_models = []
    
    classifier_hidden_dims = config.get('model.classifier.hidden_dims', [128, 64, 32])
    
    for fold_idx in range(1, 6):
        fold_path = major_model_dir / f'fold_{fold_idx}' / 'best_model.pth'
        if fold_path.exists():
            # [ä¿®æ­£] ä¼ å…¥æ–°æ¶æ„æ‰€éœ€çš„å®Œæ•´å‚æ•°
            major_model = DualStreamSpatio_TemporalFusionNetwork(
                in_channels_dynamic=dyn_ch, 
                in_channels_static=sta_ch, 
                num_classes=len(major_map),
                patch_size=patch_size,
                temporal_steps=MAX_TEMPORAL_STEPS, # å…³é”®ä¿®å¤
                classifier_hidden_dims=classifier_hidden_dims
            ).to(device)
            
            if load_model_weights(major_model, fold_path, device):
                fold_models.append(major_model)
                print(f"   âœ… åŠ è½½å¤§ç±»æ¨¡å‹ fold_{fold_idx}")
    
    if len(fold_models) > 0:
        all_indices = list(range(num_samples))
        preds = predict_subset_ensemble(fold_models, dataset, all_indices, device, args.batch_size, method='voting')
        pred_major_array = np.array(preds)
        
        append_report(f"\nğŸ“Š å¤§ç±»é›†åˆé¢„æµ‹ (Models: {len(fold_models)}):")
        major_names = [k for k, v in sorted(major_map.items(), key=lambda x: x[1])]
        major_report = classification_report(true_major_array, pred_major_array, target_names=major_names, digits=4, zero_division=0)
        append_report(major_report)
        
        # [æ–°å¢] å¤§ç±»æ€»ä½“æŒ‡æ ‡
        m_oa = accuracy_score(true_major_array, pred_major_array)
        m_prec = precision_score(true_major_array, pred_major_array, average='macro', zero_division=0)
        m_rec = recall_score(true_major_array, pred_major_array, average='macro', zero_division=0)
        m_f1 = f1_score(true_major_array, pred_major_array, average='macro', zero_division=0)
        
        append_report("-" * 40)
        append_report(f"ğŸ”¢ å¤§ç±»æ€»ä½“æŒ‡æ ‡ (Major Overall Metrics):")
        append_report(f"   â€¢ OA (Accuracy) : {m_oa:.4f}")
        append_report(f"   â€¢ Macro Precision: {m_prec:.4f}")
        append_report(f"   â€¢ Macro Recall   : {m_rec:.4f}")
        append_report(f"   â€¢ Macro F1       : {m_f1:.4f}")
        append_report("-" * 40)

    else:
        print(f"âŒ æœªæ‰¾åˆ°å¤§ç±»æ¨¡å‹æƒé‡")

    # =========================================================================
    # é˜¶æ®µ B: è¯„ä¼°å°ç±»æ¨¡å‹
    # =========================================================================
    print("\n" + "-"*50)
    print("ğŸ—ï¸  Step 2: å°ç±»é¢„æµ‹ (Detail Prediction)")
    print("-"*50)

    for major_name, major_id in major_map.items():
        sub_model_dir = output_dir / f"detail_model_{major_id}_{major_name}"
        mapping_path = sub_model_dir / "class_mapping.json"
        
        if not (sub_model_dir / 'fold_1' / 'best_model.pth').exists():
            continue 

        try:
            with open(mapping_path, 'r', encoding='utf-8') as f:
                mapping_data = json.load(f)
            local_to_global = {int(k): int(v) for k, v in mapping_data['local_to_global_map'].items()}
        except:
            continue
            
        num_sub_classes = len(local_to_global)
        fold_models = []
        
        for fold_idx in range(1, 6):
            fold_path = sub_model_dir / f'fold_{fold_idx}' / 'best_model.pth'
            if fold_path.exists():
                # [ä¿®æ­£] ä¼ å…¥æ–°æ¶æ„æ‰€éœ€çš„å®Œæ•´å‚æ•°
                sub_model = DualStreamSpatio_TemporalFusionNetwork(
                    in_channels_dynamic=dyn_ch, 
                    in_channels_static=sta_ch, 
                    num_classes=num_sub_classes,
                    patch_size=patch_size,
                    temporal_steps=MAX_TEMPORAL_STEPS, # å…³é”®ä¿®å¤
                    classifier_hidden_dims=classifier_hidden_dims
                ).to(device)
                
                if load_model_weights(sub_model, fold_path, device):
                    fold_models.append(sub_model)
        
        if len(fold_models) == 0:
            continue
        
        # Upper Bound
        true_indices = np.where(true_major_array == major_id)[0]
        if len(true_indices) > 0:
            local_preds = predict_subset_ensemble(fold_models, dataset, true_indices, device, args.batch_size, method='voting')
            global_preds = [local_to_global[p] for p in local_preds]
            pred_detail_upper[true_indices] = global_preds
            
        # Pipeline
        pred_indices = np.where(pred_major_array == major_id)[0]
        if len(pred_indices) > 0:
            local_preds = predict_subset_ensemble(fold_models, dataset, pred_indices, device, args.batch_size, method='voting')
            global_preds = [local_to_global[p] for p in local_preds]
            pred_detail_pipeline[pred_indices] = global_preds
            
        print(f"ğŸ‘‰ [{major_name}] Models: {len(fold_models)} | Samples: True {len(true_indices)}, Pred {len(pred_indices)}")

    # =========================================================================
    # é˜¶æ®µ C: ç”ŸæˆæŠ¥å‘Š
    # =========================================================================
    print("\n" + "="*60)
    print("ğŸ“Š æœ€ç»ˆè¯„ä¼°æŠ¥å‘Š")
    print("="*60)
    
    # Pipeline Report
    valid_mask_pipe = pred_detail_pipeline != -1
    if np.sum(valid_mask_pipe) > 0:
        y_true = true_detail_array[valid_mask_pipe]
        y_pred = pred_detail_pipeline[valid_mask_pipe]
        unique_labels = sorted(list(set(y_true) | set(y_pred)))
        names = [inverse_detailed_map.get(i, str(i)) for i in unique_labels]
        
        append_report("\nğŸš€ æ€»ä½“å„å°ç±»åˆ†ç±»æŠ¥å‘Š (Pipeline):")
        pipe_report = classification_report(y_true, y_pred, target_names=names, digits=4, zero_division=0)
        append_report(pipe_report)
        
        # [æ–°å¢] å°ç±»æ€»ä½“æŒ‡æ ‡ (å®Œå–„ç‰ˆ)
        oa = accuracy_score(y_true, y_pred)
        prec = precision_score(y_true, y_pred, average='macro', zero_division=0)
        rec = recall_score(y_true, y_pred, average='macro', zero_division=0)
        f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)
        
        append_report("-" * 40)
        append_report(f"ğŸ”¢ å°ç±»æ€»ä½“æŒ‡æ ‡ (Detail Overall Metrics):")
        append_report(f"   â€¢ OA (Accuracy) : {oa:.4f}")
        append_report(f"   â€¢ Macro Precision: {prec:.4f}")
        append_report(f"   â€¢ Macro Recall   : {rec:.4f}")
        append_report(f"   â€¢ Macro F1       : {f1:.4f}")
        append_report("-" * 40)
    else:
        append_report("\nâŒ æ— æ³•ç”Ÿæˆæµæ°´çº¿æŠ¥å‘Š")

    # Save
    id_col = config.get('data_specs.csv_columns.id', 'Index')
    if id_col not in eval_df.columns:
        id_col = 'sample_id_generated'
        eval_df[id_col] = eval_df.index

    results_df = pd.DataFrame({
        'sample_id': eval_df[id_col],
        'true_major': [list(major_map.keys())[list(major_map.values()).index(i)] for i in true_major_array],
        'pred_major': [list(major_map.keys())[list(major_map.values()).index(i)] if i!=-1 else 'N/A' for i in pred_major_array],
        'true_detail': [inverse_detailed_map.get(i, str(i)) for i in true_detail_array],
        'pred_detail_pipeline': [inverse_detailed_map.get(i, str(i)) if i!=-1 else 'N/A' for i in pred_detail_pipeline]
    })
    
    csv_name = f"eval_full_report_{args.split}.csv"
    save_path = output_dir / csv_name
    results_df.to_csv(save_path, index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {save_path}")

    try:
        report_path = output_dir / f"eval_report_{args.split}.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
    except Exception as e:
        print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")

if __name__ == "__main__":
    main()