#!/usr/bin/env python3
"""
quick_eval.py: åˆ†å±‚åˆ†ç±»æ¨¡å‹éªŒè¯è„šæœ¬

ä½¿ç”¨æ–¹å¼ï¼š
1. ç¡®ä¿å·²å®‰è£…æ‰€éœ€çš„ Python åŒ…ã€‚
2. åœ¨ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
   ```
   cd code
   python quick_eval.py --run_dir ../experiments/outputs/XXXXXXXX_XXXX_EXP_2023_001
   ```

ä¿®å¤è¯´æ˜ï¼š
å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°çš„ config.yaml è€Œä¸æ˜¯å®éªŒç›®å½•ä¸‹çš„ config_used.yamlï¼Œ
ä»¥é˜²æ­¢ç›¸å¯¹è·¯å¾„è§£æé”™è¯¯ (FileNotFoundError)ã€‚
"""
print("ğŸ’¡ è„šæœ¬æ­£åœ¨å¯åŠ¨...")

import sys
import json
import logging
import argparse
from pathlib import Path
import torch
from torch.utils.data import DataLoader
import pandas as pd
import numpy as np
from tqdm import tqdm
from sklearn.metrics import accuracy_score, classification_report

# å¯¼å…¥æœ¬åœ°æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent))
from config_manager import ConfigManager
from label_encoder import LabelEncoder
from raster_crawler import RasterCrawler
from point_timeseries_dataset import PointTimeSeriesDataset, collate_fn
from model_architecture import DualStreamSpatio_TemporalFusionNetwork

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)

def load_major_model(run_dir, num_classes, input_channels, device):
    """åŠ è½½å¤§ç±»æ¨¡å‹"""
    model_path = run_dir / "major_model" / "best_model.pth"
    if not model_path.exists():
        # å°è¯•åŠ è½½ last_model.pth ä½œä¸ºå¤‡é€‰
        model_path = run_dir / "major_model" / "last_model.pth"
        if not model_path.exists():
             raise FileNotFoundError(f"âŒ å¤§ç±»æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {model_path}")
    
    print(f"ğŸ“¦ åŠ è½½å¤§ç±»æ¨¡å‹: {model_path}")
    model = DualStreamSpatio_TemporalFusionNetwork(
        in_channels_dynamic=input_channels['dynamic'],
        in_channels_static=input_channels['static'],
        num_classes=num_classes
    )
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)
    model.eval()
    return model

def load_detail_models(run_dir, hierarchical_map, input_channels, device):
    """
    åŠ è½½æ‰€æœ‰å°ç±»æ¨¡å‹
    """
    models = {}
    mappings = {}
    single_class_map = {}
    
    print("ğŸ“¦ åŠ è½½å°ç±»æ¨¡å‹...")
    
    for major_name, info in hierarchical_map.items():
        major_id = info['major_id']
        detail_classes = info['detail_classes']
        
        # æƒ…å†µ1ï¼šåªæœ‰ä¸€ä¸ªå°ç±»ï¼Œæ²¡æœ‰è®­ç»ƒæ¨¡å‹ï¼Œç›´æ¥è®°å½•ID
        if len(detail_classes) <= 1:
            global_id = list(detail_classes.values())[0]
            single_class_map[major_id] = global_id
            continue
            
        # æƒ…å†µ2ï¼šæœ‰å¤šä¸ªå°ç±»ï¼ŒåŠ è½½å¯¹åº”çš„æ¨¡å‹
        model_folder = run_dir / f"detail_model_{major_id}_{major_name}"
        model_path = model_folder / "best_model.pth"
        if not model_path.exists():
             model_path = model_folder / "last_model.pth"

        mapping_path = model_folder / "class_mapping.json"
        
        if not model_path.exists() or not mapping_path.exists():
            print(f"  âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°å¤§ç±» {major_name} çš„æ¨¡å‹æ–‡ä»¶ï¼Œè·³è¿‡ã€‚")
            continue
            
        # åŠ è½½æ˜ å°„é…ç½®
        with open(mapping_path, 'r', encoding='utf-8') as f:
            map_data = json.load(f)
        # è½¬æ¢ key ä¸º int
        local_to_global = {int(k): int(v) for k, v in map_data['local_to_global_map'].items()}
        mappings[major_id] = local_to_global
        
        # åŠ è½½æ¨¡å‹
        sub_model = DualStreamSpatio_TemporalFusionNetwork(
            in_channels_dynamic=input_channels['dynamic'],
            in_channels_static=input_channels['static'],
            num_classes=len(detail_classes)
        )
        sub_model.load_state_dict(torch.load(model_path, map_location=device))
        sub_model.to(device)
        sub_model.eval()
        models[major_id] = sub_model
        
    return models, mappings, single_class_map

def predict_batch(dynamic, static, major_model, detail_models, detail_mappings, single_class_map, device):
    """
    å¯¹ä¸€ä¸ª Batch è¿›è¡Œçº§è”é¢„æµ‹
    """
    batch_size = dynamic.size(0)
    
    # 1. é¢„æµ‹å¤§ç±»
    with torch.no_grad():
        major_outputs = major_model(dynamic, static)
        major_preds = torch.argmax(major_outputs['logits'], dim=1) 
    
    detail_preds_global = torch.zeros(batch_size, dtype=torch.long, device=device)
    
    # 2. é¢„æµ‹å°ç±» (è·¯ç”±é€»è¾‘)
    unique_major_ids = torch.unique(major_preds)
    
    for mid in unique_major_ids:
        mid_item = mid.item()
        indices = (major_preds == mid)
        
        sub_dynamic = dynamic[indices]
        sub_static = static[indices]
        
        if mid_item in detail_models:
            # A. è°ƒç”¨å°ç±»æ¨¡å‹
            model = detail_models[mid_item]
            mapping = detail_mappings[mid_item]
            
            with torch.no_grad():
                sub_out = model(sub_dynamic, sub_static)
                sub_preds_local = torch.argmax(sub_out['logits'], dim=1)
            
            # æ˜ å°„å›å…¨å±€ID
            sub_preds_local_np = sub_preds_local.cpu().numpy()
            sub_preds_global_np = [mapping[loc_id] for loc_id in sub_preds_local_np]
            
            detail_preds_global[indices] = torch.tensor(sub_preds_global_np, device=device)
            
        elif mid_item in single_class_map:
            # B. åªæœ‰ä¸€ä¸ªå°ç±»
            target_global_id = single_class_map[mid_item]
            detail_preds_global[indices] = target_global_id
            
        else:
            # C. å¼‚å¸¸æƒ…å†µ
            detail_preds_global[indices] = -1 
            
    return major_preds, detail_preds_global

def main():
    parser = argparse.ArgumentParser(description="åˆ†å±‚æ¨¡å‹éªŒè¯è„šæœ¬")
    parser.add_argument('--run_dir', type=str, required=True, help="å®éªŒè¾“å‡ºç›®å½•è·¯å¾„")
    parser.add_argument('--split', type=str, default='val', choices=['val', 'test', 'train'], help="æ•°æ®é›†åˆ’åˆ†")
    parser.add_argument('--batch_size', type=int, default=32, help="æ‰¹æ¬¡å¤§å°")
    args = parser.parse_args()
    
    run_dir = Path(args.run_dir)
    if not run_dir.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {run_dir}")
        return

    # =========================================================
    # å…³é”®ä¿®å¤: å§‹ç»ˆåŠ è½½æœ¬åœ°çš„ config.yaml
    # =========================================================
    # å‡è®¾ evaluate.py å’Œ config.yaml åœ¨åŒä¸€ä¸ªç›®å½• (code/)
    local_config_path = Path(__file__).parent / 'config.yaml'
    
    if not local_config_path.exists():
        print(f"âŒ æ‰¾ä¸åˆ°æœ¬åœ°é…ç½®æ–‡ä»¶: {local_config_path}")
        print("è¯·ç¡®ä¿è„šæœ¬è¿è¡Œåœ¨ code ç›®å½•ä¸‹ï¼Œä¸” config.yaml å­˜åœ¨ã€‚")
        return
        
    print(f"ğŸ“‹ åŠ è½½é…ç½®æ–‡ä»¶: {local_config_path}")
    # ä½¿ç”¨æœ¬åœ°è·¯å¾„åˆå§‹åŒ–ï¼Œè¿™æ ·ç›¸å¯¹è·¯å¾„ (../data) æ‰ä¼šè§£ææ­£ç¡®
    config = ConfigManager(str(local_config_path))
    
    # 2. å‡†å¤‡æ•°æ®é›†
    print("ğŸ”„ åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨...")
    encoder = LabelEncoder(config=config)
    
    dynamic_crawler = RasterCrawler(
        config=config, 
        raster_dir=config.get_resolved_path('dynamic_images_dir'), 
        filename_pattern=config.get('data_specs.raster_crawler.filename_pattern'),
        file_extensions=['.tif']
    )
    static_crawler = RasterCrawler(
        config=config, 
        raster_dir=config.get_resolved_path('static_images_dir'), 
        filename_pattern=config.get('data_specs.raster_crawler.filename_pattern'),
        file_extensions=['.tif']
    )
    
    # è‡ªåŠ¨æ£€æµ‹é€šé“æ•°
    try:
        dyn_ch = dynamic_crawler.detect_num_channels()['most_common']
        sta_ch = static_crawler.detect_num_channels()['most_common']
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è‡ªåŠ¨æ£€æµ‹é€šé“æ•°ï¼Œå°è¯•è¯»å– detected_parameters.json")
        # å°è¯•ä»è¿è¡Œç›®å½•è¯»å–
        param_file = run_dir / 'detected_parameters.json'
        if param_file.exists():
            with open(param_file, 'r') as f:
                params = json.load(f)
                dyn_ch = params.get('dynamic_channels', 4)
                sta_ch = params.get('static_channels', 1)
        else:
            print("âŒ æ— æ³•ç¡®å®šè¾“å…¥é€šé“æ•°ï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„ã€‚")
            return

    input_channels = {'dynamic': dyn_ch, 'static': sta_ch}
    
    dataset = PointTimeSeriesDataset(
        config=config, 
        encoder=encoder, 
        dynamic_crawler=dynamic_crawler, 
        static_crawler=static_crawler, 
        split=args.split, 
        cache_metadata=True, 
        verbose=False
    )
    
    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False, collate_fn=collate_fn, num_workers=0) # Windowsä¸‹è®¾ä¸º0æ›´å®‰å…¨
    
    print(f"ğŸ“Š éªŒè¯é›†æ ·æœ¬æ•°: {len(dataset)}")
    
    # 3. åŠ è½½æ¨¡å‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    
    major_map = encoder.get_major_labels_map()
    hierarchical_map = encoder.get_hierarchical_map()
    
    try:
        major_model = load_major_model(run_dir, len(major_map), input_channels, device)
        detail_models, detail_mappings, single_class_map = load_detail_models(
            run_dir, hierarchical_map, input_channels, device
        )
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return

    # 4. æ‰§è¡Œæ¨ç†
    print("\nğŸš€ å¼€å§‹åˆ†å±‚æ¨ç†...")
    all_results = []
    
    pbar = tqdm(dataloader, desc="Eval")
    for batch in pbar:
        dynamic = batch['dynamic'].to(device)
        static = batch['static'].to(device)
        major_true = batch['major_label'].to(device)
        detail_true = batch['detail_label'].to(device)
        # è·å–ID, å…¼å®¹ä¸åŒ dataset å®ç°
        ids = batch.get('id', torch.zeros(len(major_true))).cpu().numpy()
        
        major_preds, detail_preds = predict_batch(
            dynamic, static, 
            major_model, detail_models, detail_mappings, single_class_map, 
            device
        )
        
        for i in range(len(ids)):
            all_results.append({
                'id': ids[i],
                'major_true': major_true[i].item(),
                'major_pred': major_preds[i].item(),
                'detail_true': detail_true[i].item(),
                'detail_pred': detail_preds[i].item()
            })
            
    # 5. ç”ŸæˆæŠ¥å‘Š
    if not all_results:
        print("âŒ æœªç”Ÿæˆä»»ä½•é¢„æµ‹ç»“æœï¼Œè¯·æ£€æŸ¥æ•°æ®åŠ è½½å™¨ã€‚")
        return

    df_res = pd.DataFrame(all_results)
    
    inv_major_map = {v: k for k, v in major_map.items()}
    detailed_map = encoder.get_detailed_labels_map()
    inv_detail_map = {v: k for k, v in detailed_map.items()}
    
    df_res['major_true_name'] = df_res['major_true'].map(inv_major_map)
    df_res['major_pred_name'] = df_res['major_pred'].map(inv_major_map)
    df_res['detail_true_name'] = df_res['detail_true'].map(inv_detail_map)
    df_res['detail_pred_name'] = df_res['detail_pred'].map(inv_detail_map)
    
    df_res['major_correct'] = df_res['major_true'] == df_res['major_pred']
    df_res['detail_correct'] = df_res['detail_true'] == df_res['detail_pred']
    
    print("\n" + "="*60)
    print("ğŸ“Š éªŒè¯ç»“æœæŠ¥å‘Š")
    print("="*60)
    
    # æŒ‡æ ‡è®¡ç®—
    major_acc = accuracy_score(df_res['major_true'], df_res['major_pred'])
    print(f"\nâœ… å¤§ç±»æ€»ä½“å‡†ç¡®ç‡ (Major Accuracy): {major_acc:.2%}")
    # é¿å… warning: æŒ‡å®š labels
    unique_major = sorted(list(df_res['major_true'].unique()))
    print("\nå¤§ç±»åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(
        df_res['major_true'], 
        df_res['major_pred'], 
        labels=unique_major,
        target_names=[inv_major_map.get(i, str(i)) for i in unique_major], 
        digits=4,
        zero_division=0
    ))
    
    detail_acc = accuracy_score(df_res['detail_true'], df_res['detail_pred'])
    print(f"\nâœ… å°ç±»æ€»ä½“å‡†ç¡®ç‡ (Detail Accuracy): {detail_acc:.2%}")
    
    conditional_df = df_res[df_res['major_correct']]
    if len(conditional_df) > 0:
        cond_acc = accuracy_score(conditional_df['detail_true'], conditional_df['detail_pred'])
        print(f"ğŸ‘‰ å¤§ç±»æ­£ç¡®æ¡ä»¶ä¸‹çš„å°ç±»å‡†ç¡®ç‡: {cond_acc:.2%}")
    
    # ä¿å­˜ç»“æœ
    output_csv = run_dir / f"evaluation_predictions_{args.split}.csv"
    cols = ['id', 'major_true_name', 'major_pred_name', 'major_correct', 
            'detail_true_name', 'detail_pred_name', 'detail_correct',
            'major_true', 'major_pred', 'detail_true', 'detail_pred']
    df_res[cols].to_csv(output_csv, index=False, encoding='utf-8-sig')

if __name__ == "__main__":
    main()