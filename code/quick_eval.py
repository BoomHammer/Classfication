#!/usr/bin/env python3
"""
quick_eval.py: åˆ†å±‚åˆ†ç±»æ¨¡å‹éªŒè¯è„šæœ¬

ä½¿ç”¨æ–¹å¼ï¼š
1. ç¡®ä¿å·²å®‰è£…æ‰€éœ€çš„ Python åŒ…ã€‚
2. åœ¨ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
   ```
   python code/quick_eval.py --run_dir ../experiments/outputs/XXXXXXXX_XXXX_EXP_2023_001
   ```ã€‚
"""
print("ğŸ’¡ è„šæœ¬æ­£åœ¨å¯åŠ¨...")

import torch
import json
import sys
import logging
import argparse
import numpy as np
import pandas as pd
from pathlib import Path
from tqdm import tqdm
from torch.utils.data import DataLoader, Subset
from sklearn.metrics import classification_report, accuracy_score

# å¯¼å…¥æœ¬åœ°æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent))
from config_manager import ConfigManager
from label_encoder import LabelEncoder
from point_timeseries_dataset import PointTimeSeriesDataset, collate_fn
from model_architecture import DualStreamSpatio_TemporalFusionNetwork

def load_model_weights(model, path, device):
    """å®‰å…¨åŠ è½½æ¨¡å‹æƒé‡"""
    try:
        # print(f"   â³ åŠ è½½æƒé‡: {path.name} ...")
        checkpoint = torch.load(path, map_location=device, weights_only=False)
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        return True
    except Exception as e:
        print(f"   âŒ æƒé‡åŠ è½½å¤±è´¥: {e}")
        return False

def predict_subset(model, dataset, indices, device, batch_size):
    """è¾…åŠ©å‡½æ•°ï¼šå¯¹æŒ‡å®šç´¢å¼•çš„å­é›†è¿›è¡Œé¢„æµ‹ï¼Œè¿”å›å±€éƒ¨é¢„æµ‹ç»“æœ"""
    # [ä¿®å¤] ä½¿ç”¨ len() åˆ¤æ–­ï¼Œå…¼å®¹ List å’Œ NumPy Array
    if len(indices) == 0:
        return []
    
    subset = Subset(dataset, indices)
    dataloader = DataLoader(subset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
    
    local_preds = []
    model.eval()
    with torch.no_grad():
        for batch in dataloader:
            dyn = batch['dynamic'].to(device)
            sta = batch['static'].to(device)
            outputs = model(dyn, sta)
            preds = torch.argmax(outputs['probabilities'], dim=1)
            local_preds.extend(preds.cpu().numpy())
            
    return local_preds

def main():
    parser = argparse.ArgumentParser(description='åˆ†å±‚æ¨¡å‹å¿«é€Ÿè¯„ä¼°')
    parser.add_argument('--config', type=str, default='config.yaml')
    parser.add_argument('--run_dir', type=str, help='æŒ‡å®šå®éªŒè¾“å‡ºç›®å½•')
    parser.add_argument('--split', type=str, default='val', help='è¯„ä¼°æ•°æ®é›†: val æˆ– test')
    parser.add_argument('--batch_size', type=int, default=32)
    args = parser.parse_args()

    print("="*60)
    print("ğŸš€ å¯åŠ¨å…¨é“¾è·¯è¯„ä¼°è„šæœ¬")
    print("="*60)
    
    # 1. åˆå§‹åŒ–é…ç½®ä¸è·¯å¾„
    config_path = Path(__file__).parent / args.config
    config = ConfigManager(str(config_path))
    
    if args.run_dir:
        output_dir = Path(args.run_dir)
        if not output_dir.exists():
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {output_dir}")
            sys.exit(1)
        print(f"ğŸ“‚ å®éªŒç›®å½•: {output_dir}")
    else:
        print("âš ï¸ æœªæŒ‡å®š --run_dirï¼Œä½¿ç”¨é»˜è®¤ç›®å½•")
        output_dir = config.get_experiment_output_dir()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"âš™ï¸  é…ç½®: Split={args.split}, Device={device}")
    
    # 2. ç¡®å®šé€šé“æ•°
    param_file = output_dir / 'detected_parameters.json'
    if param_file.exists():
        with open(param_file, 'r') as f:
            params = json.load(f)
        dyn_ch = params['dynamic_channels']
        sta_ch = params['static_channels']
    else:
        print("âš ï¸ è‡ªåŠ¨æ¨æ–­é€šé“æ•°...")
        temp_ds = PointTimeSeriesDataset(config, None, split='val', verbose=False)
        dyn_ch = temp_ds.num_channels
        sta_ch = temp_ds.num_static_channels
    print(f"ğŸ“Š é€šé“: Dynamic={dyn_ch}, Static={sta_ch}")

    # 3. åŠ è½½æ˜ å°„
    major_map_file = output_dir / 'major_labels_map.json'
    detailed_map_file = output_dir / 'detailed_labels_map.json'
    
    if not major_map_file.exists():
        print(f"âŒ ç¼ºå°‘æ˜ å°„æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ç›®å½•")
        sys.exit(1)
        
    with open(major_map_file, 'r', encoding='utf-8') as f:
        major_map = json.load(f)
    with open(detailed_map_file, 'r', encoding='utf-8') as f:
        detailed_map = json.load(f)
    
    inverse_detailed_map = {v: k for k, v in detailed_map.items()}
    encoder = LabelEncoder(config=config, output_dir=output_dir)
    
    # 4. åŠ è½½æ•°æ®é›†
    print(f"\nğŸ“¦ åŠ è½½ {args.split} æ•°æ®é›†...")
    dataset = PointTimeSeriesDataset(config, encoder, split=args.split, verbose=True)
    if len(dataset) == 0:
        print("âŒ æ•°æ®é›†ä¸ºç©º")
        sys.exit(1)
        
    # è·å–ç”¨äºç´¢å¼•çš„ DataFrame
    eval_df = dataset.points_df.iloc[dataset.indices].reset_index(drop=True)
    num_samples = len(dataset)
    
    # åˆå§‹åŒ–ç»“æœæ•°ç»„
    true_major_array = np.array(eval_df['major_label'])
    true_detail_array = np.array(eval_df['detail_label'])
    
    pred_major_array = np.full(num_samples, -1)
    
    # [å…³é”®] ä¸¤ä¸ªå°ç±»é¢„æµ‹æ•°ç»„
    # 1. Upper Bound: å‡è®¾å¤§ç±»å·²çŸ¥ï¼Œé€å…¥æ­£ç¡®çš„å°ç±»æ¨¡å‹ (åæ˜ å°ç±»æ¨¡å‹æœ¬èº«èƒ½åŠ›)
    pred_detail_upper = np.full(num_samples, -1) 
    # 2. Pipeline: ä¾æ®é¢„æµ‹çš„å¤§ç±»ï¼Œé€å…¥å¯¹åº”çš„å°ç±»æ¨¡å‹ (åæ˜ çœŸå®ç³»ç»Ÿèƒ½åŠ›)
    pred_detail_pipeline = np.full(num_samples, -1)

    # =========================================================================
    # é˜¶æ®µ A: è¯„ä¼°å¤§ç±»æ¨¡å‹
    # =========================================================================
    print("\n" + "-"*50)
    print("ğŸ—ï¸  Step 1: å¤§ç±»é¢„æµ‹ (Major Prediction)")
    print("-"*50)
    
    major_model_path = output_dir / 'major_model' / 'best_model.pth'
    if major_model_path.exists():
        major_model = DualStreamSpatio_TemporalFusionNetwork(
            in_channels_dynamic=dyn_ch, in_channels_static=sta_ch, num_classes=len(major_map)
        ).to(device)
        
        if load_model_weights(major_model, major_model_path, device):
            # å¯¹æ‰€æœ‰æ•°æ®è¿›è¡Œå¤§ç±»é¢„æµ‹
            all_indices = list(range(num_samples))
            preds = predict_subset(major_model, dataset, all_indices, device, args.batch_size)
            pred_major_array = np.array(preds)
            
            # è¾“å‡ºæŠ¥å‘Š
            print("\nğŸ“‹ å¤§ç±»åˆ†ç±»æŠ¥å‘Š:")
            major_names = [k for k, v in sorted(major_map.items(), key=lambda x: x[1])]
            print(classification_report(true_major_array, pred_major_array, target_names=major_names, digits=4, zero_division=0))
    else:
        print(f"âŒ å¤§ç±»æ¨¡å‹ç¼ºå¤±: {major_model_path}")

    # =========================================================================
    # é˜¶æ®µ B: è¯„ä¼°å°ç±»æ¨¡å‹ (åŒè·¯å¾„)
    # =========================================================================
    print("\n" + "-"*50)
    print("ğŸ—ï¸  Step 2: å°ç±»é¢„æµ‹ (Detail Prediction)")
    print("-"*50)

    # éå†æ¯ä¸€ä¸ªå¤§ç±» ID
    for major_name, major_id in major_map.items():
        sub_model_dir = output_dir / f"detail_model_{major_id}_{major_name}"
        model_path = sub_model_dir / "best_model.pth"
        mapping_path = sub_model_dir / "class_mapping.json"
        
        # å¦‚æœè¯¥å¤§ç±»æ²¡æœ‰è®­ç»ƒå¥½çš„å°ç±»æ¨¡å‹
        if not model_path.exists():
            continue 

        # åŠ è½½å±€éƒ¨æ˜ å°„
        try:
            with open(mapping_path, 'r', encoding='utf-8') as f:
                mapping_data = json.load(f)
            local_to_global = {int(k): int(v) for k, v in mapping_data['local_to_global_map'].items()}
        except:
            continue
            
        num_sub_classes = len(local_to_global)
        
        # åŠ è½½æ¨¡å‹
        sub_model = DualStreamSpatio_TemporalFusionNetwork(
            in_channels_dynamic=dyn_ch, in_channels_static=sta_ch, num_classes=num_sub_classes
        ).to(device)
        
        if not load_model_weights(sub_model, model_path, device):
            continue
        
        # --- è·¯å¾„ 1: Upper Bound (åŸºäºçœŸå®æ ‡ç­¾) ---
        true_indices = np.where(true_major_array == major_id)[0]
        if len(true_indices) > 0:
            local_preds = predict_subset(sub_model, dataset, true_indices, device, args.batch_size)
            global_preds = [local_to_global[p] for p in local_preds]
            pred_detail_upper[true_indices] = global_preds
            
        # --- è·¯å¾„ 2: Pipeline (åŸºäºå¤§ç±»é¢„æµ‹) ---
        # æ‰¾å‡ºå¤§ç±»æ¨¡å‹é¢„æµ‹ä¸ºå½“å‰ major_id çš„æ‰€æœ‰æ ·æœ¬ (å¯èƒ½åŒ…å«è¯¯åˆ¤è¿›æ¥çš„)
        pred_indices = np.where(pred_major_array == major_id)[0]
        if len(pred_indices) > 0:
            local_preds = predict_subset(sub_model, dataset, pred_indices, device, args.batch_size)
            global_preds = [local_to_global[p] for p in local_preds]
            pred_detail_pipeline[pred_indices] = global_preds
            
        print(f"ğŸ‘‰ æ¨¡å‹ [{major_name}]: å¤„ç†çœŸå®æ ·æœ¬ {len(true_indices)} ä¸ª, å¤„ç†é¢„æµ‹æ ·æœ¬ {len(pred_indices)} ä¸ª")

    # =========================================================================
    # é˜¶æ®µ C: ç”ŸæˆæŠ¥å‘Š
    # =========================================================================
    print("\n" + "="*60)
    print("ğŸ“Š æœ€ç»ˆè¯„ä¼°æŠ¥å‘Š")
    print("="*60)
    
    # 1. Upper Bound æŠ¥å‘Š
    valid_mask_upper = pred_detail_upper != -1
    if np.sum(valid_mask_upper) > 0:
        y_true = true_detail_array[valid_mask_upper]
        y_pred = pred_detail_upper[valid_mask_upper]
        unique_labels = sorted(list(set(y_true) | set(y_pred)))
        names = [inverse_detailed_map.get(i, str(i)) for i in unique_labels]
        
        print("\nâœ… å°ç±»åˆ†ç±»æŠ¥å‘Š (Upper Bound - å‡è®¾å¤§ç±»æ­£ç¡®):")
        print("   (ä»…åŒ…å«å·²è®­ç»ƒå°ç±»æ¨¡å‹çš„ç±»åˆ«)")
        print(classification_report(y_true, y_pred, target_names=names, digits=4, zero_division=0))
    
    # 2. Pipeline æŠ¥å‘Š
    valid_mask_pipe = pred_detail_pipeline != -1
    
    if np.sum(valid_mask_pipe) > 0:
        y_true = true_detail_array[valid_mask_pipe]
        y_pred = pred_detail_pipeline[valid_mask_pipe]
        
        unique_labels = sorted(list(set(y_true) | set(y_pred)))
        names = [inverse_detailed_map.get(i, str(i)) for i in unique_labels]
        
        print("\nğŸš€ æ€»ä½“å„å°ç±»åˆ†ç±»æŠ¥å‘Š (Pipeline - çœŸå®æµæ°´çº¿):")
        print("   (åŒ…å«å¤§ç±»é”™è¯¯å¯¼è‡´çš„ä¼ é€’è¯¯å·®)")
        print(classification_report(y_true, y_pred, target_names=names, digits=4, zero_division=0))
        
        acc = accuracy_score(y_true, y_pred)
        print(f"ğŸ† æ€»ä½“å°ç±»å‡†ç¡®ç‡ (Pipeline Accuracy): {acc:.2%}")
    else:
        print("\nâŒ æ— æ³•ç”Ÿæˆæµæ°´çº¿æŠ¥å‘Š (å¯èƒ½æ˜¯å¤§ç±»æ¨¡å‹æœªé¢„æµ‹å‡ºä»»ä½•æœ‰æ•ˆç±»åˆ«)")

    # 3. ä¿å­˜è¯¦ç»†ç»“æœ
    id_col = config.get('data_specs.csv_columns.id', 'Index')
    if id_col not in eval_df.columns:
        id_col = 'sample_id_generated'
        eval_df[id_col] = eval_df.index

    results_df = pd.DataFrame({
        'sample_id': eval_df[id_col],
        'true_major': [list(major_map.keys())[list(major_map.values()).index(i)] for i in true_major_array],
        'pred_major': [list(major_map.keys())[list(major_map.values()).index(i)] if i!=-1 else 'N/A' for i in pred_major_array],
        'true_detail': [inverse_detailed_map.get(i, str(i)) for i in true_detail_array],
        'pred_detail_upper': [inverse_detailed_map.get(i, str(i)) if i!=-1 else 'N/A' for i in pred_detail_upper],
        'pred_detail_pipeline': [inverse_detailed_map.get(i, str(i)) if i!=-1 else 'N/A' for i in pred_detail_pipeline]
    })
    
    csv_name = f"eval_full_report_{args.split}.csv"
    save_path = output_dir / csv_name
    results_df.to_csv(save_path, index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ è¯¦ç»†é¢„æµ‹ç»“æœå·²ä¿å­˜: {save_path}")

if __name__ == "__main__":
    main()