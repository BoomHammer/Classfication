import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
from pathlib import Path
import logging
import numpy as np
from sklearn.metrics import classification_report, f1_score, confusion_matrix
from sklearn.model_selection import StratifiedKFold
import json

# ============================================================================
# æ ‡ç­¾å¹³æ»‘äº¤å‰ç†µæŸå¤± (Label Smoothing)
# ============================================================================
class LabelSmoothingLoss(nn.Module):
    """
    å¸¦æ ‡ç­¾å¹³æ»‘çš„äº¤å‰ç†µæŸå¤±
    å‡å°‘æ¨¡å‹å¯¹é¢„æµ‹çš„è¿‡åº¦è‡ªä¿¡ï¼Œæé«˜æ³›åŒ–æ€§èƒ½
    """
    def __init__(self, num_classes, smoothing=0.1, reduction='mean', weight=None, device='cuda'):
        super().__init__()
        self.num_classes = num_classes
        self.smoothing = smoothing
        self.reduction = reduction
        self.device = device
        if weight is not None:
            self.weight = weight.to(device) if isinstance(weight, torch.Tensor) else torch.tensor(weight, device=device, dtype=torch.float)
        else:
            self.weight = None
    
    def forward(self, pred, target):
        """
        pred: (B, C) logits
        target: (B,) target indices
        """
        pred = pred.log_softmax(dim=-1)
        
        with torch.no_grad():
            # åˆ›å»ºå¹³æ»‘çš„targetåˆ†å¸ƒ
            true_dist = torch.zeros_like(pred)
            true_dist.fill_(self.smoothing / (self.num_classes - 1))
            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)
        
        # è®¡ç®—KLæ•£åº¦
        loss = torch.sum(-true_dist * pred, dim=-1)
        
        # [æ”¹è¿›] åº”ç”¨ç±»åˆ«æƒé‡æ—¶è¿›è¡Œå½’ä¸€åŒ–ï¼Œé˜²æ­¢lossçˆ†ç‚¸
        if self.weight is not None:
            weight_t = self.weight[target]
            # [å…³é”®ä¿®å¤] å½’ä¸€åŒ–æƒé‡
            weight_t = weight_t / (weight_t.max() + 1e-8)
            loss = loss * weight_t
        
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:
            return loss

# ============================================================================
# Focal Loss å®šä¹‰
# ============================================================================
class FocalLoss(nn.Module):
    """
    Focal Loss: é™ä½æ˜“åˆ†æ ·æœ¬æƒé‡ï¼Œå…³æ³¨éš¾åˆ†æ ·æœ¬
    Gamma: èšç„¦å‚æ•° (é»˜è®¤2.0)
    Alpha: ç±»åˆ«å¹³è¡¡å‚æ•° (å¯ä»¥æ˜¯åˆ—è¡¨æˆ–Tensor)
    """
    def __init__(self, alpha=None, gamma=2.0, reduction='mean', device='cuda'):
        super(FocalLoss, self).__init__()
        self.gamma = gamma
        self.reduction = reduction
        self.device = device
        
        if alpha is not None:
            if isinstance(alpha, (list, np.ndarray)):
                self.alpha = torch.tensor(alpha, dtype=torch.float32).to(device)
            else:
                self.alpha = alpha.to(device)
        else:
            self.alpha = None

    def forward(self, inputs, targets):
        # [ä¿®å¤] è®¡ç®—äº¤å‰ç†µæŸå¤±ï¼Œä¸ä½¿ç”¨ alpha æƒé‡ï¼ˆæƒé‡åœ¨ Focal ä¸­å·²éšå«å¤„ç†ï¼‰
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        
        # [é˜²æŠ¤] é™åˆ¶ce_lossçš„èŒƒå›´ï¼Œé˜²æ­¢æ•°å€¼æº¢å‡º
        ce_loss = torch.clamp(ce_loss, min=1e-6, max=100.0)
        
        pt = torch.exp(-ce_loss)
        focal_loss = (1 - pt) ** self.gamma * ce_loss
        
        # [æ”¹è¿›] åœ¨è¿™é‡Œåº”ç”¨æƒé‡ï¼Œè€Œä¸æ˜¯åœ¨ce_lossè®¡ç®—ä¸­
        if self.alpha is not None:
            weight_t = self.alpha[targets]
            # [å…³é”®ä¿®å¤] å½’ä¸€åŒ–æƒé‡ï¼Œé˜²æ­¢lossè¿‡å¤§
            weight_t = weight_t / (weight_t.max() + 1e-8)
            focal_loss = focal_loss * weight_t

        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

# ============================================================================
# Trainer ç±»
# ============================================================================
class Trainer:
    def __init__(
        self,
        model: nn.Module,
        train_dataloader: DataLoader,
        val_dataloader: DataLoader,
        test_dataloader: DataLoader = None,
        num_classes: int = 2,
        device: str = 'cuda',
        output_dir: str = './output',
        class_weights: torch.Tensor = None,
        target_key: str = 'label',
        verbose: bool = True,
        label_mapping: dict = None,
        use_focal_loss: bool = True,
        label_smoothing: float = 0.1,  # æ–°å¢å‚æ•°
        model_init_params: dict = None  # æ–°å¢ï¼šä¿å­˜æ¨¡å‹åˆå§‹åŒ–å‚æ•°
    ):
        self.model = model.to(device)
        self.model_init_params = model_init_params or {}  # ä¿å­˜æ¨¡å‹åˆå§‹åŒ–å‚æ•°
        self.train_loader = train_dataloader
        self.val_loader = val_dataloader
        self.test_loader = test_dataloader
        self.num_classes = num_classes
        self.device = device
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.verbose = verbose
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)
        self.target_key = target_key
        self.label_mapping = label_mapping

        # æ—¥å¿— Handler
        if not any(isinstance(h, logging.FileHandler) for h in self.logger.handlers):
            log_path = self.output_dir / "train.log"
            file_handler = logging.FileHandler(log_path, encoding='utf-8')
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            file_handler.setFormatter(formatter)
            self.logger.addHandler(file_handler)
        
        # æŸå¤±å‡½æ•°é€‰æ‹©
        if use_focal_loss:
            self.logger.info(f" ğŸ”§ä½¿ç”¨ Focal Loss (æ ‡ç­¾å¹³æ»‘={label_smoothing}) å¤„ç†éš¾åˆ†æ ·æœ¬")
            self.criterion = FocalLoss(alpha=class_weights, gamma=2.0, device=device)
        else:
            self.logger.info(f"ğŸ”§ ä½¿ç”¨ CrossEntropy Loss (æ ‡ç­¾å¹³æ»‘={label_smoothing})")
            # ä½¿ç”¨æ ‡ç­¾å¹³æ»‘è€Œä¸æ˜¯ç›´æ¥CrossEntropy
            self.criterion = LabelSmoothingLoss(num_classes=num_classes, smoothing=label_smoothing, weight=class_weights, device=device)
            
        self.scaler = torch.amp.GradScaler('cuda') if torch.cuda.is_available() else None
        if hasattr(torch.amp, 'GradScaler'):
             self.scaler = torch.amp.GradScaler('cuda')

        self.optimizer = None
        self.best_val_f1 = 0.0
        self.best_epoch = 0
        self.history = {'train_loss': [], 'train_acc': [], 'train_f1': [], 'val_loss': [], 'val_acc': [], 'val_f1': []}

    def mixup_data(self, x_dyn, x_sta, y, alpha=0.4):
        if alpha > 0:
            lam = np.random.beta(alpha, alpha)
        else:
            lam = 1
        batch_size = x_dyn.size(0)
        index = torch.randperm(batch_size).to(self.device)
        mixed_dyn = lam * x_dyn + (1 - lam) * x_dyn[index, :]
        mixed_sta = lam * x_sta + (1 - lam) * x_sta[index, :]
        y_a, y_b = y, y[index]
        return mixed_dyn, mixed_sta, y_a, y_b, lam

    def _get_labels(self, batch):
        labels = batch[self.target_key].to(self.device)
        if self.label_mapping:
            cpu_labels = labels.cpu().numpy()
            # [æ”¹è¿›] ç¡®ä¿æ‰€æœ‰æ ‡ç­¾éƒ½èƒ½è¢«æ­£ç¡®æ˜ å°„ï¼Œå¦åˆ™æ‰“å°è­¦å‘Š
            local_labels = []
            for x in cpu_labels:
                if x in self.label_mapping:
                    local_labels.append(self.label_mapping[x])
                else:
                    # [é˜²æŠ¤] å¦‚æœæ‰¾ä¸åˆ°æ˜ å°„ï¼Œä½¿ç”¨æ˜ å°„è¡¨ä¸­çš„ç¬¬ä¸€ä¸ªæœ‰æ•ˆå€¼
                    print(f"âš ï¸ è­¦å‘Š: æ ‡ç­¾ {x} æœªåœ¨æ˜ å°„è¡¨ä¸­æ‰¾åˆ°ï¼Œå·²è·³è¿‡æˆ–ä½¿ç”¨é»˜è®¤å€¼")
                    local_labels.append(min(self.label_mapping.values()))
            
            local_labels = np.array(local_labels)
            labels = torch.from_numpy(local_labels).to(self.device).long()
        return labels

    def train(self, num_epochs=50, learning_rate=1e-3, weight_decay=1e-4, patience=10, debug=False, resume_from=None, accumulation_steps=1):
        self.optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)
        
        # æ”¹è¿›ï¼šLinear Warmup + Cosine Annealing (æ¯”CosineAnnealingWarmRestartsæ›´ç¨³å®š)
        total_steps = num_epochs * len(self.train_loader)
        warmup_steps = len(self.train_loader) * 2  # å‰2ä¸ªepochåšwarmup
        
        def lr_lambda(current_step):
            if current_step < warmup_steps:
                return float(current_step) / float(max(1, warmup_steps))
            return max(0.0, float(num_epochs - current_step / len(self.train_loader)) / float(max(1, num_epochs)))
        
        scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)
        
        start_epoch = 0
        if resume_from and resume_from.exists():
            checkpoint = torch.load(resume_from)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            start_epoch = checkpoint['epoch'] + 1
            self.logger.info(f"ğŸ”„ ä» Epoch {start_epoch} æ¢å¤è®­ç»ƒ")

        no_improve_count = 0
        
        for epoch in range(start_epoch, num_epochs):
            self.model.train()
            train_loss = 0.0
            train_correct = 0
            train_total = 0
            train_preds = []
            train_labels = []
            
            self.optimizer.zero_grad()
            
            for i, batch in enumerate(self.train_loader):
                if not batch: continue
                
                dyn = batch['dynamic'].to(self.device)
                sta = batch['static'].to(self.device)
                labels = self._get_labels(batch)
                
                # æ··åˆç²¾åº¦ä¸Šä¸‹æ–‡
                autocast_ctx = torch.amp.autocast('cuda') if hasattr(torch.amp, 'autocast') else torch.cuda.amp.autocast()

                # [ä¿®æ­£] é™ä½ Mixup è§¦å‘æ¦‚ç‡ (0.5 -> 0.2) å’Œå¼ºåº¦ï¼Œä»¥å‡è½»æ¬ æ‹Ÿåˆ
                if not debug and np.random.rand() < 0.2:
                    # æ˜¾å¼é™ä½ alpha ä¸º 0.2
                    dyn, sta, targets_a, targets_b, lam = self.mixup_data(dyn, sta, labels, alpha=0.2)
                    with autocast_ctx:
                        outputs = self.model(dyn, sta)
                        loss = lam * self.criterion(outputs['logits'], targets_a) + (1 - lam) * self.criterion(outputs['logits'], targets_b)
                else:
                    with autocast_ctx:
                        outputs = self.model(dyn, sta)
                        loss = self.criterion(outputs['logits'], labels)
                
                loss = loss / accumulation_steps
                self.scaler.scale(loss).backward()
                
                if (i + 1) % accumulation_steps == 0:
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                    self.optimizer.zero_grad()
                
                train_loss += loss.item() * accumulation_steps
                with torch.no_grad():
                    preds = torch.argmax(outputs['logits'], dim=1)
                    train_correct += (preds == labels).sum().item()
                    train_total += labels.size(0)
                    train_preds.extend(preds.cpu().numpy())
                    train_labels.extend(labels.cpu().numpy())
                
                if debug and i >= 5: break
            
            # æ›´æ–°å­¦ä¹ ç‡
            current_lr = self.optimizer.param_groups[0]['lr']
            scheduler.step()
            
            avg_train_loss = train_loss / len(self.train_loader)
            avg_train_acc = train_correct / train_total if train_total > 0 else 0.0
            train_f1 = f1_score(train_labels, train_preds, average='macro', zero_division=0)
            val_metrics = self.evaluate(self.val_loader)
            
            self.history['train_loss'].append(avg_train_loss)
            self.history['train_acc'].append(avg_train_acc)
            self.history['train_f1'].append(train_f1)
            self.history['val_loss'].append(val_metrics['loss'])
            self.history['val_acc'].append(val_metrics['accuracy'])
            self.history['val_f1'].append(val_metrics['f1_macro'])

            history_path = self.output_dir / "training_history.json"
            with open(history_path, 'w', encoding='utf-8') as f:
                json.dump(self.history, f, indent=4)
            
            log_msg = (
                f"Epoch {epoch+1}/{num_epochs} [LR={current_lr:.6f}]: "
                f"Train Loss={avg_train_loss:.4f} | "
                f"Train Acc={avg_train_acc:.4f} | "
                f"Train F1={train_f1:.4f} | "
                f"Val Loss={val_metrics['loss']:.4f} | "
                f"Val Acc={val_metrics['accuracy']:.4f} | "
                f"Val F1={val_metrics['f1_macro']:.4f}"
            )

            if self.verbose:
                self.logger.info(log_msg)
            
            if val_metrics['f1_macro'] > self.best_val_f1:
                self.best_val_f1 = val_metrics['f1_macro']
                self.best_epoch = epoch + 1
                no_improve_count = 0
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'best_f1': self.best_val_f1
                }, self.output_dir / "best_model.pth")
                self.logger.info(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (F1: {self.best_val_f1:.4f})")
            else:
                no_improve_count += 1
                
            if no_improve_count >= patience:
                self.logger.info(f"ğŸ›‘ Early stopping at epoch {epoch+1}")
                break
                
        return self.history

    def evaluate(self, dataloader):
        self.model.eval()
        total_loss = 0.0
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            for batch in dataloader:
                if not batch: continue
                dyn = batch['dynamic'].to(self.device)
                sta = batch['static'].to(self.device)
                labels = self._get_labels(batch)
                
                outputs = self.model(dyn, sta)
                loss = self.criterion(outputs['logits'], labels)
                total_loss += loss.item()
                
                probs = outputs['probabilities']
                preds = torch.argmax(probs, dim=1)
                
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        if len(all_labels) == 0:
            return {'loss': 0, 'accuracy': 0, 'f1_macro': 0, 'preds': [], 'labels': []}

        accuracy = np.mean(np.array(all_preds) == np.array(all_labels))
        f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)
        
        return {
            'loss': total_loss / len(dataloader) if len(dataloader) > 0 else 0,
            'accuracy': accuracy,
            'f1_macro': f1,
            'preds': all_preds,
            'labels': all_labels
        }
    
    def test(self):
        if self.test_loader is None:
            self.logger.warning("âš ï¸ æ²¡æœ‰æä¾›æµ‹è¯•é›† DataLoader")
            return {}

        best_path = self.output_dir / "best_model.pth"
        if best_path.exists():
            checkpoint = torch.load(best_path)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.logger.info(f"ğŸ§ª åŠ è½½æœ€ä½³æ¨¡å‹ (Epoch {checkpoint['epoch']+1}) è¿›è¡Œæµ‹è¯•")
        
        metrics = self.evaluate(self.test_loader)
        cm = confusion_matrix(metrics['labels'], metrics['preds'])
        np.save(self.output_dir / "confusion_matrix.npy", cm)

        report = classification_report(metrics['labels'], metrics['preds'], digits=4, zero_division=0)
        print("\nTest Report:")
        print(report)
        self.logger.info("\nTest Report:\n" + report)

        return metrics
    
    # ============================================================================
    # Stratified K-Fold äº¤å‰éªŒè¯
    # ============================================================================
    def train_with_kfold(self, 
                         dataset, 
                         num_epochs=50, 
                         learning_rate=1e-3, 
                         weight_decay=1e-4, 
                         patience=10,
                         n_splits=5,
                         random_state=42,
                         debug=False,
                         accumulation_steps=1,
                         batch_size=None):
        """
        ä½¿ç”¨ Stratified K-Fold äº¤å‰éªŒè¯è®­ç»ƒæ¨¡å‹
        
        å‚æ•°:
            dataset: å®Œæ•´çš„æ•°æ®é›† (PointTimeSeriesDataset)
            num_epochs: æ¯ä¸€æŠ˜çš„è®­ç»ƒè½®æ•°
            learning_rate: å­¦ä¹ ç‡
            weight_decay: æƒé‡è¡°å‡
            patience: æ—©åœè€å¿ƒå€¼
            n_splits: KæŠ˜æ•°ï¼ˆé»˜è®¤5ï¼‰
            random_state: éšæœºç§å­
            debug: è°ƒè¯•æ¨¡å¼
            accumulation_steps: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
            batch_size: æ‰¹å¤§å°ï¼ˆå¦‚æœä¸ºNoneï¼Œä»self.train_loaderè·å–ï¼‰
            
        è¿”å›:
            kfold_results: åŒ…å«æ‰€æœ‰æŠ˜çš„è®­ç»ƒç»“æœå’Œå¹³å‡æŒ‡æ ‡
        """
        self.logger.info(f"ğŸ”„ å¼€å§‹ Stratified {n_splits}-Fold äº¤å‰éªŒè¯")
        
        # è·å– batch_size
        if batch_size is None:
            if self.train_loader is not None:
                batch_size = self.train_loader.batch_size
            else:
                batch_size = 32  # é»˜è®¤å€¼
        
        # æå–æ‰€æœ‰æ ‡ç­¾ç”¨äºåˆ†å±‚
        all_labels = []
        for idx in range(len(dataset)):
            batch = dataset[idx]
            label = batch[self.target_key]
            if isinstance(label, torch.Tensor):
                label = label.item()
            all_labels.append(label)
        all_labels = np.array(all_labels)
        
        # åˆå§‹åŒ– Stratified K-Fold
        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)
        
        kfold_results = {
            'fold_histories': [],
            'fold_metrics': [],
            'mean_metrics': {},
            'std_metrics': {}
        }
        
        fold_accuracies = []
        fold_f1_scores = []
        fold_losses = []
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(np.arange(len(dataset)), all_labels)):
            self.logger.info(f"\n{'='*60}")
            self.logger.info(f"â³ ç¬¬ {fold+1}/{n_splits} æŠ˜è®­ç»ƒ")
            self.logger.info(f"{'='*60}")
            self.logger.info(f"   è®­ç»ƒé›†å¤§å°: {len(train_idx)}, éªŒè¯é›†å¤§å°: {len(val_idx)}")
            
            # åˆ›å»ºå­é›†
            train_subset = Subset(dataset, train_idx)
            val_subset = Subset(dataset, val_idx)
            
            # åˆ›å»º DataLoader
            train_loader = DataLoader(
                train_subset,
                batch_size=batch_size,
                shuffle=True,
                collate_fn=getattr(dataset, 'collate_fn', None)
            )
            val_loader = DataLoader(
                val_subset,
                batch_size=batch_size,
                shuffle=False,
                collate_fn=getattr(dataset, 'collate_fn', None)
            )
            
            # ä¿å­˜åŸå§‹ DataLoader
            original_train_loader = self.train_loader
            original_val_loader = self.val_loader
            
            # æ›¿æ¢ä¸ºå½“å‰æŠ˜çš„ DataLoader
            self.train_loader = train_loader
            self.val_loader = val_loader
            
            # é‡ç½®æ¨¡å‹å’Œä¼˜åŒ–å™¨
            self.model = self.model.__class__(**self._get_model_init_params()).to(self.device)
            self.best_val_f1 = 0.0
            self.best_epoch = 0
            self.history = {'train_loss': [], 'train_acc': [], 'train_f1': [], 'val_loss': [], 'val_acc': [], 'val_f1': []}
            
            # ä¸ºå½“å‰æŠ˜åˆ›å»ºè¾“å‡ºç›®å½•
            fold_output_dir = self.output_dir / f"fold_{fold+1}"
            fold_output_dir.mkdir(parents=True, exist_ok=True)
            original_output_dir = self.output_dir
            self.output_dir = fold_output_dir
            
            try:
                # è®­ç»ƒå½“å‰æŠ˜
                history = self.train(
                    num_epochs=num_epochs,
                    learning_rate=learning_rate,
                    weight_decay=weight_decay,
                    patience=patience,
                    debug=debug,
                    accumulation_steps=accumulation_steps
                )
                
                # è¯„ä¼°å½“å‰æŠ˜
                best_path = fold_output_dir / "best_model.pth"
                if best_path.exists():
                    checkpoint = torch.load(best_path)
                    self.model.load_state_dict(checkpoint['model_state_dict'])
                
                val_metrics = self.evaluate(val_loader)
                
                fold_accuracies.append(val_metrics['accuracy'])
                fold_f1_scores.append(val_metrics['f1_macro'])
                fold_losses.append(val_metrics['loss'])
                
                fold_result = {
                    'fold': fold + 1,
                    'train_history': history,
                    'val_accuracy': val_metrics['accuracy'],
                    'val_f1': val_metrics['f1_macro'],
                    'val_loss': val_metrics['loss']
                }
                kfold_results['fold_metrics'].append(fold_result)
                kfold_results['fold_histories'].append(history)
                
                self.logger.info(f"âœ… ç¬¬ {fold+1} æŠ˜å®Œæˆ - Acc: {val_metrics['accuracy']:.4f}, F1: {val_metrics['f1_macro']:.4f}")
                
            finally:
                # æ¢å¤åŸå§‹ DataLoader å’Œè¾“å‡ºç›®å½•
                self.train_loader = original_train_loader
                self.val_loader = original_val_loader
                self.output_dir = original_output_dir
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        mean_accuracy = np.mean(fold_accuracies)
        std_accuracy = np.std(fold_accuracies)
        mean_f1 = np.mean(fold_f1_scores)
        std_f1 = np.std(fold_f1_scores)
        mean_loss = np.mean(fold_losses)
        std_loss = np.std(fold_losses)
        
        kfold_results['mean_metrics'] = {
            'accuracy': float(mean_accuracy),
            'accuracy_std': float(std_accuracy),
            'f1_macro': float(mean_f1),
            'f1_macro_std': float(std_f1),
            'loss': float(mean_loss),
            'loss_std': float(std_loss)
        }
        
        kfold_results['std_metrics'] = {
            'accuracy_std': float(std_accuracy),
            'f1_macro_std': float(std_f1),
            'loss_std': float(std_loss)
        }
        
        # ä¿å­˜ K-Fold ç»“æœ
        kfold_results_path = self.output_dir / "kfold_results.json"
        with open(kfold_results_path, 'w', encoding='utf-8') as f:
            # åªä¿å­˜å¯åºåˆ—åŒ–çš„éƒ¨åˆ†
            serializable_results = {
                'fold_metrics': kfold_results['fold_metrics'],
                'mean_metrics': kfold_results['mean_metrics'],
                'std_metrics': kfold_results['std_metrics']
            }
            json.dump(serializable_results, f, indent=4)
        
        # æ‰“å°æœ€ç»ˆç»“æœ
        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"ğŸ¯ K-Fold äº¤å‰éªŒè¯æœ€ç»ˆç»“æœ ({n_splits}-Fold)")
        self.logger.info(f"{'='*60}")
        self.logger.info(f"å¹³å‡å‡†ç¡®ç‡: {mean_accuracy:.4f} Â± {std_accuracy:.4f}")
        self.logger.info(f"å¹³å‡ F1 åˆ†æ•°: {mean_f1:.4f} Â± {std_f1:.4f}")
        self.logger.info(f"å¹³å‡æŸå¤±: {mean_loss:.4f} Â± {std_loss:.4f}")
        self.logger.info(f"{'='*60}\n")
        
        return kfold_results
    
    def _get_model_init_params(self):
        """è·å–æ¨¡å‹åˆå§‹åŒ–å‚æ•°ï¼ˆç”¨äºé‡æ–°åˆå§‹åŒ–ï¼‰"""
        return self.model_init_params

    # ============================================================================
    # Ensemble é¢„æµ‹ï¼ˆç”¨äº K-Fold æ¨¡å‹ï¼‰
    # ============================================================================
    def predict_with_ensemble(self, dataloader, n_splits=5, method='voting'):
        """
        ä½¿ç”¨ K-Fold è®­ç»ƒçš„å¤šä¸ªæ¨¡å‹è¿›è¡Œ Ensemble é¢„æµ‹
        
        å‚æ•°:
            dataloader: é¢„æµ‹æ•°æ®çš„ DataLoader
            n_splits: K-Fold çš„æŠ˜æ•°ï¼Œå¯¹åº”ä¿å­˜çš„æ¨¡å‹æ•°é‡
            method: é¢„æµ‹æ–¹æ³•
                - 'voting': å¤šæ•°æŠ•ç¥¨ï¼ˆåˆ†ç±»é—®é¢˜ï¼‰
                - 'averaging': æ¦‚ç‡å¹³å‡ï¼ˆæ¨èï¼‰
        
        è¿”å›:
            ensemble_preds: é›†æˆåçš„é¢„æµ‹æ ‡ç­¾
            ensemble_probs: é›†æˆåçš„é¢„æµ‹æ¦‚ç‡
            all_fold_probs: æ‰€æœ‰foldçš„é¢„æµ‹æ¦‚ç‡ (n_folds, batch_size, num_classes)
        """
        self.logger.info(f"ğŸ¯ ä½¿ç”¨ {n_splits} ä¸ªæ¨¡å‹è¿›è¡Œ Ensemble é¢„æµ‹ (method={method})")
        
        all_fold_outputs = []  # å­˜å‚¨æ‰€æœ‰foldçš„è¾“å‡º
        
        for fold_idx in range(1, n_splits + 1):
            fold_dir = self.output_dir / f"fold_{fold_idx}"
            model_path = fold_dir / "best_model.pth"
            
            if not model_path.exists():
                self.logger.warning(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
                continue
            
            # åŠ è½½å½“å‰foldçš„æ¨¡å‹
            checkpoint = torch.load(model_path, map_location=self.device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            
            # è¿›è¡Œé¢„æµ‹
            self.model.eval()
            fold_probs = []
            fold_preds = []
            
            with torch.no_grad():
                for batch in dataloader:
                    if not batch: continue
                    
                    dyn = batch['dynamic'].to(self.device)
                    sta = batch['static'].to(self.device)
                    
                    outputs = self.model(dyn, sta)
                    probs = outputs['probabilities']  # (batch_size, num_classes)
                    
                    fold_probs.append(probs.cpu().numpy())
            
            fold_probs = np.concatenate(fold_probs, axis=0)  # (total_samples, num_classes)
            all_fold_outputs.append(fold_probs)
            
            self.logger.info(f"âœ… Fold {fold_idx} é¢„æµ‹å®Œæˆ")
        
        all_fold_outputs = np.array(all_fold_outputs)  # (n_folds, total_samples, num_classes)
        
        if method == 'averaging':
            # æ¦‚ç‡å¹³å‡
            ensemble_probs = np.mean(all_fold_outputs, axis=0)  # (total_samples, num_classes)
        elif method == 'voting':
            # å¤šæ•°æŠ•ç¥¨
            fold_preds = np.argmax(all_fold_outputs, axis=2)  # (n_folds, total_samples)
            ensemble_preds_list = []
            for sample_idx in range(fold_preds.shape[1]):
                votes = fold_preds[:, sample_idx]
                # è·å–æŠ•ç¥¨æœ€å¤šçš„æ ‡ç­¾
                unique, counts = np.unique(votes, return_counts=True)
                ensemble_preds_list.append(unique[np.argmax(counts)])
            
            ensemble_preds = np.array(ensemble_preds_list)
            # è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼ˆone-hotï¼‰
            ensemble_probs = np.zeros((ensemble_preds.shape[0], self.num_classes))
            for i, pred in enumerate(ensemble_preds):
                ensemble_probs[i, pred] = 1.0
        else:
            raise ValueError(f"Unknown ensemble method: {method}")
        
        # ä»æ¦‚ç‡è·å–é¢„æµ‹æ ‡ç­¾
        ensemble_preds = np.argmax(ensemble_probs, axis=1)
        
        self.logger.info(f"ğŸ¯ Ensemble é¢„æµ‹å®Œæˆ")
        
        return ensemble_preds, ensemble_probs, all_fold_outputs
    
    def evaluate_with_ensemble(self, dataloader, n_splits=5, method='averaging'):
        """
        ä½¿ç”¨ Ensemble æ¨¡å‹åœ¨éªŒè¯/æµ‹è¯•é›†ä¸Šè¿›è¡Œè¯„ä¼°
        
        å‚æ•°:
            dataloader: éªŒè¯æ•°æ®çš„ DataLoader
            n_splits: K-Fold çš„æŠ˜æ•°
            method: é¢„æµ‹æ–¹æ³•ï¼ˆ'voting' æˆ– 'averaging'ï¼‰
        
        è¿”å›:
            metrics: åŒ…å«å‡†ç¡®ç‡ã€F1ç­‰æŒ‡æ ‡çš„å­—å…¸
            predictions: é¢„æµ‹ç»“æœï¼ˆåŒ…å«æ ‡ç­¾å’Œæ¦‚ç‡ï¼‰
        """
        ensemble_preds, ensemble_probs, _ = self.predict_with_ensemble(dataloader, n_splits, method)
        
        # æ”¶é›†çœŸå®æ ‡ç­¾
        all_labels = []
        with torch.no_grad():
            for batch in dataloader:
                if not batch: continue
                labels = self._get_labels(batch)
                all_labels.extend(labels.cpu().numpy())
        
        all_labels = np.array(all_labels)
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = np.mean(ensemble_preds == all_labels)
        f1 = f1_score(all_labels, ensemble_preds, average='macro', zero_division=0)
        
        metrics = {
            'accuracy': accuracy,
            'f1_macro': f1,
            'preds': ensemble_preds,
            'probs': ensemble_probs,
            'labels': all_labels
        }
        
        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"ğŸ¯ Ensemble æ¨¡å‹è¯„ä¼°ç»“æœ ({method} æ–¹æ³•)")
        self.logger.info(f"{'='*60}")
        self.logger.info(f"å‡†ç¡®ç‡: {accuracy:.4f}")
        self.logger.info(f"F1 åˆ†æ•°: {f1:.4f}")
        self.logger.info(f"{'='*60}\n")
        
        return metrics