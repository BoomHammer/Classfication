import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
from pathlib import Path
import logging
import numpy as np
from sklearn.metrics import classification_report, f1_score, confusion_matrix
from sklearn.model_selection import StratifiedKFold
import json

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
from pathlib import Path
import logging
import numpy as np
from sklearn.metrics import classification_report, f1_score, confusion_matrix
from sklearn.model_selection import StratifiedKFold
import json

# ============================================================================
# æ ‡ç­¾å¹³æ»‘äº¤å‰ç†µæŸå¤± (Label Smoothing) - æ”¯æŒè¯­ä¹‰åˆ†å‰²
# ============================================================================
class LabelSmoothingLoss(nn.Module):
    """
    å¸¦æ ‡ç­¾å¹³æ»‘çš„äº¤å‰ç†µæŸå¤±
    å‡å°‘æ¨¡å‹å¯¹é¢„æµ‹çš„è¿‡åº¦è‡ªä¿¡ï¼Œæé«˜æ³›åŒ–æ€§èƒ½
    æ”¯æŒ 2D (åˆ†ç±») å’Œ 4D (åˆ†å‰²) è¾“å…¥
    """
    def __init__(self, num_classes, smoothing=0.1, reduction='mean', weight=None, device='cuda'):
        super().__init__()
        self.num_classes = num_classes
        self.smoothing = smoothing
        self.reduction = reduction
        self.device = device
        if weight is not None:
            self.weight = weight.to(device) if isinstance(weight, torch.Tensor) else torch.tensor(weight, device=device, dtype=torch.float)
        else:
            self.weight = None
    
    def forward(self, pred, target):
        """
        pred: (B, C) logits æˆ– (B, C, H, W) logits
        target: (B,) target æˆ– (B, H, W) target
        """
        # å¤„ç†åˆ†å‰²ä»»åŠ¡çš„ç»´åº¦: (B, C, H, W) -> (N, C)
        if pred.dim() == 4:
            # [ä¿®å¤] ä½¿ç”¨ reshape æ›¿ä»£ view ä»¥å…¼å®¹éè¿ç»­å¼ é‡
            pred = pred.permute(0, 2, 3, 1).contiguous().view(-1, self.num_classes)
            target = target.reshape(-1)
            
        pred = pred.log_softmax(dim=-1)
        
        with torch.no_grad():
            # åˆ›å»ºå¹³æ»‘çš„targetåˆ†å¸ƒ
            true_dist = torch.zeros_like(pred)
            true_dist.fill_(self.smoothing / (self.num_classes - 1))
            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)
        
        # è®¡ç®—KLæ•£åº¦
        loss = torch.sum(-true_dist * pred, dim=-1)
        
        # åº”ç”¨ç±»åˆ«æƒé‡
        if self.weight is not None:
            weight_t = self.weight[target]
            weight_t = weight_t / (weight_t.max() + 1e-8)
            loss = loss * weight_t
        
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:
            return loss

# ============================================================================
# Focal Loss å®šä¹‰
# ============================================================================
class FocalLoss(nn.Module):
    """
    Focal Loss: é™ä½æ˜“åˆ†æ ·æœ¬æƒé‡ï¼Œå…³æ³¨éš¾åˆ†æ ·æœ¬
    """
    def __init__(self, alpha=None, gamma=2.0, reduction='mean', device='cuda'):
        super(FocalLoss, self).__init__()
        self.gamma = gamma
        self.reduction = reduction
        self.device = device
        
        if alpha is not None:
            if isinstance(alpha, (list, np.ndarray)):
                self.alpha = torch.tensor(alpha, dtype=torch.float32).to(device)
            else:
                self.alpha = alpha.to(device)
        else:
            self.alpha = None

    def forward(self, inputs, targets):
        # inputs: (B, C, H, W) or (B, C)
        # targets: (B, H, W) or (B)
        
        # è®¡ç®—äº¤å‰ç†µ (reduction='none' ä¿ç•™ç»´åº¦)
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        
        # é˜²æ­¢æ•°å€¼æº¢å‡º
        ce_loss = torch.clamp(ce_loss, min=1e-6, max=100.0)
        
        pt = torch.exp(-ce_loss)
        focal_loss = (1 - pt) ** self.gamma * ce_loss
        
        # åº”ç”¨æƒé‡
        if self.alpha is not None:
            # alpha[targets] ä¼šè‡ªåŠ¨å¤„ç† broadcast
            weight_t = self.alpha[targets]
            weight_t = weight_t / (weight_t.max() + 1e-8)
            focal_loss = focal_loss * weight_t

        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

# ============================================================================
# Trainer ç±»
# ============================================================================
class Trainer:
    def __init__(
        self,
        model: nn.Module,
        train_dataloader: DataLoader,
        val_dataloader: DataLoader,
        test_dataloader: DataLoader = None,
        num_classes: int = 2,
        device: str = 'cuda',
        output_dir: str = './output',
        class_weights: torch.Tensor = None,
        target_key: str = 'label',
        verbose: bool = True,
        label_mapping: dict = None,
        use_focal_loss: bool = True,
        label_smoothing: float = 0.1,
        model_init_params: dict = None
    ):
        self.model = model.to(device)
        self.model_init_params = model_init_params or {}
        self.train_loader = train_dataloader
        self.val_loader = val_dataloader
        self.test_loader = test_dataloader
        self.num_classes = num_classes
        self.device = device
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.verbose = verbose
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)
        self.target_key = target_key
        self.label_mapping = label_mapping

        # æ—¥å¿— Handler
        if not any(isinstance(h, logging.FileHandler) for h in self.logger.handlers):
            log_path = self.output_dir / "train.log"
            file_handler = logging.FileHandler(log_path, encoding='utf-8')
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            file_handler.setFormatter(formatter)
            self.logger.addHandler(file_handler)
        
        # æŸå¤±å‡½æ•°é€‰æ‹©
        if use_focal_loss:
            self.logger.info(f" ğŸ”§ä½¿ç”¨ Focal Loss (æ ‡ç­¾å¹³æ»‘={label_smoothing})")
            self.criterion = FocalLoss(alpha=class_weights, gamma=2.0, device=device)
        else:
            self.logger.info(f"ğŸ”§ ä½¿ç”¨ CrossEntropy Loss (æ ‡ç­¾å¹³æ»‘={label_smoothing})")
            self.criterion = LabelSmoothingLoss(num_classes=num_classes, smoothing=label_smoothing, weight=class_weights, device=device)
            
        self.scaler = torch.amp.GradScaler('cuda') if torch.cuda.is_available() else None
        if hasattr(torch.amp, 'GradScaler'):
             self.scaler = torch.amp.GradScaler('cuda')

        self.optimizer = None
        self.best_val_f1 = 0.0
        self.history = {'train_loss': [], 'train_acc': [], 'train_f1': [], 'val_loss': [], 'val_acc': [], 'val_f1': []}

    def mixup_data(self, x_dyn, x_sta, y, alpha=0.4):
        if alpha > 0:
            lam = np.random.beta(alpha, alpha)
        else:
            lam = 1
        batch_size = x_dyn.size(0)
        index = torch.randperm(batch_size).to(self.device)
        mixed_dyn = lam * x_dyn + (1 - lam) * x_dyn[index, :]
        mixed_sta = lam * x_sta + (1 - lam) * x_sta[index, :]
        y_a, y_b = y, y[index]
        return mixed_dyn, mixed_sta, y_a, y_b, lam

    def _get_labels(self, batch):
        labels = batch[self.target_key].to(self.device)
        if self.label_mapping:
            cpu_labels = labels.cpu().numpy()
            local_labels = []
            for x in cpu_labels:
                if x in self.label_mapping:
                    local_labels.append(self.label_mapping[x])
                else:
                    local_labels.append(min(self.label_mapping.values()))
            local_labels = np.array(local_labels)
            labels = torch.from_numpy(local_labels).to(self.device).long()
        return labels
    
    def _expand_labels_if_needed(self, logits, labels):
        """
        å¦‚æœæ¨¡å‹è¾“å‡ºæ˜¯ 4D (åˆ†å‰²å›¾)ï¼Œè€Œæ ‡ç­¾æ˜¯ 1D (æ ‡é‡)ï¼Œ
        åˆ™å°†æ ‡ç­¾æ‰©å±•ä¸º 3D (B, H, W)
        """
        if logits.dim() == 4 and labels.dim() == 1:
            B, C, H, W = logits.shape
            # æ‰©å±•æ ‡ç­¾: [B] -> [B, 1, 1] -> [B, H, W]
            return labels.view(B, 1, 1).expand(B, H, W)
        return labels

    def train(self, num_epochs=50, learning_rate=1e-3, weight_decay=1e-4, patience=10, debug=False, resume_from=None, accumulation_steps=1):
        self.optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)
        
        # Linear Warmup + Cosine Annealing
        total_steps = num_epochs * len(self.train_loader)
        warmup_steps = len(self.train_loader) * 2
        
        def lr_lambda(current_step):
            if current_step < warmup_steps:
                return float(current_step) / float(max(1, warmup_steps))
            return max(0.0, float(num_epochs - current_step / len(self.train_loader)) / float(max(1, num_epochs)))
        
        scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)
        
        start_epoch = 0
        if resume_from and resume_from.exists():
            checkpoint = torch.load(resume_from, weights_only=True)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            start_epoch = checkpoint['epoch'] + 1
            self.logger.info(f"ğŸ”„ ä» Epoch {start_epoch} æ¢å¤è®­ç»ƒ")

        no_improve_count = 0
        
        for epoch in range(start_epoch, num_epochs):
            self.model.train()
            train_loss = 0.0
            train_correct = 0
            train_total = 0 
            train_preds = []
            train_labels = []
            
            self.optimizer.zero_grad()
            
            for i, batch in enumerate(self.train_loader):
                if not batch: continue
                
                dyn = batch['dynamic'].to(self.device)
                sta = batch['static'].to(self.device)
                labels = self._get_labels(batch)
                
                autocast_ctx = torch.amp.autocast('cuda') if hasattr(torch.amp, 'autocast') else torch.cuda.amp.autocast()

                if not debug and np.random.rand() < 0.2:
                    # Mixup
                    dyn, sta, targets_a, targets_b, lam = self.mixup_data(dyn, sta, labels, alpha=0.2)
                    with autocast_ctx:
                        outputs = self.model(dyn, sta)
                        logits = outputs['logits']
                        
                        targets_a = self._expand_labels_if_needed(logits, targets_a)
                        targets_b = self._expand_labels_if_needed(logits, targets_b)
                        
                        loss = lam * self.criterion(logits, targets_a) + (1 - lam) * self.criterion(logits, targets_b)
                else:
                    with autocast_ctx:
                        outputs = self.model(dyn, sta)
                        logits = outputs['logits']
                        
                        labels = self._expand_labels_if_needed(logits, labels)
                        
                        loss = self.criterion(logits, labels)
                
                loss = loss / accumulation_steps
                self.scaler.scale(loss).backward()
                
                if (i + 1) % accumulation_steps == 0:
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                    self.optimizer.zero_grad()
                
                train_loss += loss.item() * accumulation_steps
                
                with torch.no_grad():
                    logits = outputs['logits']
                    preds = torch.argmax(logits, dim=1)
                    
                    if preds.dim() == 3 and labels.dim() == 1:
                         # ä»…ç”¨äºç»Ÿè®¡ï¼šå°† label æ‰©å±•æ¥å¯¹æ¯”
                         labels_exp = labels.view(-1, 1, 1).expand_as(preds)
                         train_correct += (preds == labels_exp).sum().item()
                         train_total += preds.numel()
                    else:
                         train_correct += (preds == labels).sum().item()
                         train_total += labels.numel()
                    
                if debug and i >= 5: break
            
            # æ›´æ–°å­¦ä¹ ç‡
            current_lr = self.optimizer.param_groups[0]['lr']
            scheduler.step()
            
            avg_train_loss = train_loss / len(self.train_loader)
            avg_train_acc = train_correct / train_total if train_total > 0 else 0.0
            
            val_metrics = self.evaluate(self.val_loader)
            
            self.history['train_loss'].append(avg_train_loss)
            self.history['train_acc'].append(avg_train_acc)
            self.history['val_loss'].append(val_metrics['loss'])
            self.history['val_acc'].append(val_metrics['accuracy'])
            self.history['val_f1'].append(val_metrics['f1_macro'])

            history_path = self.output_dir / "training_history.json"
            with open(history_path, 'w', encoding='utf-8') as f:
                json.dump(self.history, f, indent=4)
            
            log_msg = (
                f"Epoch {epoch+1}/{num_epochs} [LR={current_lr:.6f}]: "
                f"Train Loss={avg_train_loss:.4f} | "
                f"Train Acc={avg_train_acc:.4f} | "
                f"Val Loss={val_metrics['loss']:.4f} | "
                f"Val Acc={val_metrics['accuracy']:.4f} | "
                f"Val F1={val_metrics['f1_macro']:.4f}"
            )

            if self.verbose:
                self.logger.info(log_msg)
            
            if val_metrics['f1_macro'] > self.best_val_f1:
                self.best_val_f1 = val_metrics['f1_macro']
                self.best_epoch = epoch + 1
                no_improve_count = 0
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'best_f1': self.best_val_f1
                }, self.output_dir / "best_model.pth")
                self.logger.info(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (F1: {self.best_val_f1:.4f})")
            else:
                no_improve_count += 1
                
            if no_improve_count >= patience:
                self.logger.info(f"ğŸ›‘ Early stopping at epoch {epoch+1}")
                break
                
        return self.history

    def evaluate(self, dataloader):
        self.model.eval()
        total_loss = 0.0
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            for batch in dataloader:
                if not batch: continue
                dyn = batch['dynamic'].to(self.device)
                sta = batch['static'].to(self.device)
                labels = self._get_labels(batch)
                
                outputs = self.model(dyn, sta)
                logits = outputs['logits']
                
                # [ä¿®å¤] æ£€æŸ¥å¹¶æ‰©å±•æ ‡ç­¾
                labels_spatial = self._expand_labels_if_needed(logits, labels)
                
                loss = self.criterion(logits, labels_spatial)
                total_loss += loss.item()
                
                probs = outputs['probabilities']
                preds = torch.argmax(probs, dim=1) # (B, H, W)
                
                # æ”¶é›†ç»“æœç”¨äºè®¡ç®— Metrics
                if preds.dim() == 3:
                     if labels.dim() == 1:
                        labels = labels.view(-1, 1, 1).expand_as(preds)
                     
                     # [å…³é”®ä¿®å¤] ä½¿ç”¨ reshape è€Œä¸æ˜¯ viewï¼Œå› ä¸º expand åçš„å¼ é‡åœ¨ flatten æ—¶å¯èƒ½ä¸è¿ç»­
                     all_preds.extend(preds.reshape(-1).cpu().numpy())
                     all_labels.extend(labels.reshape(-1).cpu().numpy())
                else:
                     all_preds.extend(preds.cpu().numpy())
                     all_labels.extend(labels.cpu().numpy())
        
        if len(all_labels) == 0:
            return {'loss': 0, 'accuracy': 0, 'f1_macro': 0, 'preds': [], 'labels': []}

        # è½¬ä¸º numpy
        y_true = np.array(all_labels)
        y_pred = np.array(all_preds)
        
        accuracy = np.mean(y_true == y_pred)
        f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)
        
        return {
            'loss': total_loss / len(dataloader) if len(dataloader) > 0 else 0,
            'accuracy': accuracy,
            'f1_macro': f1,
            'preds': [], 
            'labels': []
        }
    
    def test(self):
        if self.test_loader is None:
            self.logger.warning("âš ï¸ æ²¡æœ‰æä¾›æµ‹è¯•é›† DataLoader")
            return {}

        best_path = self.output_dir / "best_model.pth"
        if best_path.exists():
            checkpoint = torch.load(best_path, weights_only=True)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.logger.info(f"ğŸ§ª åŠ è½½æœ€ä½³æ¨¡å‹ (Epoch {checkpoint['epoch']+1}) è¿›è¡Œæµ‹è¯•")
        
        metrics = self.evaluate(self.test_loader)
        self.logger.info(f"Test Acc: {metrics['accuracy']:.4f}, F1: {metrics['f1_macro']:.4f}")
        return metrics
    
    # ============================================================================
    # Stratified K-Fold äº¤å‰éªŒè¯
    # ============================================================================
    def train_with_kfold(self, 
                         dataset, 
                         num_epochs=50, 
                         learning_rate=1e-3, 
                         weight_decay=1e-4, 
                         patience=10,
                         n_splits=5,
                         random_state=42,
                         debug=False,
                         accumulation_steps=1,
                         batch_size=None):
        self.logger.info(f"ğŸ”„ å¼€å§‹ Stratified {n_splits}-Fold äº¤å‰éªŒè¯")
        
        if batch_size is None:
            batch_size = self.train_loader.batch_size if self.train_loader else 32
        
        all_labels = []
        for idx in range(len(dataset)):
            batch = dataset[idx]
            label = batch[self.target_key]
            if isinstance(label, torch.Tensor):
                label = label.item()
            all_labels.append(label)
        all_labels = np.array(all_labels)
        
        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)
        
        kfold_results = {
            'fold_histories': [],
            'fold_metrics': [],
            'mean_metrics': {},
            'std_metrics': {}
        }
        
        fold_accuracies = []
        fold_f1_scores = []
        fold_losses = []
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(np.arange(len(dataset)), all_labels)):
            self.logger.info(f"\n{'='*60}")
            self.logger.info(f"â³ ç¬¬ {fold+1}/{n_splits} æŠ˜è®­ç»ƒ")
            self.logger.info(f"{'='*60}")
            self.logger.info(f"   è®­ç»ƒé›†å¤§å°: {len(train_idx)}, éªŒè¯é›†å¤§å°: {len(val_idx)}")
            
            train_subset = Subset(dataset, train_idx)
            val_subset = Subset(dataset, val_idx)
            
            train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, collate_fn=getattr(dataset, 'collate_fn', None))
            val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, collate_fn=getattr(dataset, 'collate_fn', None))
            
            original_train_loader = self.train_loader
            original_val_loader = self.val_loader
            self.train_loader = train_loader
            self.val_loader = val_loader
            
            self.model = self.model.__class__(**self._get_model_init_params()).to(self.device)
            self.best_val_f1 = 0.0
            self.history = {'train_loss': [], 'train_acc': [], 'train_f1': [], 'val_loss': [], 'val_acc': [], 'val_f1': []}
            
            fold_output_dir = self.output_dir / f"fold_{fold+1}"
            fold_output_dir.mkdir(parents=True, exist_ok=True)
            original_output_dir = self.output_dir
            self.output_dir = fold_output_dir
            
            try:
                history = self.train(
                    num_epochs=num_epochs,
                    learning_rate=learning_rate,
                    weight_decay=weight_decay,
                    patience=patience,
                    debug=debug,
                    accumulation_steps=accumulation_steps
                )
                
                best_path = fold_output_dir / "best_model.pth"
                if best_path.exists():
                    checkpoint = torch.load(best_path, weights_only=True)
                    self.model.load_state_dict(checkpoint['model_state_dict'])
                
                val_metrics = self.evaluate(val_loader)
                
                fold_accuracies.append(val_metrics['accuracy'])
                fold_f1_scores.append(val_metrics['f1_macro'])
                fold_losses.append(val_metrics['loss'])
                
                fold_result = {
                    'fold': fold + 1,
                    'val_accuracy': val_metrics['accuracy'],
                    'val_f1': val_metrics['f1_macro'],
                    'val_loss': val_metrics['loss']
                }
                kfold_results['fold_metrics'].append(fold_result)
                kfold_results['fold_histories'].append(history)
                
                self.logger.info(f"âœ… ç¬¬ {fold+1} æŠ˜å®Œæˆ - Acc: {val_metrics['accuracy']:.4f}, F1: {val_metrics['f1_macro']:.4f}")
                
            finally:
                self.train_loader = original_train_loader
                self.val_loader = original_val_loader
                self.output_dir = original_output_dir
        
        mean_accuracy = np.mean(fold_accuracies)
        std_accuracy = np.std(fold_accuracies)
        mean_f1 = np.mean(fold_f1_scores)
        std_f1 = np.std(fold_f1_scores)
        mean_loss = np.mean(fold_losses)
        std_loss = np.std(fold_losses)
        
        kfold_results['mean_metrics'] = {
            'accuracy': float(mean_accuracy),
            'accuracy_std': float(std_accuracy),
            'f1_macro': float(mean_f1),
            'f1_macro_std': float(std_f1),
            'loss': float(mean_loss),
            'loss_std': float(std_loss)
        }
        
        kfold_results_path = self.output_dir / "kfold_results.json"
        with open(kfold_results_path, 'w', encoding='utf-8') as f:
            serializable_results = {
                'fold_metrics': kfold_results['fold_metrics'],
                'mean_metrics': kfold_results['mean_metrics']
            }
            json.dump(serializable_results, f, indent=4)
        
        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"ğŸ¯ K-Fold ç»“æœ: Acc {mean_accuracy:.4f}Â±{std_accuracy:.4f}, F1 {mean_f1:.4f}Â±{std_f1:.4f}")
        self.logger.info(f"{'='*60}\n")
        
        return kfold_results
    
    def _get_model_init_params(self):
        return self.model_init_params

    def predict_with_ensemble(self, dataloader, n_splits=5, method='voting'):
        # ç®€å•å ä½
        pass
    # ============================================================================
    # Ensemble é¢„æµ‹ï¼ˆç”¨äº K-Fold æ¨¡å‹ï¼‰
    # ============================================================================
    def predict_with_ensemble(self, dataloader, n_splits=5, method='voting'):
        """
        ä½¿ç”¨ K-Fold è®­ç»ƒçš„å¤šä¸ªæ¨¡å‹è¿›è¡Œ Ensemble é¢„æµ‹
        
        å‚æ•°:
            dataloader: é¢„æµ‹æ•°æ®çš„ DataLoader
            n_splits: K-Fold çš„æŠ˜æ•°ï¼Œå¯¹åº”ä¿å­˜çš„æ¨¡å‹æ•°é‡
            method: é¢„æµ‹æ–¹æ³•
                - 'voting': å¤šæ•°æŠ•ç¥¨ï¼ˆåˆ†ç±»é—®é¢˜ï¼‰
                - 'averaging': æ¦‚ç‡å¹³å‡ï¼ˆæ¨èï¼‰
        
        è¿”å›:
            ensemble_preds: é›†æˆåçš„é¢„æµ‹æ ‡ç­¾
            ensemble_probs: é›†æˆåçš„é¢„æµ‹æ¦‚ç‡
            all_fold_probs: æ‰€æœ‰foldçš„é¢„æµ‹æ¦‚ç‡ (n_folds, batch_size, num_classes)
        """
        self.logger.info(f"ğŸ¯ ä½¿ç”¨ {n_splits} ä¸ªæ¨¡å‹è¿›è¡Œ Ensemble é¢„æµ‹ (method={method})")
        
        all_fold_outputs = []  # å­˜å‚¨æ‰€æœ‰foldçš„è¾“å‡º
        
        for fold_idx in range(1, n_splits + 1):
            fold_dir = self.output_dir / f"fold_{fold_idx}"
            model_path = fold_dir / "best_model.pth"
            
            if not model_path.exists():
                self.logger.warning(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
                continue
            
            # åŠ è½½å½“å‰foldçš„æ¨¡å‹
            checkpoint = torch.load(model_path, map_location=self.device, weights_only=True)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            
            # è¿›è¡Œé¢„æµ‹
            self.model.eval()
            fold_probs = []
            fold_preds = []
            
            with torch.no_grad():
                for batch in dataloader:
                    if not batch: continue
                    
                    dyn = batch['dynamic'].to(self.device)
                    sta = batch['static'].to(self.device)
                    
                    outputs = self.model(dyn, sta)
                    probs = outputs['probabilities']  # (batch_size, num_classes)
                    
                    fold_probs.append(probs.cpu().numpy())
            
            fold_probs = np.concatenate(fold_probs, axis=0)  # (total_samples, num_classes)
            all_fold_outputs.append(fold_probs)
            
            self.logger.info(f"âœ… Fold {fold_idx} é¢„æµ‹å®Œæˆ")
        
        all_fold_outputs = np.array(all_fold_outputs)  # (n_folds, total_samples, num_classes)
        
        if method == 'averaging':
            # æ¦‚ç‡å¹³å‡
            ensemble_probs = np.mean(all_fold_outputs, axis=0)  # (total_samples, num_classes)
        elif method == 'voting':
            # å¤šæ•°æŠ•ç¥¨
            fold_preds = np.argmax(all_fold_outputs, axis=2)  # (n_folds, total_samples)
            ensemble_preds_list = []
            for sample_idx in range(fold_preds.shape[1]):
                votes = fold_preds[:, sample_idx]
                # è·å–æŠ•ç¥¨æœ€å¤šçš„æ ‡ç­¾
                unique, counts = np.unique(votes, return_counts=True)
                ensemble_preds_list.append(unique[np.argmax(counts)])
            
            ensemble_preds = np.array(ensemble_preds_list)
            # è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼ˆone-hotï¼‰
            ensemble_probs = np.zeros((ensemble_preds.shape[0], self.num_classes))
            for i, pred in enumerate(ensemble_preds):
                ensemble_probs[i, pred] = 1.0
        else:
            raise ValueError(f"Unknown ensemble method: {method}")
        
        # ä»æ¦‚ç‡è·å–é¢„æµ‹æ ‡ç­¾
        ensemble_preds = np.argmax(ensemble_probs, axis=1)
        
        self.logger.info(f"ğŸ¯ Ensemble é¢„æµ‹å®Œæˆ")
        
        return ensemble_preds, ensemble_probs, all_fold_outputs
    
    def evaluate_with_ensemble(self, dataloader, n_splits=5, method='averaging'):
        """
        ä½¿ç”¨ Ensemble æ¨¡å‹åœ¨éªŒè¯/æµ‹è¯•é›†ä¸Šè¿›è¡Œè¯„ä¼°
        
        å‚æ•°:
            dataloader: éªŒè¯æ•°æ®çš„ DataLoader
            n_splits: K-Fold çš„æŠ˜æ•°
            method: é¢„æµ‹æ–¹æ³•ï¼ˆ'voting' æˆ– 'averaging'ï¼‰
        
        è¿”å›:
            metrics: åŒ…å«å‡†ç¡®ç‡ã€F1ç­‰æŒ‡æ ‡çš„å­—å…¸
            predictions: é¢„æµ‹ç»“æœï¼ˆåŒ…å«æ ‡ç­¾å’Œæ¦‚ç‡ï¼‰
        """
        ensemble_preds, ensemble_probs, _ = self.predict_with_ensemble(dataloader, n_splits, method)
        
        # æ”¶é›†çœŸå®æ ‡ç­¾
        all_labels = []
        with torch.no_grad():
            for batch in dataloader:
                if not batch: continue
                labels = self._get_labels(batch)
                all_labels.extend(labels.cpu().numpy())
        
        all_labels = np.array(all_labels)
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = np.mean(ensemble_preds == all_labels)
        f1 = f1_score(all_labels, ensemble_preds, average='macro', zero_division=0)
        
        metrics = {
            'accuracy': accuracy,
            'f1_macro': f1,
            'preds': ensemble_preds,
            'probs': ensemble_probs,
            'labels': all_labels
        }
        
        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"ğŸ¯ Ensemble æ¨¡å‹è¯„ä¼°ç»“æœ ({method} æ–¹æ³•)")
        self.logger.info(f"{'='*60}")
        self.logger.info(f"å‡†ç¡®ç‡: {accuracy:.4f}")
        self.logger.info(f"F1 åˆ†æ•°: {f1:.4f}")
        self.logger.info(f"{'='*60}\n")
        
        return metrics