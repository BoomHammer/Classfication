"""
preprocess_dataset.py: ç¦»çº¿æ•°æ®é¢„å¤„ç†è„šæœ¬ (ä¿®å¤ç‰ˆ)
åŠŸèƒ½ï¼šå°†å¤šæºå¼‚æ„ TIFF æ•°æ®å¯¹é½å¹¶è½¬æ¢ä¸º .pt æ–‡ä»¶å­˜å‚¨ï¼Œå¹¶ä¿å­˜å…ƒæ•°æ®ã€‚
"""

import sys
import shutil
import logging
import json
from pathlib import Path
import torch
import numpy as np
import rasterio
from rasterio.windows import Window
from tqdm import tqdm
from collections import defaultdict
from datetime import date

# å¯¼å…¥æœ¬åœ°æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent))
from config_manager import ConfigManager
from label_encoder import LabelEncoder
from raster_crawler import RasterCrawler

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("Preprocessor")

class DataPreprocessor:
    def __init__(self, config_path):
        self.config = ConfigManager(config_path)
        self.output_dir = self.config.get_resolved_path('data_dir') / "processed_tensors"
        self.stats_file = self.config.get_experiment_output_dir() / 'normalization_stats.json'
        
        # åˆå§‹åŒ–ç»„ä»¶
        logger.info("åˆå§‹åŒ–ç»„ä»¶...")
        self.encoder = LabelEncoder(config=self.config)
        self.crawler = RasterCrawler(config=self.config)
        self.points_df = self.encoder.get_geodataframe().reset_index(drop=True)
        
        # è·å–è¶…çº§é€šé“å®šä¹‰
        def_dict = self.crawler.get_super_channel_definition()
        self.channel_map = def_dict['channel_map']
        self.timeline = def_dict['timeline']
        self.num_channels = len(self.channel_map)
        
        # åŠ è½½å½’ä¸€åŒ–ç»Ÿè®¡é‡
        if self.stats_file.exists():
            with open(self.stats_file, 'r', encoding='utf-8') as f:
                self.stats = json.load(f)
        else:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°ç»Ÿè®¡æ–‡ä»¶ï¼Œå°†è·³è¿‡å½’ä¸€åŒ–ï¼å»ºè®®å…ˆè¿è¡Œ stats_calculator.py")
            self.stats = None

    def _get_file_map(self, point_geom):
        """ä¸ºå•ä¸ªç‚¹æ„å»ºæ—¶é—´-æ–‡ä»¶æ˜ å°„"""
        x, y = point_geom.x, point_geom.y
        rasters = self.crawler.find_rasters_by_point(x, y)
        
        daily = defaultdict(dict)
        monthly = defaultdict(dict)
        
        for r in rasters:
            if not r.variable: continue
            if r.is_monthly:
                monthly[(r.date.year, r.date.month)][r.variable] = str(r.filepath)
            elif r.date:
                daily[r.date][r.variable] = str(r.filepath)
        
        aligned_map = {}
        for t, target_date in enumerate(self.timeline):
            step_files = {}
            m_key = (target_date.year, target_date.month)
            for var, ch_idx in self.channel_map.items():
                path = None
                if var in daily.get(target_date, {}):
                    path = daily[target_date][var]
                elif var in monthly.get(m_key, {}):
                    path = monthly[m_key][var]
                if path: step_files[ch_idx] = path
            if step_files: aligned_map[t] = step_files
            
        return aligned_map

    def process_all(self):
        """æ‰§è¡Œé¢„å¤„ç†"""
        patch_size = self.config.get('data_specs.spatial.patch_size', 64)
        max_len = self.config.get('data.max_len', 60)
        
        # æ¸…ç†æ—§æ•°æ®
        if self.output_dir.exists():
            logger.warning(f"æ¸…ç†æ—§æ•°æ®: {self.output_dir}")
            shutil.rmtree(self.output_dir)
        self.output_dir.mkdir(parents=True)

        logger.info(f"å¼€å§‹å¤„ç† {len(self.points_df)} ä¸ªæ ·æœ¬...")
        logger.info(f"ç›®æ ‡: {self.output_dir}")
        logger.info(f"å‚æ•°: Patch={patch_size}, MaxLen={max_len}, Channels={self.num_channels}")

        success_count = 0
        
        for idx, row in tqdm(self.points_df.iterrows(), total=len(self.points_df)):
            file_map = self._get_file_map(row.geometry)
            
            # 1. æ—¶é—´æ­¥ç­›é€‰ä¸æˆªæ–­
            active_steps = sorted(file_map.keys())
            if len(active_steps) > max_len:
                indices = np.linspace(0, len(active_steps)-1, max_len, dtype=int)
                active_steps = [active_steps[i] for i in indices]
            
            T_actual = len(active_steps)
            if T_actual == 0: continue

            dyn_tensor = np.zeros((T_actual, self.num_channels, patch_size, patch_size), dtype=np.float32)
            dates = np.zeros(T_actual, dtype=int)
            
            # 2. è¯»å–æ•°æ®
            for k, t_idx in enumerate(active_steps):
                date_obj = self.timeline[t_idx]
                dates[k] = date_obj.timetuple().tm_yday
                
                step_files = file_map[t_idx]
                for ch_idx, fpath in step_files.items():
                    try:
                        with rasterio.open(fpath) as src:
                            r, c = src.index(row.geometry.x, row.geometry.y)
                            w = Window(c - patch_size//2, r - patch_size//2, patch_size, patch_size)
                            data = src.read(1, window=w, boundless=True, fill_value=0)
                            dyn_tensor[k, ch_idx] = data
                    except: pass

            # 3. å½’ä¸€åŒ–
            if self.stats:
                stats_list = self.stats.get('dynamic_stats', {}).get('channels', [])
                for ch in range(self.num_channels):
                    if ch < len(stats_list):
                        mu = stats_list[ch]['mean']
                        sigma = stats_list[ch]['std']
                        if sigma > 1e-6:
                            dyn_tensor[:, ch] = (dyn_tensor[:, ch] - mu) / sigma

            # 4. ä¿å­˜ä¸º .pt æ–‡ä»¶
            sample_data = {
                'dynamic': torch.from_numpy(dyn_tensor).float(),
                'static': torch.zeros(1, patch_size, patch_size),
                'label': torch.tensor(int(row.detail_label)).long(),
                'major_label': torch.tensor(int(row.major_label)).long(),
                'detail_label': torch.tensor(int(row.detail_label)).long(),
                'dates': torch.from_numpy(dates).long(),
                'coords': (row.geometry.x, row.geometry.y),
                'sample_id': idx
            }
            
            torch.save(sample_data, self.output_dir / f"{idx}.pt")
            success_count += 1

        # [æ–°å¢] ä¿å­˜å…ƒæ•°æ®æ–‡ä»¶ï¼Œä¾› Dataset è¯»å– channel_map
        metadata = {
            'channel_map': self.channel_map,
            'num_channels': self.num_channels,
            'timeline_start': self.timeline[0].isoformat() if self.timeline else None,
            'timeline_end': self.timeline[-1].isoformat() if self.timeline else None
        }
        with open(self.output_dir / "dataset_metadata.json", 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2)
            
        logger.info(f"âœ… é¢„å¤„ç†å®Œæˆï¼æˆåŠŸè½¬æ¢ {success_count} ä¸ªæ ·æœ¬ã€‚")
        logger.info(f"ğŸ“„ å…ƒæ•°æ®å·²ä¿å­˜: dataset_metadata.json")

if __name__ == "__main__":
    config_path = Path(__file__).parent / 'config.yaml'
    preprocessor = DataPreprocessor(str(config_path))
    preprocessor.process_all()