"""
DataLoader: æ•´åˆæ•°æ®åŠ è½½æ¨¡å—

åŠŸèƒ½ï¼š
1. æ•´åˆ ConfigManagerã€LabelEncoderã€RasterCrawler
2. ä¸ºæ¯ä¸ªæ ‡ç­¾ç‚¹å…³è”æ—¶é—´åºåˆ—æ …æ ¼
3. ç”Ÿæˆç”¨äºæ¨¡å‹è®­ç»ƒçš„æ•°æ®ç´¢å¼•
4. æ”¯æŒæ—¶é—´åºåˆ—é‡‡æ ·å’Œæ•°æ®å¢å¼º
"""

import logging
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional

import pandas as pd
import geopandas as gpd
import numpy as np


class DataLoader:
    """
    æ•°æ®åŠ è½½ç±»
    
    æ•´åˆæ‰€æœ‰ç»„ä»¶ï¼š
    1. ConfigManager - é…ç½®ç®¡ç†
    2. LabelEncoder - æ ‡ç­¾ç¼–ç 
    3. RasterCrawler - å½±åƒç´¢å¼•
    
    åŠŸèƒ½ï¼š
    - ä¸ºæ¯ä¸ªæ ‡ç­¾ç‚¹å…³è”æ—¶é—´åºåˆ—æ …æ ¼
    - ç”Ÿæˆè®­ç»ƒæ•°æ®ç´¢å¼•
    - æ”¯æŒæ•°æ®é‡‡æ ·å’Œåˆ’åˆ†
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
        loader = DataLoader(config=config)
        train_index, val_index = loader.create_train_val_split(test_size=0.2)
        sample = loader.get_sample(sample_id=0)
    """
    
    def __init__(
        self,
        config: 'ConfigManager',
        encoder: Optional['LabelEncoder'] = None,
        crawler: Optional['RasterCrawler'] = None,
    ):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        
        Args:
            config: ConfigManager å¯¹è±¡
            encoder: LabelEncoder å¯¹è±¡ï¼ˆå¦‚æœä¸º Noneï¼Œåˆ™åˆ›å»ºæ–°å®ä¾‹ï¼‰
            crawler: RasterCrawler å¯¹è±¡ï¼ˆå¦‚æœä¸º Noneï¼Œåˆ™åˆ›å»ºæ–°å®ä¾‹ï¼‰
        """
        self._setup_logging()
        logger = logging.getLogger(__name__)
        
        self.config = config
        self.output_dir = config.get_experiment_output_dir()
        
        logger.info("ğŸ“Š åˆå§‹åŒ– DataLoader...")
        
        # åˆå§‹åŒ–æˆ–ä½¿ç”¨æä¾›çš„ encoder
        if encoder is None:
            logger.info("ğŸ“ åˆå§‹åŒ– LabelEncoder...")
            try:
                from label_encoder import LabelEncoder
                self.encoder = LabelEncoder(config=config)
            except ImportError:
                error_msg = "âŒ LabelEncoder ä¸å¯ç”¨"
                logger.error(error_msg)
                raise ImportError(error_msg)
        else:
            self.encoder = encoder
            logger.info("âœ… ä½¿ç”¨å·²æœ‰çš„ LabelEncoder")
        
        # åˆå§‹åŒ–æˆ–ä½¿ç”¨æä¾›çš„ crawler
        if crawler is None:
            logger.info("ğŸ“š åˆå§‹åŒ– RasterCrawler...")
            try:
                from raster_crawler import RasterCrawler
                filename_pattern = config.get('data_specs.raster_crawler.filename_pattern')
                self.crawler = RasterCrawler(config=config, filename_pattern=filename_pattern)
            except ImportError:
                logger.warning("âš ï¸  RasterCrawler ä¸å¯ç”¨ï¼Œå°†ä¸èƒ½å…³è”æ—¶é—´åºåˆ—æ …æ ¼")
                self.crawler = None
            except Exception as e:
                logger.warning(f"âš ï¸  RasterCrawler åˆå§‹åŒ–å¤±è´¥: {e}")
                self.crawler = None
        else:
            self.crawler = crawler
            logger.info("âœ… ä½¿ç”¨å·²æœ‰çš„ RasterCrawler")
        
        # è·å–æ ‡ç­¾æ•°æ®
        logger.info("ğŸ“¥ åŠ è½½æ ‡ç­¾æ•°æ®...")
        self.labels_gdf = self.encoder.get_geodataframe()
        self.sample_count = len(self.labels_gdf)
        logger.info(f"âœ… åŠ è½½äº† {self.sample_count} ä¸ªæ ·æœ¬")
        
        # å…³è”æ …æ ¼
        if self.crawler:
            logger.info("ğŸ”— å…³è”æ—¶é—´åºåˆ—æ …æ ¼...")
            self._associate_rasters()
            logger.info("âœ… æ …æ ¼å…³è”å®Œæˆ")
        
        # ç”Ÿæˆè®­ç»ƒç´¢å¼•
        self.train_indices = None
        self.val_indices = None
        
        logger.info("âœ… DataLoader åˆå§‹åŒ–å®Œæˆ")
    
    @staticmethod
    def _setup_logging():
        """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
        if not logging.getLogger(__name__).handlers:
            handler = logging.StreamHandler(sys.stdout)
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logging.getLogger(__name__).addHandler(handler)
            logging.getLogger(__name__).setLevel(logging.INFO)
    
    def _associate_rasters(self):
        """
        ä¸ºæ¯ä¸ªæ ‡ç­¾ç‚¹å…³è”æ—¶é—´åºåˆ—æ …æ ¼
        
        ä½¿ç”¨ RasterCrawler çš„æ‰¹é‡ç´¢å¼•åŠŸèƒ½ï¼ˆé«˜æ•ˆï¼‰
        """
        logger = logging.getLogger(__name__)
        
        # ä¸ºæ¯ä¸ªç‚¹æŸ¥æ‰¾åŒ…å«çš„æ …æ ¼
        logger.info(f"ğŸ” ä¸º {len(self.labels_gdf)} ä¸ªç‚¹æŸ¥è¯¢æ …æ ¼...")
        
        raster_files_list = []
        for idx, row in self.labels_gdf.iterrows():
            x, y = row['x'], row['y']
            rasters = self.crawler.find_rasters_by_point(x, y)
            raster_files = [m.filepath for m in rasters]
            raster_files_list.append(raster_files)
        
        self.labels_gdf['raster_files'] = raster_files_list
        
        # ç»Ÿè®¡å…³è”ç»“æœ
        raster_counts = [len(rf) for rf in raster_files_list]
        logger.info(f"âœ… æ …æ ¼å…³è”ç»Ÿè®¡:")
        logger.info(f"   å¹³å‡æ¯ç‚¹ {np.mean(raster_counts):.1f} ä¸ªæ …æ ¼")
        logger.info(f"   æœ€å¤š {max(raster_counts)} ä¸ªæ …æ ¼")
        logger.info(f"   æœ€å°‘ {min(raster_counts)} ä¸ªæ …æ ¼")
        
        # ç»Ÿè®¡è¦†ç›–æƒ…å†µ
        covered = sum(1 for rc in raster_counts if rc > 0)
        logger.info(f"   {covered}/{len(raster_counts)} ä¸ªç‚¹æœ‰è¦†ç›–æ …æ ¼ ({covered/len(raster_counts)*100:.1f}%)")
    
    def get_sample(self, sample_id: int) -> Dict:
        """
        è·å–å•ä¸ªæ ·æœ¬çš„å®Œæ•´ä¿¡æ¯
        
        Args:
            sample_id: æ ·æœ¬ IDï¼ˆ0-based ç´¢å¼•ï¼‰
        
        Returns:
            Dict: åŒ…å«æ ‡ç­¾ã€åæ ‡ã€æ …æ ¼åˆ—è¡¨çš„å­—å…¸
        
        Example:
            >>> sample = loader.get_sample(0)
            >>> print(sample['detail_label'])
            >>> print(sample['raster_files'])
        """
        if sample_id < 0 or sample_id >= len(self.labels_gdf):
            raise ValueError(f"æ ·æœ¬ ID {sample_id} è¶…å‡ºèŒƒå›´ [0, {len(self.labels_gdf)-1}]")
        
        row = self.labels_gdf.iloc[sample_id]
        
        sample = {
            'sample_id': sample_id,
            'x': row['x'],
            'y': row['y'],
            'detail_class': row.get('detail_class', 'unknown'),
            'major_class': row.get('major_class', 'unknown'),
            'detail_label': int(row['detail_label']),
            'major_label': int(row['major_label']),
        }
        
        # æ·»åŠ æ …æ ¼ä¿¡æ¯
        if 'raster_files' in row:
            raster_files = row['raster_files']
            sample['raster_count'] = len(raster_files)
            sample['raster_files'] = [str(f) for f in raster_files]
        else:
            sample['raster_count'] = 0
            sample['raster_files'] = []
        
        return sample
    
    def get_samples_batch(self, sample_ids: List[int]) -> List[Dict]:
        """
        æ‰¹é‡è·å–æ ·æœ¬ä¿¡æ¯
        
        Args:
            sample_ids: æ ·æœ¬ ID åˆ—è¡¨
        
        Returns:
            List[Dict]: æ ·æœ¬åˆ—è¡¨
        """
        return [self.get_sample(sid) for sid in sample_ids]
    
    def create_train_val_split(
        self,
        test_size: float = 0.2,
        random_state: int = 42,
    ) -> Tuple[List[int], List[int]]:
        """
        åˆ›å»ºè®­ç»ƒ/éªŒè¯é›†åˆ†å‰²
        
        Args:
            test_size: éªŒè¯é›†æ¯”ä¾‹ (0-1)
            random_state: éšæœºç§å­
        
        Returns:
            Tuple: (train_indices, val_indices)
        
        Example:
            >>> train_idx, val_idx = loader.create_train_val_split(test_size=0.2)
            >>> print(f"Train: {len(train_idx)}, Val: {len(val_idx)}")
        """
        logger = logging.getLogger(__name__)
        
        np.random.seed(random_state)
        
        total = len(self.labels_gdf)
        indices = np.arange(total)
        np.random.shuffle(indices)
        
        split_point = int(total * (1 - test_size))
        
        self.train_indices = indices[:split_point].tolist()
        self.val_indices = indices[split_point:].tolist()
        
        logger.info(f"âœ… æ•°æ®åˆ†å‰²å®Œæˆ:")
        logger.info(f"   è®­ç»ƒé›†: {len(self.train_indices)} ({len(self.train_indices)/total*100:.1f}%)")
        logger.info(f"   éªŒè¯é›†: {len(self.val_indices)} ({len(self.val_indices)/total*100:.1f}%)")
        
        return self.train_indices, self.val_indices
    
    def create_class_balanced_split(
        self,
        test_size: float = 0.2,
        random_state: int = 42,
    ) -> Tuple[List[int], List[int]]:
        """
        åˆ›å»ºç±»åˆ«å¹³è¡¡çš„è®­ç»ƒ/éªŒè¯é›†åˆ†å‰²
        
        ç¡®ä¿æ¯ä¸ªç±»åˆ«åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸­çš„æ¯”ä¾‹ä¸€è‡´
        
        Args:
            test_size: éªŒè¯é›†æ¯”ä¾‹
            random_state: éšæœºç§å­
        
        Returns:
            Tuple: (train_indices, val_indices)
        """
        logger = logging.getLogger(__name__)
        
        np.random.seed(random_state)
        
        train_indices = []
        val_indices = []
        
        # æŒ‰ç±»åˆ«åˆ†å‰²
        for major_label in self.labels_gdf['major_label'].unique():
            mask = self.labels_gdf['major_label'] == major_label
            class_indices = self.labels_gdf[mask].index.tolist()
            
            np.random.shuffle(class_indices)
            split_point = int(len(class_indices) * (1 - test_size))
            
            train_indices.extend(class_indices[:split_point])
            val_indices.extend(class_indices[split_point:])
        
        self.train_indices = train_indices
        self.val_indices = val_indices
        
        logger.info(f"âœ… ç±»åˆ«å¹³è¡¡åˆ†å‰²å®Œæˆ:")
        logger.info(f"   è®­ç»ƒé›†: {len(self.train_indices)}")
        logger.info(f"   éªŒè¯é›†: {len(self.val_indices)}")
        
        return self.train_indices, self.val_indices
    
    def get_class_distribution(self, indices: Optional[List[int]] = None) -> Dict:
        """
        è·å–ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡
        
        Args:
            indices: è¦ç»Ÿè®¡çš„æ ·æœ¬ç´¢å¼•ã€‚å¦‚æœä¸º Noneï¼Œåˆ™ç»Ÿè®¡å…¨éƒ¨
        
        Returns:
            Dict: ç±»åˆ«åˆ†å¸ƒä¿¡æ¯
        
        Example:
            >>> dist = loader.get_class_distribution(loader.train_indices)
            >>> print(dist)
        """
        if indices is None:
            subset = self.labels_gdf
        else:
            subset = self.labels_gdf.iloc[indices]
        
        detailed_dist = subset['detail_label'].value_counts().sort_index().to_dict()
        major_dist = subset['major_label'].value_counts().sort_index().to_dict()
        
        # è½¬æ¢æ ‡ç­¾ä¸ºç±»åˆ«åç§°
        detailed_dist_named = {
            self.encoder.label_to_category(k, 'detailed'): v
            for k, v in detailed_dist.items()
        }
        major_dist_named = {
            self.encoder.label_to_category(k, 'major'): v
            for k, v in major_dist.items()
        }
        
        return {
            'total_samples': len(subset),
            'detailed_classes': detailed_dist_named,
            'major_classes': major_dist_named,
        }
    
    def get_coverage_statistics(self) -> Dict:
        """
        è·å–æ …æ ¼è¦†ç›–ç»Ÿè®¡ä¿¡æ¯ï¼ˆä»…å½“æœ‰ crawler æ—¶ï¼‰
        
        Returns:
            Dict: è¦†ç›–ç»Ÿè®¡
        """
        if not self.crawler or 'raster_files' not in self.labels_gdf.columns:
            return {'message': 'æ …æ ¼å…³è”ä¸å¯ç”¨'}
        
        raster_counts = [len(rf) for rf in self.labels_gdf['raster_files']]
        
        return {
            'total_samples': len(self.labels_gdf),
            'covered_samples': sum(1 for rc in raster_counts if rc > 0),
            'coverage_rate': sum(1 for rc in raster_counts if rc > 0) / len(raster_counts),
            'avg_rasters_per_point': np.mean(raster_counts),
            'max_rasters': max(raster_counts),
            'min_rasters': min(raster_counts),
            'raster_distribution': {
                f'{i}_rasters': sum(1 for rc in raster_counts if rc == i)
                for i in range(max(raster_counts) + 1)
            }
        }
    
    def save_index(self, output_path: Optional[Path] = None):
        """
        ä¿å­˜æ•°æ®ç´¢å¼•åˆ° CSV æ–‡ä»¶
        
        Args:
            output_path: è¾“å‡ºè·¯å¾„ã€‚å¦‚æœä¸º Noneï¼Œåˆ™ä¿å­˜åˆ°å®éªŒç›®å½•
        
        Example:
            >>> loader.save_index()
            # ä¿å­˜åˆ° {experiment_output_dir}/data_index.csv
        """
        logger = logging.getLogger(__name__)
        
        if output_path is None:
            output_path = self.output_dir / 'data_index.csv'
        else:
            output_path = Path(output_path)
        
        # å‡†å¤‡è¾“å‡ºæ•°æ®
        export_data = []
        for idx, row in self.labels_gdf.iterrows():
            data = {
                'sample_id': idx,
                'x': row['x'],
                'y': row['y'],
                'detail_class': row.get('detail_class', 'unknown'),
                'major_class': row.get('major_class', 'unknown'),
                'detail_label': int(row['detail_label']),
                'major_label': int(row['major_label']),
            }
            
            if 'raster_files' in row:
                raster_files = row['raster_files']
                data['raster_count'] = len(raster_files)
                data['raster_files'] = '|'.join([str(f) for f in raster_files])
            else:
                data['raster_count'] = 0
                data['raster_files'] = ''
            
            # æ·»åŠ é›†åˆæ ‡è®°
            if self.train_indices and idx in self.train_indices:
                data['split'] = 'train'
            elif self.val_indices and idx in self.val_indices:
                data['split'] = 'val'
            else:
                data['split'] = 'unknown'
            
            export_data.append(data)
        
        # ä¿å­˜ä¸º CSV
        export_df = pd.DataFrame(export_data)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        export_df.to_csv(output_path, index=False, encoding='utf-8')
        
        logger.info(f"ğŸ’¾ æ•°æ®ç´¢å¼•å·²ä¿å­˜: {output_path}")
    
    def get_statistics(self) -> Dict:
        """
        è·å–æ•°æ®åŠ è½½å™¨ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict: ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {
            'total_samples': len(self.labels_gdf),
            'detailed_classes': len(self.encoder.detailed_labels_map),
            'major_classes': len(self.encoder.major_labels_map),
            'class_distribution': self.get_class_distribution(),
        }
        
        if self.crawler:
            stats['coverage_statistics'] = self.get_coverage_statistics()
        
        if self.train_indices is not None:
            stats['train_size'] = len(self.train_indices)
            stats['val_size'] = len(self.val_indices)
        
        return stats
    
    def __repr__(self) -> str:
        """å­—ç¬¦ä¸²è¡¨ç¤º"""
        crawler_status = "âœ…" if self.crawler else "âŒ"
        return (
            f"DataLoader(\n"
            f"  total_samples={len(self.labels_gdf)},\n"
            f"  detailed_classes={len(self.encoder.detailed_labels_map)},\n"
            f"  major_classes={len(self.encoder.major_labels_map)},\n"
            f"  raster_crawler={crawler_status},\n"
            f"  train_size={len(self.train_indices) if self.train_indices else 'None'},\n"
            f"  val_size={len(self.val_indices) if self.val_indices else 'None'}\n"
            f")"
        )


# ============================================================================
# ä½¿ç”¨ç¤ºä¾‹
# ============================================================================

if __name__ == "__main__":
    try:
        from config_manager import ConfigManager
        from label_encoder import LabelEncoder
        
        print("=" * 80)
        print("DataLoader ä½¿ç”¨ç¤ºä¾‹")
        print("=" * 80)
        
        # åˆå§‹åŒ–
        print("\n1ï¸âƒ£  åˆå§‹åŒ– DataLoader...")
        config = ConfigManager('./config.yaml')
        encoder = LabelEncoder(config=config)
        
        # å°è¯•åˆå§‹åŒ– RasterCrawler
        try:
            from raster_crawler import RasterCrawler
            filename_pattern = config.get('data_specs.raster_crawler.filename_pattern')
            crawler = RasterCrawler(config=config, filename_pattern=filename_pattern)
            print("   âœ… RasterCrawler å·²åˆå§‹åŒ–")
        except Exception as e:
            print(f"   âš ï¸  RasterCrawler åˆå§‹åŒ–å¤±è´¥: {e}")
            crawler = None
        
        # åˆ›å»º DataLoader
        loader = DataLoader(config=config, encoder=encoder, crawler=crawler)
        print(f"\n{loader}\n")
        
        # è·å–æ ·æœ¬
        print("2ï¸âƒ£  è·å–æ ·æœ¬ä¿¡æ¯...")
        sample = loader.get_sample(0)
        print(f"   æ ·æœ¬ 0:")
        for key, value in sample.items():
            if key != 'raster_files':
                print(f"      {key}: {value}")
            else:
                print(f"      {key}: {len(value)} ä¸ªæ–‡ä»¶")
        
        # åˆ›å»ºæ•°æ®åˆ†å‰²
        print("\n3ï¸âƒ£  åˆ›å»ºè®­ç»ƒ/éªŒè¯åˆ†å‰²...")
        train_idx, val_idx = loader.create_train_val_split(test_size=0.2)
        
        # è·å–ç±»åˆ«åˆ†å¸ƒ
        print("\n4ï¸âƒ£  è·å–ç±»åˆ«åˆ†å¸ƒ...")
        dist = loader.get_class_distribution()
        print(f"   å…¨éƒ¨: {dist['total_samples']} ä¸ªæ ·æœ¬")
        print(f"   è¯¦ç»†ç±»åˆ«: {dist['detailed_classes']}")
        print(f"   å¤§ç±»: {dist['major_classes']}")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        print("\n5ï¸âƒ£  è·å–ç»Ÿè®¡ä¿¡æ¯...")
        stats = loader.get_statistics()
        print(f"   âœ… ç»Ÿè®¡å®Œæˆ:")
        for key, value in stats.items():
            if key != 'class_distribution' and key != 'coverage_statistics':
                print(f"      {key}: {value}")
        
        # ä¿å­˜ç´¢å¼•
        print("\n6ï¸âƒ£  ä¿å­˜æ•°æ®ç´¢å¼•...")
        loader.save_index()
        print("   âœ… ç´¢å¼•å·²ä¿å­˜")
        
        print("\n" + "=" * 80)
        print("âœ… DataLoader ç¤ºä¾‹å®Œæˆ!")
        print("=" * 80 + "\n")
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
