#!/usr/bin/env python3
"""
train.py: å®Œæ•´çš„è®­ç»ƒæµç¨‹

ã€ç¬¬å…­é˜¶æ®µã€‘è®­ç»ƒå¾ªç¯ä¸æ—¥å¿—ç³»ç»Ÿ

è¯¥è„šæœ¬æ‰§è¡Œï¼š
1. åŠ è½½å·²å‡†å¤‡å¥½çš„æ•°æ®é›†
2. åˆå§‹åŒ–æ¨¡å‹
3. æ‰§è¡Œè®­ç»ƒï¼ˆæ”¯æŒ Debug æ¨¡å¼å¿«é€Ÿè¿‡æ‹Ÿåˆæµ‹è¯•ï¼‰
4. è¯„ä¼°æµ‹è¯•é›†æ€§èƒ½
5. ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š

è¿è¡Œæ–¹å¼ï¼š
    # æ­£å¸¸è®­ç»ƒ
    python train.py
    
    # Debug æ¨¡å¼ï¼ˆå¿«é€ŸéªŒè¯æ¨¡å‹å­¦ä¹ èƒ½åŠ›ï¼‰
    python train.py --debug
    
    # æ–­ç‚¹ç»­è®­
    python train.py --resume_from ./experiments/outputs/.../last_model.pth
    
    # è‡ªå®šä¹‰å‚æ•°
    python train.py --epochs 100 --lr 1e-3 --batch_size 32

è¾“å‡ºæ–‡ä»¶ï¼š
    experiments/outputs/{timestamp}_{experiment_id}/
    â”œâ”€â”€ best_model.pth              # æœ€ä½³æ¨¡å‹æƒé‡
    â”œâ”€â”€ last_model.pth              # æœ€åä¸€ä¸ª checkpoint
    â”œâ”€â”€ training_log.txt            # è®­ç»ƒæ—¥å¿—
    â”œâ”€â”€ training_metrics.json        # è®­ç»ƒæŒ‡æ ‡
    â”œâ”€â”€ confusion_matrix.npy         # æµ‹è¯•é›†æ··æ·†çŸ©é˜µ
    â”œâ”€â”€ training_report.json         # æœ€ç»ˆè®­ç»ƒæŠ¥å‘Š
    â””â”€â”€ model_summary.txt            # æ¨¡å‹ä¿¡æ¯æ±‡æ€»
"""

import sys
import json
import argparse
from pathlib import Path
from datetime import datetime

import torch
from torch.utils.data import DataLoader

# å¯¼å…¥æœ¬åœ°æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent))

from config_manager import ConfigManager
from label_encoder import LabelEncoder
from raster_crawler import RasterCrawler
from point_timeseries_dataset import PointTimeSeriesDataset, collate_fn
from model_architecture import DualStreamSpatio_TemporalFusionNetwork
from trainer import Trainer


# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================

def load_or_prepare_data(config: ConfigManager, force_recompute: bool = False):
    """
    åŠ è½½æˆ–å‡†å¤‡æ•°æ®é›†
    
    Args:
        config: ConfigManager å¯¹è±¡
        force_recompute: æ˜¯å¦å¼ºåˆ¶é‡æ–°è®¡ç®—
    
    Returns:
        (train_loader, val_loader, test_loader, num_classes)
    """
    print("\n" + "=" * 80)
    print("ğŸ“Š åŠ è½½æ•°æ®é›†...")
    print("=" * 80 + "\n")
    
    # ä»ä¹‹å‰çš„é˜¶æ®µæ£€æŸ¥å¿…è¦æ–‡ä»¶
    output_dir = config.get_experiment_output_dir()
    
    required_files = [
        'normalization_stats.json',
        'dataset_info.json',
        'detected_parameters.json',
    ]
    
    for filename in required_files:
        filepath = Path(output_dir) / filename
        if not filepath.exists():
            print(f"âŒ å¿…è¦æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
            print(f"   è¯·å…ˆè¿è¡Œ python main.py å®Œæˆæ•°æ®å‡†å¤‡")
            return None
    
    # åŠ è½½è‡ªåŠ¨æ£€æµ‹çš„å‚æ•°
    with open(Path(output_dir) / 'detected_parameters.json', 'r') as f:
        params = json.load(f)
    
    num_classes = params['num_classes']
    dynamic_channels = params['dynamic_channels']
    static_channels = params['static_channels']
    
    print(f"âœ… è‡ªåŠ¨æ£€æµ‹å‚æ•°:")
    print(f"   - ç±»åˆ«æ•°: {num_classes}")
    print(f"   - åŠ¨æ€é€šé“æ•°: {dynamic_channels}")
    print(f"   - é™æ€é€šé“æ•°: {static_channels}\n")
    
    # åˆå§‹åŒ–æ ‡ç­¾ç¼–ç å™¨å’Œçˆ¬è™«
    encoder = LabelEncoder(config=config)
    
    dynamic_crawler = RasterCrawler(
        config=config,
        raster_dir=config.get_resolved_path('dynamic_images_dir'),
        filename_pattern=config.get('data_specs.raster_crawler.filename_pattern'),
        file_extensions=tuple(config.get('data_specs.raster_crawler.file_extensions', ['.tif', '.tiff', '.jp2'])),
    )
    
    static_crawler = RasterCrawler(
        config=config,
        raster_dir=config.get_resolved_path('static_images_dir'),
        filename_pattern=config.get('data_specs.raster_crawler.filename_pattern'),
        file_extensions=tuple(config.get('data_specs.raster_crawler.file_extensions', ['.tif', '.tiff', '.jp2'])),
    )
    
    # åˆå§‹åŒ–æ•°æ®é›†
    print("åˆå§‹åŒ–æ•°æ®é›†...")
    
    stats_file = Path(output_dir) / 'normalization_stats.json'
    split_ratio = tuple(config.get('train.split_ratio', (0.7, 0.15, 0.15)))
    
    train_dataset = PointTimeSeriesDataset(
        config=config,
        encoder=encoder,
        dynamic_crawler=dynamic_crawler,
        static_crawler=static_crawler,
        stats_file=str(stats_file) if stats_file.exists() else None,
        split='train',
        split_ratio=split_ratio,
        seed=config.get('train.seed', 42),
        cache_metadata=True,
        verbose=False,
    )
    
    val_dataset = PointTimeSeriesDataset(
        config=config,
        encoder=encoder,
        dynamic_crawler=dynamic_crawler,
        static_crawler=static_crawler,
        stats_file=str(stats_file) if stats_file.exists() else None,
        split='val',
        split_ratio=split_ratio,
        seed=config.get('train.seed', 42),
        cache_metadata=True,
        verbose=False,
    )
    
    test_dataset = PointTimeSeriesDataset(
        config=config,
        encoder=encoder,
        dynamic_crawler=dynamic_crawler,
        static_crawler=static_crawler,
        stats_file=str(stats_file) if stats_file.exists() else None,
        split='test',
        split_ratio=split_ratio,
        seed=config.get('train.seed', 42),
        cache_metadata=True,
        verbose=False,
    )
    
    print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆ:")
    print(f"   - è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"   - éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
    print(f"   - æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬\n")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    batch_size = config.get('train.batch_size', 32)
    num_workers = config.get('train.num_workers', 0)
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=True if torch.cuda.is_available() else False,
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=True if torch.cuda.is_available() else False,
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=True if torch.cuda.is_available() else False,
    )
    
    return train_loader, val_loader, test_loader, num_classes, dynamic_channels, static_channels


def create_model(
    num_classes: int,
    dynamic_channels: int,
    static_channels: int,
    config: ConfigManager,
):
    """
    åˆ›å»ºæ¨¡å‹
    
    Args:
        num_classes: ç±»åˆ«æ•°
        dynamic_channels: åŠ¨æ€é€šé“æ•°
        static_channels: é™æ€é€šé“æ•°
        config: é…ç½®å¯¹è±¡
    
    Returns:
        æ¨¡å‹å®ä¾‹
    """
    print("\n" + "=" * 80)
    print("ğŸ—ï¸  æ„å»ºæ¨¡å‹...")
    print("=" * 80 + "\n")
    
    model = DualStreamSpatio_TemporalFusionNetwork(
        in_channels_dynamic=dynamic_channels,
        in_channels_static=static_channels,
        num_classes=num_classes,
        patch_size=config.get('data_specs.spatial.patch_size', 64),
        temporal_steps=12,
        hidden_dim=config.get('model.hidden_dim', 64),
        fusion_dim=config.get('model.fusion_dim', 128),
        dropout=config.get('model.dropout', 0.2),
    )
    
    summary = model.get_model_summary()
    print(f"âœ… æ¨¡å‹æ„å»ºæˆåŠŸ")
    print(f"   - æ¨¡å‹åç§°: {summary['model_name']}")
    print(f"   - æ€»å‚æ•°æ•°: {summary['total_parameters']:,}")
    print(f"   - å¯è®­ç»ƒå‚æ•°: {summary['trainable_parameters']:,}\n")
    
    return model


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='è®­ç»ƒé¥æ„Ÿå½±åƒåˆ†ç±»æ¨¡å‹')
    parser.add_argument('--epochs', type=int, default=50, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=1e-3, help='å­¦ä¹ ç‡')
    parser.add_argument('--weight_decay', type=float, default=1e-4, help='æƒé‡è¡°å‡')
    parser.add_argument('--patience', type=int, default=10, help='æ—©åœè€å¿ƒæ•°')
    parser.add_argument('--debug', action='store_true', help='Debug æ¨¡å¼')
    parser.add_argument('--resume_from', type=str, default=None, help='ä»æŒ‡å®š checkpoint æ¢å¤')
    parser.add_argument('--config', type=str, default='config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # =========================================================================
    # ç¬¬ä¸€æ­¥ï¼šåŠ è½½é…ç½®
    # =========================================================================
    print("\n" + "=" * 80)
    print("ğŸ“‹ åŠ è½½é…ç½®...")
    print("=" * 80 + "\n")
    
    config_path = Path(__file__).parent / args.config
    if not config_path.exists():
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        return 1
    
    config = ConfigManager(str(config_path))
    output_dir = config.get_experiment_output_dir()
    
    print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ")
    print(f"   - è¾“å‡ºç›®å½•: {output_dir}\n")
    
    # =========================================================================
    # ç¬¬äºŒæ­¥ï¼šåŠ è½½æ•°æ®é›†
    # =========================================================================
    data_result = load_or_prepare_data(config)
    if data_result is None:
        return 1
    
    train_loader, val_loader, test_loader, num_classes, dynamic_channels, static_channels = data_result
    
    # =========================================================================
    # ç¬¬ä¸‰æ­¥ï¼šåˆ›å»ºæ¨¡å‹
    # =========================================================================
    model = create_model(num_classes, dynamic_channels, static_channels, config)
    
    # =========================================================================
    # ç¬¬å››æ­¥ï¼šåˆå§‹åŒ–è®­ç»ƒå™¨
    # =========================================================================
    print("\n" + "=" * 80)
    print("ğŸ“ åˆå§‹åŒ–è®­ç»ƒå™¨...")
    print("=" * 80 + "\n")
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    trainer = Trainer(
        model=model,
        train_dataloader=train_loader,
        val_dataloader=val_loader,
        test_dataloader=test_loader,
        num_classes=num_classes,
        device=device,
        output_dir=output_dir,
        verbose=True,
    )
    
    print(f"âœ… è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ\n")
    
    # =========================================================================
    # ç¬¬äº”æ­¥ï¼šæ‰§è¡Œè®­ç»ƒ
    # =========================================================================
    try:
        resume_from = None
        if args.resume_from:
            resume_from = Path(args.resume_from)
        
        history = trainer.train(
            num_epochs=args.epochs,
            learning_rate=args.lr,
            weight_decay=args.weight_decay,
            patience=args.patience,
            debug=args.debug,
            resume_from=resume_from,
        )
        
    except KeyboardInterrupt:
        print("\n\nâ¸ï¸  è®­ç»ƒè¢«ä¸­æ–­")
        return 0
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    # =========================================================================
    # ç¬¬å…­æ­¥ï¼šæµ‹è¯•é›†è¯„ä¼°
    # =========================================================================
    print("\n")
    test_metrics = trainer.test()
    
    # =========================================================================
    # ç¬¬ä¸ƒæ­¥ï¼šç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    # =========================================================================
    print("\n" + "=" * 80)
    print("ğŸ“Š ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...")
    print("=" * 80 + "\n")
    
    final_report = {
        'experiment_info': {
            'timestamp': datetime.now().isoformat(),
            'config_file': str(config_path),
            'output_directory': str(output_dir),
        },
        'model_info': {
            'num_classes': num_classes,
            'dynamic_channels': dynamic_channels,
            'static_channels': static_channels,
        },
        'training_config': {
            'num_epochs': args.epochs,
            'learning_rate': args.lr,
            'weight_decay': args.weight_decay,
            'patience': args.patience,
            'debug_mode': args.debug,
        },
        'dataset_info': {
            'train_size': len(train_loader.dataset),
            'val_size': len(val_loader.dataset),
            'test_size': len(test_loader.dataset),
        },
        'training_history': history,
        'test_metrics': test_metrics,
        'best_model': {
            'epoch': trainer.best_epoch,
            'val_f1_score': float(trainer.best_val_f1),
        }
    }
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = Path(output_dir) / 'training_report.json'
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(final_report, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… æœ€ç»ˆæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "=" * 80)
    print("ğŸ“‹ è®­ç»ƒæ‘˜è¦")
    print("=" * 80 + "\n")
    
    print(f"æ•°æ®é›†:")
    print(f"  - è®­ç»ƒé›†: {len(train_loader.dataset)} æ ·æœ¬")
    print(f"  - éªŒè¯é›†: {len(val_loader.dataset)} æ ·æœ¬")
    print(f"  - æµ‹è¯•é›†: {len(test_loader.dataset)} æ ·æœ¬")
    print(f"\næœ€ä½³æ¨¡å‹:")
    print(f"  - Epoch: {trainer.best_epoch}")
    print(f"  - éªŒè¯ F1-Score: {trainer.best_val_f1:.4f}")
    print(f"\næµ‹è¯•ç»“æœ:")
    print(f"  - Accuracy: {test_metrics.get('accuracy', 0):.4f}")
    print(f"  - F1 (Macro): {test_metrics.get('f1_macro', 0):.4f}")
    print(f"  - F1 (Weighted): {test_metrics.get('f1_weighted', 0):.4f}")
    print(f"  - IoU: {test_metrics.get('iou', 0):.4f}")
    print(f"\nè¾“å‡ºç›®å½•: {output_dir}")
    print(f"\nğŸ“ é‡è¦æ–‡ä»¶:")
    print(f"  - {output_dir}/best_model.pth              (æœ€ä½³æ¨¡å‹)")
    print(f"  - {output_dir}/training_log.txt            (è®­ç»ƒæ—¥å¿—)")
    print(f"  - {output_dir}/training_metrics.json        (è®­ç»ƒæŒ‡æ ‡)")
    print(f"  - {output_dir}/training_report.json         (æœ€ç»ˆæŠ¥å‘Š)")
    print(f"  - {output_dir}/confusion_matrix.npy         (æ··æ·†çŸ©é˜µ)")
    
    print("\n" + "=" * 80)
    print("âœ… è®­ç»ƒå®Œæˆ!")
    print("=" * 80 + "\n")
    
    return 0


if __name__ == '__main__':
    sys.exit(main())
