"""
DataPreprocessor: æ•°æ®é¢„å¤„ç†ä¸éªŒè¯æ¨¡å—

åŠŸèƒ½ï¼š
1. æ•´åˆ ConfigManagerã€LabelEncoderã€RasterCrawler
2. æ‰§è¡Œæ•°æ®è´¨é‡éªŒè¯ï¼ˆåæ ‡æ£€æŸ¥ã€CRSä¸€è‡´æ€§ç­‰ï¼‰
3. ç”Ÿæˆæ•°æ®æ¸…å•æ–‡ä»¶ï¼ˆdata_inventory.csvï¼‰
4. ç”ŸæˆéªŒè¯æŠ¥å‘Šï¼ˆverification_report.jsonï¼‰
5. è¾“å‡ºè¯¦ç»†çš„æ§åˆ¶å°æ—¥å¿—

ä½¿ç”¨ç¤ºä¾‹ï¼š
    preprocessor = DataPreprocessor(config=config)
    preprocessor.run()
"""

import json
import logging
import sys
import traceback
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime

import pandas as pd
import numpy as np


class DataPreprocessor:
    """
    æ•°æ®é¢„å¤„ç†ç±»
    
    æ‰§è¡Œå®Œæ•´çš„æ•°æ®éªŒè¯å’Œæ¸…å•ç”Ÿæˆæµç¨‹
    """
    
    def __init__(
        self,
        config: 'ConfigManager',
        encoder: Optional['LabelEncoder'] = None,
        dynamic_crawler: Optional['RasterCrawler'] = None,
        static_crawler: Optional['RasterCrawler'] = None,
    ):
        """
        åˆå§‹åŒ–æ•°æ®é¢„å¤„ç†å™¨
        
        Args:
            config: ConfigManager å¯¹è±¡
            encoder: LabelEncoder å¯¹è±¡ï¼ˆå¦‚æœä¸º Noneï¼Œåˆ™åˆ›å»ºæ–°å®ä¾‹ï¼‰
            dynamic_crawler: åŠ¨æ€å½±åƒçˆ¬è™«ï¼ˆå¦‚æœä¸º Noneï¼Œåˆ™åˆ›å»ºæ–°å®ä¾‹ï¼‰
            static_crawler: é™æ€å½±åƒçˆ¬è™«ï¼ˆå¦‚æœä¸º Noneï¼Œåˆ™åˆ›å»ºæ–°å®ä¾‹ï¼‰
        """
        self._setup_logging()
        logger = logging.getLogger(__name__)
        
        logger.info("=" * 80)
        logger.info("ğŸš€ æ•°æ®é¢„å¤„ç†å™¨å¯åŠ¨")
        logger.info("=" * 80)
        
        self.config = config
        self.output_dir = config.get_experiment_output_dir()
        
        # åˆå§‹åŒ–å„ç»„ä»¶
        logger.info("\nğŸ“‹ [é˜¶æ®µ1] åˆå§‹åŒ–æ•°æ®å¤„ç†ç»„ä»¶...")
        
        if encoder is None:
            from label_encoder import LabelEncoder
            logger.info("  ğŸ“ åˆå§‹åŒ– LabelEncoder...")
            self.encoder = LabelEncoder(config=config)
        else:
            self.encoder = encoder
            logger.info("  âœ… ä½¿ç”¨å·²æœ‰çš„ LabelEncoder")
        
        if dynamic_crawler is None:
            from raster_crawler import RasterCrawler
            logger.info("  ğŸ“š åˆå§‹åŒ–åŠ¨æ€å½±åƒçˆ¬è™«...")
            try:
                self.dynamic_crawler = RasterCrawler(
                    config=config,
                    raster_dir=config.get_resolved_path('dynamic_images_dir'),
                    filename_pattern=config.get('data_specs.raster_crawler.filename_pattern'),
                    file_extensions=tuple(config.get('data_specs.raster_crawler.file_extensions', ['.tif', '.tiff', '.jp2'])),
                )
            except Exception as e:
                logger.warning(f"  âš ï¸  åŠ¨æ€å½±åƒçˆ¬è™«åˆå§‹åŒ–å¤±è´¥: {e}")
                self.dynamic_crawler = None
        else:
            self.dynamic_crawler = dynamic_crawler
            logger.info("  âœ… ä½¿ç”¨å·²æœ‰çš„åŠ¨æ€å½±åƒçˆ¬è™«")
        
        if static_crawler is None:
            from raster_crawler import RasterCrawler
            logger.info("  ğŸ“š åˆå§‹åŒ–é™æ€å½±åƒçˆ¬è™«...")
            try:
                self.static_crawler = RasterCrawler(
                    config=config,
                    raster_dir=config.get_resolved_path('static_images_dir'),
                    filename_pattern=config.get('data_specs.raster_crawler.filename_pattern'),
                    file_extensions=tuple(config.get('data_specs.raster_crawler.file_extensions', ['.tif', '.tiff', '.jp2'])),
                )
            except Exception as e:
                logger.warning(f"  âš ï¸  é™æ€å½±åƒçˆ¬è™«åˆå§‹åŒ–å¤±è´¥: {e}")
                self.static_crawler = None
        else:
            self.static_crawler = static_crawler
            logger.info("  âœ… ä½¿ç”¨å·²æœ‰çš„é™æ€å½±åƒçˆ¬è™«")
        
        # æ•°æ®å­˜å‚¨
        self.labels_gdf = self.encoder.get_geodataframe()
        self.verification_results = {}
        self.data_inventory = []
        
        logger.info("  âœ… æ‰€æœ‰ç»„ä»¶åˆå§‹åŒ–å®Œæˆ\n")
    
    @staticmethod
    def _setup_logging():
        """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
        if not logging.getLogger(__name__).handlers:
            handler = logging.StreamHandler(sys.stdout)
            formatter = logging.Formatter(
                '[%(levelname)s] %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            )
            handler.setFormatter(formatter)
            logger = logging.getLogger(__name__)
            logger.setLevel(logging.INFO)
            logger.addHandler(handler)
    
    def run(self):
        """
        æ‰§è¡Œå®Œæ•´çš„æ•°æ®é¢„å¤„ç†æµç¨‹
        
        åŒ…æ‹¬ï¼š
        1. æ ‡ç­¾æ•°æ®éªŒè¯
        2. å½±åƒç´¢å¼•éªŒè¯
        3. CRSä¸€è‡´æ€§æ£€æŸ¥
        4. ç”Ÿæˆæ•°æ®æ¸…å•
        5. ç”ŸæˆéªŒè¯æŠ¥å‘Š
        """
        logger = logging.getLogger(__name__)
        
        try:
            # é˜¶æ®µ1ï¼šæ ‡ç­¾å¤„ç†éªŒè¯
            self._verify_labels()
            
            # é˜¶æ®µ2ï¼šåŠ¨æ€å½±åƒéªŒè¯
            if self.dynamic_crawler:
                self._verify_dynamic_rasters()
            
            # é˜¶æ®µ3ï¼šé™æ€å½±åƒéªŒè¯
            if self.static_crawler:
                self._verify_static_rasters()
            
            # é˜¶æ®µ4ï¼šCRSä¸€è‡´æ€§æ£€æŸ¥
            self._verify_crs_consistency()
            
            # é˜¶æ®µ5ï¼šç”Ÿæˆæ•°æ®æ¸…å•
            self._generate_data_inventory()
            
            # é˜¶æ®µ6ï¼šä¿å­˜æŠ¥å‘Š
            self._save_reports()
            
            logger.info("\n" + "=" * 80)
            logger.info("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼")
            logger.info("=" * 80 + "\n")
            
        except Exception as e:
            logger.error(f"\nâŒ æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
            traceback.print_exc()
            raise
    
    def _verify_labels(self):
        """
        [é˜¶æ®µ1] æ ‡ç­¾æ•°æ®éªŒè¯
        
        æ£€æŸ¥é¡¹ï¼š
        1. åŸå§‹ç‚¹æ•°
        2. æœ‰æ•ˆç‚¹æ•°ï¼ˆå‰”é™¤åæ ‡å¼‚å¸¸ï¼‰
        3. ç±»åˆ«åˆ†å¸ƒ
        4. æŠ•å½±ä¿¡æ¯
        """
        logger = logging.getLogger(__name__)
        logger.info("\nğŸ“‹ [é˜¶æ®µ1] æ ‡ç­¾å¤„ç†éªŒè¯...")
        
        # åŸå§‹ç‚¹æ•°
        original_count = len(self.labels_gdf)
        logger.info(f"  åŸå§‹ç‚¹æ•°: {original_count}")
        
        # æ£€æŸ¥åæ ‡æœ‰æ•ˆæ€§
        invalid_coords = []
        for idx, row in self.labels_gdf.iterrows():
            x, y = row['x'], row['y']
            # æ£€æŸ¥NaNå’Œæç«¯å€¼
            if pd.isna(x) or pd.isna(y) or np.isinf(x) or np.isinf(y):
                invalid_coords.append(idx)
        
        valid_count = original_count - len(invalid_coords)
        logger.info(f"  æœ‰æ•ˆç‚¹æ•°: {valid_count} (å‰”é™¤åæ ‡å¼‚å¸¸ç‚¹ {len(invalid_coords)})")
        
        # ç±»åˆ«åˆ†å¸ƒ - ä½¿ç”¨ç¼–ç å™¨çš„é…ç½®åˆ—å
        major_class_col = self.encoder.major_class_col
        detail_class_col = self.encoder.detail_class_col
        
        major_dist = self.labels_gdf[major_class_col].value_counts().to_dict() if major_class_col in self.labels_gdf.columns else {}
        detailed_dist = self.labels_gdf[detail_class_col].value_counts().to_dict() if detail_class_col in self.labels_gdf.columns else {}
        
        logger.info(f"  å¤§ç±»æ•°é‡: {len(major_dist)}")
        logger.info(f"  è¯¦ç»†ç±»åˆ«æ•°: {len(detailed_dist)}")
        
        # ä¿å­˜éªŒè¯ç»“æœ
        self.verification_results['labels'] = {
            'original_count': original_count,
            'valid_count': valid_count,
            'invalid_count': len(invalid_coords),
            'invalid_indices': invalid_coords,
            'major_classes': len(major_dist),
            'detailed_classes': len(detailed_dist),
            'major_distribution': major_dist,
            'detailed_distribution': detailed_dist,
            'target_crs': str(self.labels_gdf.crs),
        }
        
        # ç±»åˆ«æ˜ å°„
        detailed_map = self.encoder.get_detailed_labels_map()
        major_map = self.encoder.get_major_labels_map()
        
        logger.info(f"  ç±»åˆ«æ˜ å°„: {detailed_map} -> å·²ä¿å­˜è‡³ detailed_labels_map.json")
        logger.info(f"  å¤§ç±»æ˜ å°„: {major_map} -> å·²ä¿å­˜è‡³ major_labels_map.json")
        
        logger.info("  âœ… æ ‡ç­¾éªŒè¯å®Œæˆ\n")
    
    def _verify_dynamic_rasters(self):
        """
        [é˜¶æ®µ2] åŠ¨æ€å½±åƒéªŒè¯
        
        æ£€æŸ¥é¡¹ï¼š
        1. æ‰«ææ–‡ä»¶æ•°
        2. æˆåŠŸè§£æçš„æ–‡ä»¶æ•°
        3. æ—¶é—´è·¨åº¦
        4. æŠ•å½±ä¸€è‡´æ€§
        """
        logger = logging.getLogger(__name__)
        logger.info("ğŸ“‹ [é˜¶æ®µ2] åŠ¨æ€å½±åƒç´¢å¼•éªŒè¯...")
        
        if not self.dynamic_crawler:
            logger.warning("  âš ï¸  åŠ¨æ€å½±åƒçˆ¬è™«ä¸å¯ç”¨")
            return
        
        rasters = self.dynamic_crawler.get_all_rasters()
        total_files = len(rasters)
        
        logger.info(f"  æ‰«ææ–‡ä»¶æ•°: {total_files}")
        
        # ç»Ÿè®¡æˆåŠŸè§£æçš„æ–‡ä»¶
        # æˆåŠŸè§£æçš„æ ‡å‡†ï¼šæœ‰ year æˆ– month ä¿¡æ¯ï¼ˆä¸ä¸€å®šè¦æœ‰å…·ä½“æ—¥æœŸï¼‰
        # PR2020.tif è¿”å› date=None, year=2020, month=Noneï¼Œåº”è¯¥è¢«è®¤ä¸ºæ˜¯æˆåŠŸè§£æ
        parsed_count = sum(1 for r in rasters if r.year is not None)
        unparsed_count = total_files - parsed_count
        
        logger.info(f"  æˆåŠŸè§£ææ—¶é—´å…ƒæ•°æ®: {parsed_count} ({unparsed_count}ä¸ªæ–‡ä»¶å‘½åä¸è§„èŒƒè¢«è·³è¿‡)")
        
        # æ—¶é—´è·¨åº¦ï¼ˆåŸºäºæœ‰æ•ˆçš„å¹´ä»½å’Œæ—¥æœŸä¿¡æ¯ï¼‰
        dates = [r.date for r in rasters if r.date is not None]
        if dates:
            min_date = min(dates)
            max_date = max(dates)
            logger.info(f"  æ—¶é—´è·¨åº¦ï¼ˆæ—¥æœŸï¼‰: {min_date.strftime('%Y-%m')} è‡³ {max_date.strftime('%Y-%m')}")
        
        # ç»Ÿè®¡æ‰€æœ‰æœ‰æ•ˆçš„å¹´ä»½
        years = [r.year for r in rasters if r.year is not None]
        if years:
            min_year = min(years)
            max_year = max(years)
            logger.info(f"  å¹´ä»½èŒƒå›´: {min_year} è‡³ {max_year}")
        
        # æŠ•å½±ç»Ÿè®¡
        crs_set = set(r.crs for r in rasters)
        logger.info(f"  æŠ•å½±ç±»å‹: {crs_set}")
        
        self.verification_results['dynamic_rasters'] = {
            'total_files': total_files,
            'parsed_files': parsed_count,
            'unparsed_files': unparsed_count,
            'time_range': {
                'start': min_date.isoformat() if dates else None,
                'end': max_date.isoformat() if dates else None,
            },
            'year_range': {
                'min': min_year if years else None,
                'max': max_year if years else None,
            },
            'crs_distribution': list(crs_set),
        }
        
        logger.info("  âœ… åŠ¨æ€å½±åƒéªŒè¯å®Œæˆ\n")
    
    def _verify_static_rasters(self):
        """
        [é˜¶æ®µ3] é™æ€å½±åƒéªŒè¯
        
        æ£€æŸ¥é¡¹ï¼š
        1. é™æ€å½±åƒæ–‡ä»¶æ•°
        2. æŠ•å½±ä¿¡æ¯
        """
        logger = logging.getLogger(__name__)
        logger.info("ğŸ“‹ [é˜¶æ®µ3] é™æ€å½±åƒç´¢å¼•éªŒè¯...")
        
        if not self.static_crawler:
            logger.warning("  âš ï¸  é™æ€å½±åƒçˆ¬è™«ä¸å¯ç”¨")
            return
        
        rasters = self.static_crawler.get_all_rasters()
        total_files = len(rasters)
        
        logger.info(f"  æ‰«ææ–‡ä»¶æ•°: {total_files}")
        
        # æŠ•å½±ç»Ÿè®¡
        crs_set = set(r.crs for r in rasters)
        logger.info(f"  æŠ•å½±ç±»å‹: {crs_set}")
        
        self.verification_results['static_rasters'] = {
            'total_files': total_files,
            'crs_distribution': list(crs_set),
        }
        
        logger.info("  âœ… é™æ€å½±åƒéªŒè¯å®Œæˆ\n")
    
    def _verify_crs_consistency(self):
        """
        [CRSä¸€è‡´æ€§æ£€æŸ¥ä¸è‡ªåŠ¨æ£€æµ‹]
        
        åŠŸèƒ½ï¼š
        1. ä½¿ç”¨ CRS ç®¡ç†å™¨è‡ªåŠ¨æ£€æµ‹æ‰€æœ‰å½±åƒçš„åæ ‡ç³»
        2. éªŒè¯åæ ‡ç³»ä¸€è‡´æ€§
        3. ç”Ÿæˆè¯¦ç»†çš„ CRS æŠ¥å‘Š
        4. æä¾›é‡æŠ•å½±å»ºè®®
        """
        logger = logging.getLogger(__name__)
        logger.info("ğŸ“‹ [CRSä¸€è‡´æ€§æ£€æŸ¥ä¸è‡ªåŠ¨æ£€æµ‹]...")
        
        from crs_manager import CRSManager
        crs_manager = CRSManager(self.config)
        
        # ç›®æ ‡CRSå’Œé…ç½®
        target_crs = self.config.get('data_specs.spatial.target_crs')
        auto_reproject = self.config.get('data_specs.spatial.auto_reproject', False)
        
        logger.info(f"  ğŸ“ ç›®æ ‡æŠ•å½±: {target_crs}")
        logger.info(f"  ğŸ”„ è‡ªåŠ¨é‡æŠ•å½±: {'å¯ç”¨' if auto_reproject else 'ç¦ç”¨'}")
        
        crs_issues = []
        
        # æ£€æŸ¥åŠ¨æ€å½±åƒ
        if self.dynamic_crawler:
            logger.info("\n  ğŸ“š æ£€æŸ¥åŠ¨æ€å½±åƒåæ ‡ç³»...")
            dynamic_rasters = self.dynamic_crawler.get_all_rasters()
            
            if dynamic_rasters:
                # ä½¿ç”¨ CRS ç®¡ç†å™¨æ£€æµ‹ä¸€è‡´æ€§
                dynamic_filepaths = [r.filepath for r in dynamic_rasters]
                crs_validation = crs_manager.validate_multiple_crs(
                    dynamic_filepaths,
                    verbose=True
                )
                
                # æ£€æŸ¥æ˜¯å¦ä¸ç›®æ ‡ CRS åŒ¹é…
                if not crs_validation['is_consistent']:
                    logger.warning(f"  âš ï¸  åŠ¨æ€å½±åƒåæ ‡ç³»ä¸ä¸€è‡´")
                    for fp in crs_validation['inconsistent_files'][:3]:
                        crs_issues.append({
                            'type': 'dynamic',
                            'file': Path(fp).name,
                            'crs': crs_validation['crs_details'].get(fp, 'UNKNOWN'),
                            'expected': target_crs
                        })
                
                # æ£€æŸ¥ä¸ç›®æ ‡çš„åŒ¹é…
                if crs_validation['most_common_crs'] and crs_validation['most_common_crs'] != target_crs:
                    logger.warning(
                        f"  âš ï¸  åŠ¨æ€å½±åƒåæ ‡ç³» ({crs_validation['most_common_crs']}) "
                        f"ä¸ç›®æ ‡åæ ‡ç³» ({target_crs}) ä¸åŒ¹é…"
                    )
                    
                    if auto_reproject:
                        logger.info(f"  ğŸ”„ å»ºè®®è¿›è¡Œè‡ªåŠ¨é‡æŠ•å½±")
                else:
                    logger.info(f"  âœ… åŠ¨æ€å½±åƒåæ ‡ç³»éªŒè¯é€šè¿‡")
        
        # æ£€æŸ¥é™æ€å½±åƒ
        if self.static_crawler:
            logger.info("\n  ğŸ“š æ£€æŸ¥é™æ€å½±åƒåæ ‡ç³»...")
            static_rasters = self.static_crawler.get_all_rasters()
            
            if static_rasters:
                # ä½¿ç”¨ CRS ç®¡ç†å™¨æ£€æµ‹ä¸€è‡´æ€§
                static_filepaths = [r.filepath for r in static_rasters]
                crs_validation = crs_manager.validate_multiple_crs(
                    static_filepaths,
                    verbose=True
                )
                
                # æ£€æŸ¥æ˜¯å¦ä¸ç›®æ ‡ CRS åŒ¹é…
                if not crs_validation['is_consistent']:
                    logger.warning(f"  âš ï¸  é™æ€å½±åƒåæ ‡ç³»ä¸ä¸€è‡´")
                    for fp in crs_validation['inconsistent_files'][:3]:
                        crs_issues.append({
                            'type': 'static',
                            'file': Path(fp).name,
                            'crs': crs_validation['crs_details'].get(fp, 'UNKNOWN'),
                            'expected': target_crs
                        })
                
                # æ£€æŸ¥ä¸ç›®æ ‡çš„åŒ¹é…
                if crs_validation['most_common_crs'] and crs_validation['most_common_crs'] != target_crs:
                    logger.warning(
                        f"  âš ï¸  é™æ€å½±åƒåæ ‡ç³» ({crs_validation['most_common_crs']}) "
                        f"ä¸ç›®æ ‡åæ ‡ç³» ({target_crs}) ä¸åŒ¹é…"
                    )
                    
                    if auto_reproject:
                        logger.info(f"  ğŸ”„ å»ºè®®è¿›è¡Œè‡ªåŠ¨é‡æŠ•å½±")
                else:
                    logger.info(f"  âœ… é™æ€å½±åƒåæ ‡ç³»éªŒè¯é€šè¿‡")
        
        # ç”ŸæˆæŠ¥å‘Š
        if self.dynamic_crawler:
            self.dynamic_crawler.save_crs_report()
        if self.static_crawler:
            self.static_crawler.save_crs_report()
        
        if crs_issues:
            logger.warning(f"\n  âš ï¸  å‘ç° {len(crs_issues)} ä¸ªåæ ‡ç³»é—®é¢˜")
        else:
            logger.info(f"\n  âœ… æ‰€æœ‰æ–‡ä»¶åæ ‡ç³»éªŒè¯é€šè¿‡")
        
        self.verification_results['crs_consistency'] = {
            'target_crs': target_crs,
            'auto_reproject': auto_reproject,
            'issues_count': len(crs_issues),
            'issues': crs_issues[:10],  # ä¿å­˜å‰10ä¸ªé—®é¢˜
        }
        
        logger.info("  âœ… CRS æ£€æŸ¥å®Œæˆ\n")
    
    def _generate_data_inventory(self):
        """
        [é˜¶æ®µ5] ç”Ÿæˆæ•°æ®æ¸…å•
        
        åˆ›å»º data_inventory.csvï¼ŒåŒ…å«æ‰€æœ‰æ–‡ä»¶çš„è¯¦ç»†ä¿¡æ¯ï¼š
        - æ–‡ä»¶è·¯å¾„
        - æ–‡ä»¶ç±»å‹ï¼ˆåŠ¨æ€/é™æ€ï¼‰
        - è§£æçš„æ—¥æœŸ
        - æŠ•å½±ç³»ç»Ÿ
        - ç©ºé—´èŒƒå›´ï¼ˆmin_x, min_y, max_x, max_yï¼‰
        """
        logger = logging.getLogger(__name__)
        logger.info("ğŸ“‹ [é˜¶æ®µ5] ç”Ÿæˆæ•°æ®æ¸…å•...")
        
        inventory_list = []
        
        # æ·»åŠ åŠ¨æ€å½±åƒ
        if self.dynamic_crawler:
            rasters = self.dynamic_crawler.get_all_rasters()
            for r in rasters:
                inventory_list.append({
                    'file_path': str(r.filepath),
                    'type': 'Dynamic',
                    'filename': r.filename,
                    'parsed_date': r.date.isoformat() if r.date else 'N/A',
                    'year': r.year,
                    'month': r.month,
                    'epsg': r.crs.split(':')[-1] if ':' in str(r.crs) else r.crs,
                    'min_x': r.bounds[0],
                    'min_y': r.bounds[1],
                    'max_x': r.bounds[2],
                    'max_y': r.bounds[3],
                    'width': r.width,
                    'height': r.height,
                    'resolution_x': r.resolution[0],
                    'resolution_y': r.resolution[1],
                })
        
        # æ·»åŠ é™æ€å½±åƒ
        if self.static_crawler:
            rasters = self.static_crawler.get_all_rasters()
            for r in rasters:
                inventory_list.append({
                    'file_path': str(r.filepath),
                    'type': 'Static',
                    'filename': r.filename,
                    'parsed_date': 'N/A',
                    'year': None,
                    'month': None,
                    'epsg': r.crs.split(':')[-1] if ':' in str(r.crs) else r.crs,
                    'min_x': r.bounds[0],
                    'min_y': r.bounds[1],
                    'max_x': r.bounds[2],
                    'max_y': r.bounds[3],
                    'width': r.width,
                    'height': r.height,
                    'resolution_x': r.resolution[0],
                    'resolution_y': r.resolution[1],
                })
        
        self.data_inventory = inventory_list
        
        # åˆ›å»º DataFrame
        inventory_df = pd.DataFrame(inventory_list)
        
        # ä¿å­˜ä¸º CSV
        inventory_path = self.output_dir / 'data_inventory.csv'
        inventory_df.to_csv(inventory_path, index=False, encoding='utf-8-sig')
        
        logger.info(f"  æ–‡ä»¶æ€»æ•°: {len(inventory_list)}")
        logger.info(f"  åŠ¨æ€å½±åƒ: {sum(1 for item in inventory_list if item['type'] == 'Dynamic')}")
        logger.info(f"  é™æ€å½±åƒ: {sum(1 for item in inventory_list if item['type'] == 'Static')}")
        logger.info(f"  âœ… æ•°æ®æ¸…å•å·²ä¿å­˜: {inventory_path}\n")
    
    def _save_reports(self):
        """
        [é˜¶æ®µ6] ä¿å­˜éªŒè¯æŠ¥å‘Š
        
        ç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼š
        1. verification_report.json - å®Œæ•´çš„éªŒè¯æŠ¥å‘Š
        2. data_summary.txt - äººç±»å¯è¯»çš„æ‘˜è¦æŠ¥å‘Š
        """
        logger = logging.getLogger(__name__)
        logger.info("ğŸ“‹ [é˜¶æ®µ6] ä¿å­˜éªŒè¯æŠ¥å‘Š...")
        
        # ä¿å­˜å®Œæ•´çš„éªŒè¯ç»“æœ
        report_path = self.output_dir / 'verification_report.json'
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(self.verification_results, f, indent=2, ensure_ascii=False, default=str)
        logger.info(f"  âœ… éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
        summary_path = self.output_dir / 'data_summary.txt'
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("ğŸ“Š æ•°æ®é¢„å¤„ç†éªŒè¯æŠ¥å‘Š\n")
            f.write("=" * 80 + "\n\n")
            
            # æ ‡ç­¾ç»Ÿè®¡
            if 'labels' in self.verification_results:
                labels = self.verification_results['labels']
                f.write("ã€æ ‡ç­¾æ•°æ®ç»Ÿè®¡ã€‘\n")
                f.write(f"  åŸå§‹ç‚¹æ•°: {labels['original_count']}\n")
                f.write(f"  æœ‰æ•ˆç‚¹æ•°: {labels['valid_count']} (å‰”é™¤å¼‚å¸¸ç‚¹: {labels['invalid_count']})\n")
                f.write(f"  å¤§ç±»æ•°: {labels['major_classes']}\n")
                f.write(f"  è¯¦ç»†ç±»åˆ«æ•°: {labels['detailed_classes']}\n")
                f.write(f"  æŠ•å½±ç³»ç»Ÿ: {labels['target_crs']}\n")
                f.write(f"  å¤§ç±»åˆ†å¸ƒ: {labels['major_distribution']}\n")
                f.write(f"  è¯¦ç»†åˆ†å¸ƒ: {labels['detailed_distribution']}\n\n")
            
            # åŠ¨æ€å½±åƒç»Ÿè®¡
            if 'dynamic_rasters' in self.verification_results:
                dynamic = self.verification_results['dynamic_rasters']
                f.write("ã€åŠ¨æ€å½±åƒç»Ÿè®¡ã€‘\n")
                f.write(f"  æ€»æ–‡ä»¶æ•°: {dynamic['total_files']}\n")
                f.write(f"  æˆåŠŸè§£æ: {dynamic['parsed_files']}\n")
                f.write(f"  è§£æå¤±è´¥: {dynamic['unparsed_files']}\n")
                if dynamic['time_range']['start']:
                    f.write(f"  æ—¶é—´èŒƒå›´: {dynamic['time_range']['start']} ~ {dynamic['time_range']['end']}\n")
                f.write(f"  æŠ•å½±ç±»å‹: {dynamic['crs_distribution']}\n\n")
            
            # é™æ€å½±åƒç»Ÿè®¡
            if 'static_rasters' in self.verification_results:
                static = self.verification_results['static_rasters']
                f.write("ã€é™æ€å½±åƒç»Ÿè®¡ã€‘\n")
                f.write(f"  æ€»æ–‡ä»¶æ•°: {static['total_files']}\n")
                f.write(f"  æŠ•å½±ç±»å‹: {static['crs_distribution']}\n\n")
            
            # CRSä¸€è‡´æ€§æ£€æŸ¥
            if 'crs_consistency' in self.verification_results:
                crs = self.verification_results['crs_consistency']
                f.write("ã€CRSä¸€è‡´æ€§æ£€æŸ¥ã€‘\n")
                f.write(f"  ç›®æ ‡æŠ•å½±: {crs['target_crs']}\n")
                f.write(f"  ä¸ä¸€è‡´æ–‡ä»¶æ•°: {crs['issues_count']}\n")
                if crs['issues']:
                    f.write(f"  é—®é¢˜è¯¦æƒ…:\n")
                    for issue in crs['issues']:
                        f.write(f"    - {issue['file']}: {issue['crs']}\n")
                f.write("\n")
            
            f.write("=" * 80 + "\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("=" * 80 + "\n")
        
        logger.info(f"  âœ… æ‘˜è¦æŠ¥å‘Šå·²ä¿å­˜: {summary_path}")
        logger.info("  âœ… æ‰€æœ‰æŠ¥å‘Šå·²ä¿å­˜\n")


# ============================================================================
# ä½¿ç”¨ç¤ºä¾‹
# ============================================================================

if __name__ == "__main__":
    try:
        from config_manager import ConfigManager
        
        print("=" * 80)
        print("ğŸš€ æ•°æ®é¢„å¤„ç†è„šæœ¬")
        print("=" * 80)
        
        # åˆå§‹åŒ–é…ç½®
        config = ConfigManager('./config.yaml')
        
        # åˆ›å»ºå¹¶è¿è¡Œé¢„å¤„ç†å™¨
        preprocessor = DataPreprocessor(config=config)
        preprocessor.run()
        
        print("\n" + "=" * 80)
        print("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼")
        print("=" * 80)
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        traceback.print_exc()
        sys.exit(1)
