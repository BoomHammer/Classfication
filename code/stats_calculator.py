"""
stats_calculator.py: ç»Ÿè®¡è®¡ç®—å™¨ (ä¿®å¤ç‰ˆ - åŒ…å«é™æ€æ•°æ®å…¨é‡ç»Ÿè®¡)
"""
import json
import logging
import random
import numpy as np
import rasterio
from tqdm import tqdm
from collections import defaultdict
from dataclasses import dataclass
from pathlib import Path

@dataclass
class RunningStats:
    count: int = 0
    mean: float = 0.0
    M2: float = 0.0
    
    def update(self, val_array):
        val_array = val_array.flatten().astype(np.float64)
        n = len(val_array)
        if n == 0: return
        new_mean = np.mean(val_array)
        new_M2 = np.sum((val_array - new_mean)**2)
        delta = new_mean - self.mean
        new_count = self.count + n
        self.M2 += new_M2 + delta**2 * self.count * n / new_count
        self.mean += delta * n / new_count
        self.count = new_count

    @property
    def std(self):
        return np.sqrt(self.M2 / self.count) if self.count > 0 else 0.0

class StatsCalculator:
    def __init__(self, config):
        self.config = config
        self.output_dir = config.get_experiment_output_dir()
        self.logger = logging.getLogger(__name__)
        
        self.dynamic_stats = defaultdict(RunningStats)
        self.static_stats = defaultdict(RunningStats)
        self.dynamic_channel_order = [] 
        self.static_channel_order = []

    def compute_all_stats(self, dynamic_crawler, static_crawler, sampling_rate=0.1):
        self._compute_dynamic(dynamic_crawler, sampling_rate)
        self._compute_static(static_crawler) # é™æ€æ•°æ®ä¸é‡‡æ ·ï¼Œå…¨é‡è®¡ç®—
        self.save_stats()

    def _compute_dynamic(self, crawler, sampling_rate):
        """è®¡ç®—åŠ¨æ€å½±åƒç»Ÿè®¡é‡ (é‡‡æ ·)"""
        def_dict = crawler.get_super_channel_definition()
        self.dynamic_channel_order = sorted(def_dict['channel_map'].keys())
        
        var_files = defaultdict(list)
        for r in crawler.get_all_rasters():
            if r.variable:
                var_files[r.variable].append(r)
        
        self.logger.info(f"ğŸ“Š æ­£åœ¨è®¡ç®—åŠ¨æ€å˜é‡ç»Ÿè®¡é‡ ({len(self.dynamic_channel_order)} ä¸ªå˜é‡)...")
        for i, var_name in enumerate(self.dynamic_channel_order):
            files = var_files.get(var_name, [])
            if not files: continue
            
            k = max(1, int(len(files) * sampling_rate))
            sampled = random.sample(files, k)
            stats = self.dynamic_stats[var_name]
            
            self.logger.info(f"  [{i+1}/{len(self.dynamic_channel_order)}] è®¡ç®— {var_name} (é‡‡æ · {len(sampled)} å¼ )...")
            
            for meta in sampled:
                try:
                    with rasterio.open(meta.filepath) as src:
                        data = src.read(1)
                        valid_data = data[data != 0] # å‡è®¾ 0 æ˜¯ nodata
                        stats.update(valid_data)
                except: pass
            
            # è®¡ç®—å®Œä¸€ä¸ªå˜é‡åï¼Œè¾“å‡ºç»“æœ
            self.logger.info(f"    -> {var_name}: Mean={stats.mean:.4f}, Std={stats.std:.4f}")

    def _compute_static(self, crawler):
        """è®¡ç®—é™æ€å½±åƒç»Ÿè®¡é‡ (å…¨é‡)"""
        # è·å–æ‰€æœ‰é™æ€æ–‡ä»¶
        rasters = crawler.get_all_rasters()
        # é™æ€æ–‡ä»¶é€šå¸¸æŒ‰æ–‡ä»¶åä½œä¸ºå˜é‡å (å¦‚ DEM.tif -> DEM)
        # è¿™é‡Œæˆ‘ä»¬æŒ‰æ–‡ä»¶åæ’åºä»¥ä¿è¯é¡ºåºä¸€è‡´
        rasters.sort(key=lambda x: x.filepath.stem)
        
        self.static_channel_order = [r.filepath.stem for r in rasters]
        self.logger.info(f"ğŸ”ï¸ æ­£åœ¨è®¡ç®—é™æ€å˜é‡ç»Ÿè®¡é‡ ({len(rasters)} ä¸ªæ–‡ä»¶)...")
        
        for r in rasters:
            var_name = r.filepath.stem
            stats = self.static_stats[var_name]
            
            self.logger.info(f"  - è¯»å–å…¨é‡æ–‡ä»¶: {r.filename} ...")
            try:
                with rasterio.open(r.filepath) as src:
                    # é™æ€æ•°æ®å¯èƒ½å¾ˆå¤§ï¼Œåˆ†å—è¯»å–æˆ–è€…è¯»æ•´ä¸ª(å†…å­˜å…è®¸çš„è¯)
                    # è€ƒè™‘åˆ°é™æ€æ•°æ®é€šå¸¸åªæœ‰ä¸€æ™¯ï¼Œå°è¯•ç›´æ¥è¯»å–
                    data = src.read(1)
                    # é™æ€æ•°æ®å¤„ç† nodata (é€šå¸¸ DEM çš„ nodata æ˜¯ -9999 æˆ– -32768)
                    if src.nodata is not None:
                        valid_data = data[data != src.nodata]
                    else:
                        valid_data = data # æ— æ³•åˆ¤æ–­åˆ™å…¨éƒ¨è®¡ç®—
                    
                    # å†æ¬¡è¿‡æ»¤å¯èƒ½çš„å¡«å……å€¼ (å¦‚å¡åº¦ < 0)
                    if 'slope' in var_name.lower():
                        valid_data = valid_data[valid_data >= 0]
                        
                    stats.update(valid_data)
                self.logger.info(f"    {var_name}: Mean={stats.mean:.4f}, Std={stats.std:.4f}")
            except Exception as e:
                self.logger.error(f"    è®¡ç®— {var_name} å¤±è´¥: {e}")

    def save_stats(self, filename='normalization_stats.json'):
        output = {
            "dynamic_stats": {
                "channels": [
                    {"mean": float(self.dynamic_stats[n].mean), 
                     "std": float(self.dynamic_stats[n].std) if self.dynamic_stats[n].std > 1e-6 else 1.0, 
                     "name": n} 
                    for n in self.dynamic_channel_order
                ]
            },
            "static_stats": {
                "channels": [
                    {"mean": float(self.static_stats[n].mean), 
                     "std": float(self.static_stats[n].std) if self.static_stats[n].std > 1e-6 else 1.0, 
                     "name": n} 
                    for n in self.static_channel_order
                ]
            }
        }
        
        with open(self.output_dir / filename, 'w') as f:
            json.dump(output, f, indent=2)
        self.logger.info(f"âœ… ç»Ÿè®¡é‡å·²ä¿å­˜: {filename}")