#!/usr/bin/env python3
"""
main.py: â€œå…ˆå¤§ç±»ï¼Œåå°ç±»â€ åˆ†å±‚è®­ç»ƒæµæ°´çº¿ (é€‚é… Super-Channel ç­–ç•¥äºŒ)

ã€æ ¸å¿ƒå˜æ›´ã€‘
1. ç§»é™¤äº†ç‹¬ç«‹çš„ static_crawlerï¼Œç»Ÿä¸€ä½¿ç”¨æ–°çš„ RasterCrawler è¿›è¡Œå¤šæºå¼‚æ„æ•°æ®ç®¡ç†ã€‚
2. é€‚é…äº†æ–°çš„ PointTimeSeriesDataset æ¥å£ (åŸºäº Super-Channel å¯¹é½)ã€‚
3. é€‚é…äº†æ–°çš„ StatsCalculator æ¥å£ (åŸºäºå˜é‡åèšåˆ)ã€‚
"""

import sys
import json
import logging
import multiprocessing
from pathlib import Path
import torch
from torch.utils.data import DataLoader, Subset
import numpy as np

# å¯¼å…¥æœ¬åœ°æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent))
from config_manager import ConfigManager
from label_encoder import LabelEncoder
from raster_crawler import RasterCrawler
from point_timeseries_dataset import PointTimeSeriesDataset, collate_fn
from model_architecture import DualStreamSpatio_TemporalFusionNetwork
from trainer import Trainer
from stats_calculator import StatsCalculator

def setup_logging():
    logging.basicConfig(level=logging.INFO, format='%(message)s')

def get_subset_indices(dataset, filter_func):
    """
    è¾…åŠ©å‡½æ•°ï¼šéå†æ•°æ®é›†ï¼Œè¿”å›æ»¡è¶³ filter_func æ¡ä»¶çš„å±€éƒ¨ç´¢å¼•åˆ—è¡¨ã€‚
    """
    indices = []
    # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ encoder.get_dataframe() è¿”å›çš„é¡ºåºä¸ dataset.indices çš„å…¨å±€é¡ºåºä¸€è‡´
    # dataset.points_df æ˜¯åœ¨ init æ—¶ copy è¿‡æ¥çš„ï¼Œæ‰€ä»¥ç›´æ¥ç”¨ dataset.points_df æ›´å®‰å…¨
    df = dataset.points_df 
    
    for local_idx, global_idx in enumerate(dataset.indices):
        row = df.iloc[global_idx]
        if filter_func(row):
            indices.append(local_idx) 
    return indices

def main():
    setup_logging()
    print("="*60)
    print("ğŸš€ å¯åŠ¨åˆ†å±‚è®­ç»ƒæµæ°´çº¿ (Super-Channel èåˆç‰ˆ)")
    print("="*60)

    # 1. åŠ è½½é…ç½®
    config = ConfigManager(str(Path(__file__).parent / 'config.yaml'))
    output_dir = config.get_experiment_output_dir()
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è·å–è¶…å‚æ•° (å¸¦é»˜è®¤å€¼)
    major_cfg = config.get('train.major_model', {
        'epochs': 30, 'batch_size': 32, 'learning_rate': 1e-3, 'weight_decay': 1e-4, 'patience': 10
    })
    detail_cfg = config.get('train.detail_model', {
        'epochs': 40, 'batch_size': 16, 'learning_rate': 1e-3, 'weight_decay': 1e-4, 'patience': 10, 'min_samples': 5
    })
    common_cfg = {
        'num_workers': config.get('train.num_workers', 0), # Windowsè°ƒè¯•å»ºè®®è®¾ä¸º0
        'pin_memory': True if torch.cuda.is_available() else False
    }

    print("\nğŸ“‹ è®­ç»ƒé…ç½®:")
    print(f"   [å¤§ç±»] Epochs: {major_cfg['epochs']}, BS: {major_cfg['batch_size']}, LR: {major_cfg['learning_rate']}")
    print(f"   [å°ç±»] Epochs: {detail_cfg['epochs']}, BS: {detail_cfg['batch_size']}, LR: {detail_cfg['learning_rate']}")

    # 2. åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
    encoder = LabelEncoder(config=config)
    
    # [è¯´æ˜] è®­ç»ƒé˜¶æ®µä¸éœ€è¦ crawlerï¼Œå› ä¸ºç›´æ¥è¯»å–é¢„å¤„ç†åçš„ .pt æ–‡ä»¶
    # ä½†å¦‚æœç¼ºå°‘ç»Ÿè®¡æ–‡ä»¶ï¼Œä¸‹é¢ä¼šä¸´æ—¶åˆ›å»ºä¸€ä¸ª crawler æ¥è®¡ç®—
    crawler = None 

    # 3. è‡ªåŠ¨å½’ä¸€åŒ–è®¡ç®— (å¦‚æœä¸å­˜åœ¨)
    stats_file = output_dir / 'normalization_stats.json'
    if not stats_file.exists():
        print("\nğŸ“Š æœªæ£€æµ‹åˆ°ç»Ÿè®¡æ–‡ä»¶ï¼Œæ­£åœ¨ä»åŸå§‹ TIFF è®¡ç®—å…¨å±€ç»Ÿè®¡é‡...")
        print("   (è¿™ä¸€æ­¥å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè®¡ç®—å®Œæˆåå°†æ°¸ä¹…ä¿å­˜)")
        
        # [æ ¸å¿ƒä¿®å¤] ä¸´æ—¶åˆå§‹åŒ–çˆ¬è™«ï¼Œä»…ç”¨äºç»Ÿè®¡è®¡ç®—
        # å¿…é¡»ä¼ å…¥ config æ‰èƒ½æ‰¾åˆ° dynamic_images_dir
        temp_crawler = RasterCrawler(config=config)
        
        calculator = StatsCalculator(config=config)
        # ä½¿ç”¨ temp_crawler è¿›è¡Œè®¡ç®—
        calculator.compute_global_stats(temp_crawler, sampling_rate=0.2) 
        calculator.save_stats('normalization_stats.json')
        
        print("âœ… ç»Ÿè®¡é‡è®¡ç®—å®Œæˆå¹¶ä¿å­˜ã€‚")
        # é‡Šæ”¾å†…å­˜
        del temp_crawler
        del calculator
    else:
        print(f"\nâœ… æ£€æµ‹åˆ°ç»Ÿè®¡æ–‡ä»¶: {stats_file.name}ï¼Œè·³è¿‡è®¡ç®—ã€‚")

    # 4. åˆå§‹åŒ–å…¨é‡æ•°æ®é›†
    print("\nğŸ“¦ åŠ è½½é¢„å¤„ç†æ•°æ®é›†...")
    try:
        full_train_dataset = PointTimeSeriesDataset(config, encoder, crawler=None, split='train')
        full_val_dataset = PointTimeSeriesDataset(config, encoder, crawler=None, split='val')
    except FileNotFoundError as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ: python code/preprocess_dataset.py")
        sys.exit(1)
    
    # è·å–é€šé“æ•°ä¿¡æ¯
    dyn_ch = full_train_dataset.num_channels
    # [æ³¨æ„] ç›®å‰ PointTimeSeriesDataset å¯¹é™æ€æ•°æ®ä½¿ç”¨å ä½ç¬¦ (zeros)ï¼Œé€šé“æ•°ä¸º 1
    # å¦‚æœåç»­ä½ å®Œå–„äº†é™æ€æ•°æ®é€»è¾‘ï¼Œè¿™é‡Œéœ€è¦ä¿®æ”¹
    sta_ch = 1 
    
    print(f"   åŠ¨æ€é€šé“æ•°: {dyn_ch} (åŒ…å«å˜é‡: {list(full_train_dataset.channel_map.keys())})")
    print(f"   é™æ€é€šé“æ•°: {sta_ch} (Placeholder)")

    major_map = encoder.get_major_labels_map()
    hierarchical_map = encoder.get_hierarchical_map()

    # =========================================================================
    # é˜¶æ®µ A: è®­ç»ƒå¤§ç±»æ¨¡å‹ (Major Model)
    # =========================================================================
    print("\n" + "="*60)
    print("ğŸ—ï¸  [é˜¶æ®µ A] è®­ç»ƒå¤§ç±»åˆ†ç±»æ¨¡å‹")
    print("="*60)
    
    major_model_dir = output_dir / "major_model"
    major_model = DualStreamSpatio_TemporalFusionNetwork(
        in_channels_dynamic=dyn_ch,
        in_channels_static=sta_ch,
        num_classes=len(major_map)
    )
    
    major_loader_args = {
        'batch_size': major_cfg['batch_size'],
        'num_workers': common_cfg['num_workers'],
        'pin_memory': common_cfg['pin_memory'],
        'collate_fn': collate_fn
    }
    
    major_trainer = Trainer(
        model=major_model,
        train_dataloader=DataLoader(full_train_dataset, shuffle=True, **major_loader_args),
        val_dataloader=DataLoader(full_val_dataset, shuffle=False, **major_loader_args),
        num_classes=len(major_map),
        target_key='major_label', 
        output_dir=major_model_dir
    )
    
    major_trainer.train(
        num_epochs=major_cfg['epochs'],
        lr=major_cfg['learning_rate'],
        weight_decay=major_cfg['weight_decay'],
        patience=major_cfg['patience']
    )
    print(f"âœ… å¤§ç±»æ¨¡å‹ä¿å­˜äº: {major_model_dir}")

    # =========================================================================
    # é˜¶æ®µ B: è®­ç»ƒå°ç±»æ¨¡å‹ (Detail Models)
    # =========================================================================
    print("\n" + "="*60)
    print("ğŸ—ï¸  [é˜¶æ®µ B] è®­ç»ƒå„åˆ†æ”¯å°ç±»æ¨¡å‹")
    print("="*60)

    detail_loader_args = {
        'batch_size': detail_cfg['batch_size'],
        'num_workers': common_cfg['num_workers'],
        'pin_memory': common_cfg['pin_memory'],
        'collate_fn': collate_fn
    }

    for major_name, major_id in major_map.items():
        print(f"\nğŸ‘‰ å¤„ç†å¤§ç±»: {major_name} (ID: {major_id})")
        
        sub_info = hierarchical_map[major_name]
        detail_classes_map = sub_info['detail_classes']
        num_sub_classes = len(detail_classes_map)
        
        if num_sub_classes <= 1:
            print(f"   âš ï¸ è¯¥å¤§ç±»ä»…æœ‰ {num_sub_classes} ä¸ªå°ç±»ï¼Œè·³è¿‡ã€‚")
            continue

        # æ˜ å°„æ„å»º
        sorted_details = sorted(detail_classes_map.items(), key=lambda x: x[1])
        global_to_local = {gid: lidx for lidx, (_, gid) in enumerate(sorted_details)}
        local_to_global = {lidx: gid for lidx, (_, gid) in enumerate(sorted_details)}
            
        # ç­›é€‰å­é›†
        train_indices = get_subset_indices(full_train_dataset, lambda row: row['major_label'] == major_id)
        val_indices = get_subset_indices(full_val_dataset, lambda row: row['major_label'] == major_id)
        
        print(f"   æ ·æœ¬æ•°: Train {len(train_indices)} | Val {len(val_indices)}")
        
        if len(train_indices) < detail_cfg.get('min_samples', 5):
            print("   âš ï¸ æ ·æœ¬ä¸è¶³ï¼Œè·³è¿‡ã€‚")
            continue

        train_subset = Subset(full_train_dataset, train_indices)
        val_subset = Subset(full_val_dataset, val_indices)
        
        sub_model_dir = output_dir / f"detail_model_{major_id}_{major_name}"
        sub_model = DualStreamSpatio_TemporalFusionNetwork(
            in_channels_dynamic=dyn_ch,
            in_channels_static=sta_ch,
            num_classes=num_sub_classes
        )
        
        sub_trainer = Trainer(
            model=sub_model,
            train_dataloader=DataLoader(train_subset, shuffle=True, **detail_loader_args),
            val_dataloader=DataLoader(val_subset, shuffle=False, **detail_loader_args),
            num_classes=num_sub_classes,
            target_key='detail_label',
            label_mapping=global_to_local,
            output_dir=sub_model_dir
        )
        
        sub_trainer.train(
            num_epochs=detail_cfg['epochs'],
            lr=detail_cfg['learning_rate'],
            weight_decay=detail_cfg['weight_decay'],
            patience=detail_cfg['patience']
        )
        
        # ä¿å­˜æ˜ å°„
        with open(sub_model_dir / 'class_mapping.json', 'w', encoding='utf-8') as f:
            json.dump({
                'major_class': major_name,
                'major_id': major_id,
                'local_to_global_map': local_to_global, 
                'global_to_local_map': global_to_local
            }, f, ensure_ascii=False, indent=2)
            
        print(f"   âœ… {major_name} å°ç±»æ¨¡å‹å®Œæˆã€‚")

    print("\n" + "="*60)
    print("ğŸ‰ è®­ç»ƒæµæ°´çº¿ç»“æŸï¼")
    print("="*60)

if __name__ == '__main__':
    try:
        multiprocessing.set_start_method('spawn', force=True)
    except RuntimeError:
        pass
    main()