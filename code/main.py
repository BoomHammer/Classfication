#!/usr/bin/env python3
"""
main.py: â€œå…ˆå¤§ç±»ï¼Œåå°ç±»â€ åˆ†å±‚è®­ç»ƒæµæ°´çº¿ (ä¿®å¤ BatchNorm å•æ ·æœ¬ Batch é—®é¢˜)
"""

import sys
import json
import logging
import multiprocessing
from pathlib import Path
import torch
from torch.utils.data import DataLoader, Subset
from sklearn.model_selection import StratifiedKFold
import numpy as np

# å¯¼å…¥æœ¬åœ°æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent))
from config_manager import ConfigManager
from label_encoder import LabelEncoder
from raster_crawler import RasterCrawler
from point_timeseries_dataset import PointTimeSeriesDataset, collate_fn
from model_architecture import DualStreamSpatio_TemporalFusionNetwork
from trainer import Trainer
from stats_calculator import StatsCalculator

def setup_logging():
    logging.basicConfig(level=logging.INFO, format='%(message)s')

def get_subset_indices(dataset, filter_func):
    """è¾…åŠ©å‡½æ•°ï¼šéå†æ•°æ®é›†ï¼Œè¿”å›æ»¡è¶³æ¡ä»¶çš„å±€éƒ¨ç´¢å¼•åˆ—è¡¨"""
    indices = []
    df = dataset.points_df 
    for local_idx, global_idx in enumerate(dataset.indices):
        row = df.iloc[global_idx]
        if filter_func(row):
            indices.append(local_idx) 
    return indices

def compute_class_weights(dataset, label_key, num_classes):
    """è®¡ç®—ç±»åˆ«æƒé‡ï¼Œç”¨äºå¤„ç†ç±»ä¸å¹³è¡¡é—®é¢˜
    [æ”¹è¿›] ä½¿ç”¨å¹³è¡¡æƒé‡å…¬å¼ï¼Œå¹¶è¿›è¡Œå½’ä¸€åŒ–é˜²æ­¢lossçˆ†ç‚¸
    æƒé‡å…¬å¼: w_i = (1 - beta) / (1 - beta^{n_i})ï¼Œå…¶ä¸­ beta = (N-1)/N
    """
    class_counts = np.zeros(num_classes)
    
    # å¤„ç† Subset å¯¹è±¡ï¼šæå–åŸå§‹ dataset å’Œç´¢å¼•æ˜ å°„
    if hasattr(dataset, 'dataset'):
        # Subset å¯¹è±¡
        original_dataset = dataset.dataset
        original_indices = original_dataset.indices
        subset_indices = dataset.indices
        df = original_dataset.points_df
        
        # Subsetä¸­çš„indicesæ˜¯original_datasetä¸­çš„å±€éƒ¨ç´¢å¼•
        # éœ€è¦æ˜ å°„åˆ°original_datasetçš„global_idx
        for local_idx in subset_indices:
            global_idx = original_indices[local_idx]
            row = df.iloc[global_idx]
            label = int(row[label_key])
            if 0 <= label < num_classes:
                class_counts[label] += 1
    else:
        # åŸå§‹ Dataset å¯¹è±¡
        df = dataset.points_df
        for local_idx, global_idx in enumerate(dataset.indices):
            row = df.iloc[global_idx]
            label = int(row[label_key])
            if 0 <= label < num_classes:
                class_counts[label] += 1
    
    # [æ”¹è¿›] ä½¿ç”¨å¹³è¡¡æƒé‡å…¬å¼
    total_samples = class_counts.sum()
    
    # æ–¹æ¡ˆ1ï¼šç®€å•åå‘é¢‘ç‡æƒé‡ï¼ˆç¨³å®šç‰ˆæœ¬ï¼‰
    # æƒé‡ = å¹³å‡æ ·æœ¬æ•° / è¯¥ç±»æ ·æœ¬æ•°
    avg_count = total_samples / (num_classes + 1e-6)
    weights = np.ones(num_classes)
    for i in range(num_classes):
        if class_counts[i] > 0:
            weights[i] = avg_count / class_counts[i]
        else:
            weights[i] = 1.0  # ç±»åˆ«ä¸å­˜åœ¨æ—¶è®¾ä¸º1.0
    
    # [å…³é”®ä¿®å¤] å½’ä¸€åŒ–æƒé‡ï¼Œä½¿å¾—å¹³å‡æƒé‡ä¸º1ï¼Œé˜²æ­¢lossè¿‡å¤§
    weights = weights / (weights.mean() + 1e-8)
    
    # [é˜²æŠ¤] é™åˆ¶æƒé‡èŒƒå›´ [0.1, 10.0]ï¼Œé˜²æ­¢æç«¯ä¸å¹³è¡¡ç±»çš„æƒé‡è¿‡å¤§
    weights = np.clip(weights, 0.1, 10.0)
    
    weights = torch.from_numpy(weights).float()
    
    return weights

def main():
    setup_logging()
    print("="*60)
    print("ğŸš€ å¯åŠ¨åˆ†å±‚è®­ç»ƒæµæ°´çº¿ (Fix: BatchNorm Drop Last)")
    print("="*60)

    # 1. åŠ è½½é…ç½®
    config = ConfigManager(str(Path(__file__).parent / 'config.yaml'), create_experiment_dir=True)
    output_dir = config.get_experiment_output_dir()
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è·å–è¶…å‚æ•°
    major_cfg = config.get('train.major_model', {
        'epochs': 30, 'batch_size': 32, 'learning_rate': 1e-3, 'weight_decay': 1e-4, 'patience': 10
    })
    detail_cfg = config.get('train.detail_model', {
        'epochs': 40, 'batch_size': 16, 'learning_rate': 1e-3, 'weight_decay': 1e-4, 'patience': 10, 'min_samples': 5
    })
    common_cfg = {
        'num_workers': config.get('train.num_workers', 0),
        'pin_memory': True if torch.cuda.is_available() else False
    }

    # 2. åˆå§‹åŒ–ç»„ä»¶
    encoder = LabelEncoder(config=config)
    
    # 3. è‡ªåŠ¨å½’ä¸€åŒ–è®¡ç®—
    stats_file = output_dir / 'normalization_stats.json'
    if not stats_file.exists():
        print("\nğŸ“Š æ­£åœ¨è®¡ç®—å…¨å±€ç»Ÿè®¡é‡ (åŠ¨æ€+é™æ€)...")
        dyn_crawler = RasterCrawler(config=config)
        static_crawler = RasterCrawler(
            config=config, 
            raster_dir=config.get_resolved_path('static_images_dir'),
            filename_pattern='.*',
            file_extensions=('.tif', '.tiff')
        )
        calculator = StatsCalculator(config=config)
        calculator.compute_all_stats(dyn_crawler, static_crawler, sampling_rate=0.2) 
        print("âœ… ç»Ÿè®¡é‡è®¡ç®—å®Œæˆã€‚")
        del dyn_crawler, static_crawler, calculator

    # 4. åŠ è½½æ•°æ®é›†
    print("\nğŸ“¦ åŠ è½½é¢„å¤„ç†æ•°æ®é›†...")
    try:
        full_train_dataset = PointTimeSeriesDataset(config, encoder, split='train')
        full_val_dataset = PointTimeSeriesDataset(config, encoder, split='val')
    except FileNotFoundError as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ: python code/preprocess_dataset.py")
        sys.exit(1)
    
    # 5. è·å–é€šé“å‚æ•°
    dyn_ch = full_train_dataset.num_channels
    sta_ch = full_train_dataset.num_static_channels
    
    print(f"   åŠ¨æ€é€šé“æ•°: {dyn_ch}")
    print(f"   é™æ€é€šé“æ•°: {sta_ch}")
    
    if sta_ch == 0:
        print("âš ï¸ è­¦å‘Šï¼šæ£€æµ‹åˆ°é™æ€é€šé“æ•°ä¸º 0ï¼Œè¯·æ£€æŸ¥ preprocess_dataset.py æ˜¯å¦æ­£ç¡®è¯»å–äº†é™æ€æ•°æ®ã€‚")

    major_map = encoder.get_major_labels_map()
    hierarchical_map = encoder.get_hierarchical_map()

    # =========================================================================
    # é˜¶æ®µ A: è®­ç»ƒå¤§ç±»æ¨¡å‹ (Major Model) - K-Fold äº¤å‰éªŒè¯
    # =========================================================================
    print("\n" + "="*60)
    print("ğŸ—ï¸  [é˜¶æ®µ A] è®­ç»ƒå¤§ç±»åˆ†ç±»æ¨¡å‹ (K-Fold äº¤å‰éªŒè¯)")
    print("="*60)
    
    # è¯»å– K-Fold é…ç½®
    kfold_config = config.get('train.kfold', {})
    major_kfold_n_splits = kfold_config.get('n_splits', 5)
    major_kfold_random_state = kfold_config.get('random_state', 42)
    
    # è®¡ç®—å¤§ç±»æƒé‡
    major_weights = compute_class_weights(full_train_dataset, 'major_label', len(major_map))
    print(f"ğŸ“Š å¤§ç±»æƒé‡: {major_weights.tolist()}")
    
    major_label_smoothing = major_cfg.get('label_smoothing', config.get('model.label_smoothing', 0.05))
    major_model_dir = output_dir / "major_model"
    
    print(f"\nğŸ“Š å¯ç”¨ K-Fold äº¤å‰éªŒè¯ (n_splits={major_kfold_n_splits})")
    major_model = DualStreamSpatio_TemporalFusionNetwork(
        in_channels_dynamic=dyn_ch,
        in_channels_static=sta_ch,
        num_classes=len(major_map),
        dropout=config.get('model.dropout', 0.25),
        classifier_hidden_dims=config.get('model.classifier.hidden_dims', [128, 64, 32])
    )
    
    major_trainer = Trainer(
        model=major_model,
        train_dataloader=None,  # K-Fold å†…éƒ¨ä¼šåˆ›å»º
        val_dataloader=None,
        num_classes=len(major_map),
        target_key='major_label',
        output_dir=major_model_dir,
        class_weights=major_weights,
        use_focal_loss=True,
        label_smoothing=major_label_smoothing,
        model_init_params={  # ä¼ å…¥æ¨¡å‹åˆå§‹åŒ–å‚æ•°
            'in_channels_dynamic': dyn_ch,
            'in_channels_static': sta_ch,
            'num_classes': len(major_map),
            'dropout': config.get('model.dropout', 0.25),
            'classifier_hidden_dims': config.get('model.classifier.hidden_dims', [128, 64, 32])
        }
    )
    
    kfold_results = major_trainer.train_with_kfold(
        dataset=full_train_dataset,
        num_epochs=major_cfg['epochs'],
        learning_rate=major_cfg['learning_rate'],
        weight_decay=major_cfg['weight_decay'],
        patience=major_cfg['patience'],
        n_splits=major_kfold_n_splits,
        random_state=major_kfold_random_state,
        debug=False,
        accumulation_steps=1,
        batch_size=major_cfg['batch_size']
    )
    
    print(f"âœ… å¤§ç±»æ¨¡å‹ K-Fold è®­ç»ƒå®Œæˆ")
    print(f"   å¹³å‡ç²¾åº¦: {kfold_results['mean_metrics'].get('accuracy', 0):.4f} Â± {kfold_results['std_metrics'].get('accuracy_std', 0):.4f}")
    print(f"âœ… å¤§ç±»æ¨¡å‹ä¿å­˜äº: {major_model_dir}")

    # =========================================================================
    # é˜¶æ®µ B: è®­ç»ƒå°ç±»æ¨¡å‹ (Detail Models) - K-Fold äº¤å‰éªŒè¯
    # =========================================================================
    print("\n" + "="*60)
    print("ğŸ—ï¸  [é˜¶æ®µ B] è®­ç»ƒå„åˆ†æ”¯å°ç±»æ¨¡å‹ (K-Fold äº¤å‰éªŒè¯)")
    print("="*60)

    for major_name, major_id in major_map.items():
        print(f"\nğŸ‘‰ å¤„ç†å¤§ç±»: {major_name} (ID: {major_id})")
        
        sub_info = hierarchical_map[major_name]
        detail_classes_map = sub_info['detail_classes']
        num_sub_classes = len(detail_classes_map)
        
        if num_sub_classes <= 1:
            print(f"   âš ï¸ è¯¥å¤§ç±»ä»…æœ‰ {num_sub_classes} ä¸ªå°ç±»ï¼Œè·³è¿‡ã€‚")
            continue

        sorted_details = sorted(detail_classes_map.items(), key=lambda x: x[1])
        global_to_local = {gid: lidx for lidx, (_, gid) in enumerate(sorted_details)}
        local_to_global = {lidx: gid for lidx, (_, gid) in enumerate(sorted_details)}
            
        train_indices = get_subset_indices(full_train_dataset, lambda row: row['major_label'] == major_id)
        val_indices = get_subset_indices(full_val_dataset, lambda row: row['major_label'] == major_id)
        
        print(f"   æ ·æœ¬æ•°: Train {len(train_indices)} | Val {len(val_indices)}")
        
        if len(train_indices) < detail_cfg.get('min_samples', 5):
            print("   âš ï¸ æ ·æœ¬ä¸è¶³ï¼Œè·³è¿‡ã€‚")
            continue

        train_subset = Subset(full_train_dataset, train_indices)
        val_subset = Subset(full_val_dataset, val_indices)
        
        # è®¡ç®—å½“å‰å°ç±»çš„ç±»åˆ«æƒé‡
        detail_weights = compute_class_weights(train_subset, 'detail_label', num_sub_classes)
        print(f"   ğŸ“Š å°ç±»æƒé‡: {detail_weights.tolist()}")
        
        detail_label_smoothing = detail_cfg.get('label_smoothing', config.get('model.label_smoothing', 0.1))
        
        sub_model_dir = output_dir / f"detail_model_{major_id}_{major_name}"
        
        # æ£€æŸ¥æ ·æœ¬æ˜¯å¦å……è¶³è¿›è¡Œ K-Fold
        if len(train_indices) >= major_kfold_n_splits:
            # æ ·æœ¬å……è¶³ï¼Œä½¿ç”¨ K-Fold
            print(f"   ğŸ“Š å¯ç”¨ K-Fold äº¤å‰éªŒè¯ (n_splits={major_kfold_n_splits})")
            
            sub_model = DualStreamSpatio_TemporalFusionNetwork(
                in_channels_dynamic=dyn_ch,
                in_channels_static=sta_ch,
                num_classes=num_sub_classes,
                dropout=config.get('model.dropout', 0.25),
                classifier_hidden_dims=config.get('model.classifier.hidden_dims', [128, 64, 32])
            )
            
            sub_trainer = Trainer(
                model=sub_model,
                train_dataloader=None,  # K-Fold å†…éƒ¨ä¼šåˆ›å»º
                val_dataloader=None,
                num_classes=num_sub_classes,
                target_key='detail_label',
                label_mapping=global_to_local,
                output_dir=sub_model_dir,
                class_weights=detail_weights,
                use_focal_loss=True,
                label_smoothing=detail_label_smoothing,
                model_init_params={  # ä¼ å…¥æ¨¡å‹åˆå§‹åŒ–å‚æ•°
                    'in_channels_dynamic': dyn_ch,
                    'in_channels_static': sta_ch,
                    'num_classes': num_sub_classes,
                    'dropout': config.get('model.dropout', 0.25),
                    'classifier_hidden_dims': config.get('model.classifier.hidden_dims', [128, 64, 32])
                }
            )
            
            kfold_results = sub_trainer.train_with_kfold(
                dataset=train_subset,
                num_epochs=detail_cfg['epochs'],
                learning_rate=detail_cfg['learning_rate'],
                weight_decay=detail_cfg['weight_decay'],
                patience=detail_cfg['patience'],
                n_splits=major_kfold_n_splits,
                random_state=major_kfold_random_state,
                debug=False,
                accumulation_steps=1,
                batch_size=detail_cfg['batch_size']
            )
            
            print(f"   âœ… K-Fold è®­ç»ƒå®Œæˆ | å¹³å‡ç²¾åº¦: {kfold_results['mean_metrics'].get('accuracy', 0):.4f}")
        else:
            # æ ·æœ¬ä¸è¶³ï¼Œè‡ªåŠ¨é™çº§åˆ°å¸¸è§„è®­ç»ƒ
            print(f"   â­ï¸  æ ·æœ¬æ•°({len(train_indices)}) < K-Fold æŠ˜æ•°({major_kfold_n_splits})ï¼Œä½¿ç”¨å¸¸è§„è®­ç»ƒ")
            
            sub_model = DualStreamSpatio_TemporalFusionNetwork(
                in_channels_dynamic=dyn_ch,
                in_channels_static=sta_ch,
                num_classes=num_sub_classes,
                dropout=config.get('model.dropout', 0.25),
                classifier_hidden_dims=config.get('model.classifier.hidden_dims', [128, 64, 32])
            )
            
            # åŠ¨æ€å†³å®šæ˜¯å¦ drop_last
            use_drop_last = len(train_indices) > detail_cfg['batch_size']
            
            sub_trainer = Trainer(
                model=sub_model,
                train_dataloader=DataLoader(
                    train_subset, 
                    shuffle=True, 
                    batch_size=detail_cfg['batch_size'], 
                    collate_fn=collate_fn, 
                    drop_last=use_drop_last,
                    **common_cfg
                ),
                val_dataloader=DataLoader(
                    val_subset, 
                    shuffle=False, 
                    batch_size=detail_cfg['batch_size'], 
                    collate_fn=collate_fn, 
                    **common_cfg
                ),
                num_classes=num_sub_classes,
                target_key='detail_label',
                label_mapping=global_to_local,
                output_dir=sub_model_dir,
                class_weights=detail_weights,
                use_focal_loss=True,
                label_smoothing=detail_label_smoothing
            )
            
            sub_trainer.train(
                num_epochs=detail_cfg['epochs'],
                learning_rate=detail_cfg['learning_rate'],
                weight_decay=detail_cfg['weight_decay'],
                patience=detail_cfg['patience']
            )
        
        with open(sub_model_dir / 'class_mapping.json', 'w', encoding='utf-8') as f:
            json.dump({
                'major_class': major_name,
                'major_id': major_id,
                'local_to_global_map': local_to_global, 
                'global_to_local_map': global_to_local
            }, f, ensure_ascii=False, indent=2)
            
        print(f"   âœ… {major_name} å°ç±»æ¨¡å‹å®Œæˆã€‚")

    print("\n" + "="*60)
    print("ğŸ‰ è®­ç»ƒæµæ°´çº¿ç»“æŸï¼")
    print("="*60)

if __name__ == '__main__':
    try:
        multiprocessing.set_start_method('spawn', force=True)
    except RuntimeError:
        pass
    main()