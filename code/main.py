#!/usr/bin/env python3
"""
main.py: æ•°æ®å¤„ç†æµæ°´çº¿ä¸»å…¥å£

è¯¥è„šæœ¬å®ç°äº†å®Œæ•´çš„å…­é˜¶æ®µæµç¨‹ï¼š
1. ç¬¬ä¸€é˜¶æ®µï¼šé…ç½®ç®¡ç†ä¸åŸºç¡€è®¾æ–½æ­å»º (Configuration & Infrastructure)
2. ç¬¬äºŒé˜¶æ®µï¼šæ•°æ®æ¸…æ´—ä¸æ—¶ç©ºç´¢å¼•æ„å»º (Data Ingestion & Indexing)
3. ç¬¬ä¸‰é˜¶æ®µï¼šåœ¨çº¿ç»Ÿè®¡ä¸æ•°æ®å½’ä¸€åŒ– (Statistical Analysis & Normalization)
4. ç¬¬å››é˜¶æ®µï¼šè‡ªå®šä¹‰æ—¶ç©ºæ•°æ®é›†æ„å»º (Custom Dataset Implementation)
5. ç¬¬äº”é˜¶æ®µï¼šæ¨¡å‹æ¶æ„è®¾è®¡ (Model Architecture)
6. ç¬¬å…­é˜¶æ®µï¼šè®­ç»ƒå¾ªç¯ä¸æ—¥å¿—ç³»ç»Ÿ (Training Loop & Logging)

è¿è¡Œæ–¹å¼ï¼š
    python main.py

è¾“å‡ºæ–‡ä»¶ä½ç½®ï¼š
    experiments/outputs/{timestamp}_{experiment_id}/
        â”œâ”€â”€ config_used.yaml                 # ä½¿ç”¨çš„é…ç½®æ–‡ä»¶å‰¯æœ¬
        â”œâ”€â”€ data_inventory.csv               # æ•°æ®æ¸…å•
        â”œâ”€â”€ verification_report.json         # è¯¦ç»†éªŒè¯æŠ¥å‘Š
        â”œâ”€â”€ data_summary.txt                 # æ–‡æœ¬æ‘˜è¦æŠ¥å‘Š
        â”œâ”€â”€ detailed_labels_map.json         # è¯¦ç»†ç±»åˆ«æ˜ å°„
        â”œâ”€â”€ major_labels_map.json            # å¤§ç±»æ˜ å°„
        â”œâ”€â”€ hierarchical_labels_map.json     # å±‚çº§æ˜ å°„
        â”œâ”€â”€ labels_geodata.geojson           # GeoJSONæ ¼å¼çš„æ ‡ç­¾
        â”œâ”€â”€ rasters_metadata.json            # æ …æ ¼å…ƒæ•°æ®
        â”œâ”€â”€ rasters_summary.json             # æ …æ ¼æ±‡æ€»
        â””â”€â”€ normalization_stats.json         # å½’ä¸€åŒ–å‚æ•°

ã€æµç¨‹è®¾è®¡ã€‘
=============================================================================

ç¬¬ä¸€é˜¶æ®µï¼šé…ç½®ç®¡ç†ä¸åŸºç¡€è®¾æ–½æ­å»º
  â†“
  åŠ è½½é…ç½®æ–‡ä»¶ â†’ è·¯å¾„éªŒè¯ â†’ åˆ›å»ºå®éªŒè¾“å‡ºç›®å½• â†’ é…ç½®å†»ç»“
  è¾“å‡ºï¼šconfig_used.yaml

ç¬¬äºŒé˜¶æ®µï¼šæ•°æ®æ¸…æ´—ä¸æ—¶ç©ºç´¢å¼•æ„å»º
  â†“
  åˆå§‹åŒ–æ ‡ç­¾ç¼–ç å™¨ â†’ è¯»å–CSVæ ‡ç­¾ â†’ ç”Ÿæˆç±»åˆ«æ˜ å°„ â†’ æŠ•å½±è½¬æ¢
  â†“
  åˆå§‹åŒ–æ …æ ¼çˆ¬è™« â†’ æ‰«æå½±åƒæ–‡ä»¶ â†’ è§£ææ—¶é—´å…ƒæ•°æ® â†’ æ„å»ºR-æ ‘ç´¢å¼•
  â†“
  ç”Ÿæˆæ•°æ®æ¸…å• â†’ éªŒè¯CRSä¸€è‡´æ€§ â†’ ç”ŸæˆéªŒè¯æŠ¥å‘Š
  è¾“å‡ºï¼š
    - detailed_labels_map.json
    - major_labels_map.json
    - data_inventory.csv
    - verification_report.json
    - rasters_metadata.json

ç¬¬ä¸‰é˜¶æ®µï¼šåœ¨çº¿ç»Ÿè®¡ä¸æ•°æ®å½’ä¸€åŒ–
  â†“
  é‡‡æ ·æ …æ ¼æ–‡ä»¶ â†’ Welfordå¢é‡ç®—æ³•è®¡ç®—ç»Ÿè®¡é‡ â†’ ä¿å­˜å½’ä¸€åŒ–å‚æ•°
  è¾“å‡ºï¼šnormalization_stats.json

ç¬¬å››é˜¶æ®µï¼šè‡ªå®šä¹‰æ—¶ç©ºæ•°æ®é›†æ„å»º
  â†“
  ç»§æ‰¿Datasetç±» â†’ æ—¶é—´è½´å¯¹é½ â†’ çª—å£è¯»å–ä¼˜åŒ– â†’ ç¼ºå¤±å€¼å¤„ç† â†’ è‡ªåŠ¨å½’ä¸€åŒ–
  â†“
  è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†åˆ’åˆ† â†’ æ•°æ®éªŒè¯ â†’ æ€§èƒ½åŸºå‡†æµ‹è¯•
  è¾“å‡ºï¼šdataset_info.json

ç¬¬äº”é˜¶æ®µï¼šæ¨¡å‹æ¶æ„è®¾è®¡
  â†“
  æ„å»ºåŒæµèåˆç½‘ç»œ â†’ æ˜¾ç¤ºæ¨¡å‹æ‘˜è¦ â†’ å‰å‘ä¼ æ’­æµ‹è¯• â†’ æ—¶é—´æ³¨æ„åŠ›å¯è§†åŒ–
  è¾“å‡ºï¼šmodel_architecture.json

ç¬¬å…­é˜¶æ®µï¼šè®­ç»ƒå¾ªç¯ä¸æ—¥å¿—ç³»ç»Ÿ
  â†“
  åˆå§‹åŒ–è®­ç»ƒå™¨ â†’ æ‰§è¡Œè®­ç»ƒå¾ªç¯ â†’ åŠ¨æ€ç›‘æ§ â†’ æ£€æŸ¥ç‚¹ä¿å­˜ â†’ ç”ŸæˆæŠ¥å‘Š
  â†“
  å¤šæŒ‡æ ‡è¯„ä¼°ï¼ˆAccuracy, F1, IoU, Precision, Recall, æ··æ·†çŸ©é˜µï¼‰
  â†“
  å¯è§†åŒ–ç»“æœ â†’ è‡ªåŠ¨åˆ†ææŠ¥å‘Š
  è¾“å‡ºï¼š
    - training_report.json
    - best_model.pth
    - metrics_curves.png
    - confusion_matrix.png

=============================================================================

ã€å…³é”®ç‰¹æ€§ã€‘

âœ“ æ¨¡å—åŒ–æ¶æ„ï¼šå„ä¸ªç»„ä»¶èŒè´£æ¸…æ™°ã€æ¾è€¦åˆ
âœ“ é”™è¯¯å¤„ç†ï¼šå¿«é€Ÿå¤±è´¥æœºåˆ¶ï¼ŒåŠæ—¶åé¦ˆé—®é¢˜
âœ“ æµå¼å¤„ç†ï¼šWelfordç®—æ³•æ”¯æŒå¤§è§„æ¨¡æ•°æ®
âœ“ å®Œæ•´æ—¥å¿—ï¼šæ¯ä¸ªé˜¶æ®µéƒ½æœ‰è¯¦ç»†çš„è¿›åº¦å’ŒçŠ¶æ€è¾“å‡º
âœ“ æ•°æ®å¯è¿½æº¯ï¼šå®Œæ•´çš„å…ƒæ•°æ®å’ŒéªŒè¯æŠ¥å‘Š
âœ“ è‡ªåŠ¨åŒ–è¾“å‡ºï¼šæ‰€æœ‰ä¸­é—´å’Œæœ€ç»ˆç»“æœè‡ªåŠ¨ä¿å­˜

ã€ç³»ç»Ÿè¦æ±‚ã€‘

Python >= 3.8

å¿…è¦åŒ…ï¼š
  - yaml
  - pandas
  - geopandas
  - numpy
  - rasterio
  - rtree
  - shapely
  - tqdm

ã€ä½¿ç”¨ç¤ºä¾‹ã€‘

$ python main.py
[INFO] ================================================== =============================
[INFO] ğŸš€ åœ°ç†ç©ºé—´æ•°æ®å¤„ç†æµæ°´çº¿å¯åŠ¨
[INFO] ================================================== =============================
[INFO]
[INFO] ğŸ“‹ [é˜¶æ®µ1] åŠ è½½é…ç½®æ–‡ä»¶...
[INFO]   âœ… é…ç½®åŠ è½½æˆåŠŸ
[INFO]
[INFO] ğŸ“‹ [é˜¶æ®µ2] æ•°æ®æ¸…æ´—ä¸æ—¶ç©ºç´¢å¼•æ„å»º...
[INFO]   âœ… æ ‡ç­¾å¤„ç†å®Œæˆ (2500ä¸ªæ ·æœ¬)
[INFO]   âœ… å½±åƒç´¢å¼•å®Œæˆ (15000ä¸ªæ–‡ä»¶)
[INFO]
[INFO] ğŸ“‹ [é˜¶æ®µ3] åœ¨çº¿ç»Ÿè®¡ä¸æ•°æ®å½’ä¸€åŒ–...
[INFO]   âœ… ç»Ÿè®¡é‡è®¡ç®—å®Œæˆ
[INFO]
[INFO] ================================================== =============================
[INFO] âœ… å…¨æµç¨‹å®Œæˆï¼
[INFO] ================================================== =============================
"""

import sys
import logging
import json
from pathlib import Path

# ç¡®ä¿èƒ½å¯¼å…¥æœ¬åœ°æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent))

from config_manager import ConfigManager
from label_encoder import LabelEncoder
from raster_crawler import RasterCrawler
from data_preprocessor import DataPreprocessor
from stats_calculator import StatsCalculator
from point_timeseries_dataset import PointTimeSeriesDataset
from trainer import Trainer


def setup_logging():
    """é…ç½®å…¨å±€æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(levelname)s: %(message)s',
        stream=sys.stdout
    )


def print_header(title):
    """æ‰“å°æ ‡é¢˜"""
    print("\n" + "=" * 80)
    print(f"ğŸš€ {title}")
    print("=" * 80)


def print_section(num, title):
    """æ‰“å°é˜¶æ®µæ ‡é¢˜"""
    print(f"\nğŸ“‹ [é˜¶æ®µ{num}] {title}")


def print_success(message):
    """æ‰“å°æˆåŠŸæ¶ˆæ¯"""
    print(f"âœ… {message}")


def print_warning(message):
    """æ‰“å°è­¦å‘Šæ¶ˆæ¯"""
    print(f"âš ï¸  {message}")


def print_error(message):
    """æ‰“å°é”™è¯¯æ¶ˆæ¯"""
    print(f"âŒ {message}")


def phase_1_configuration():
    """
    ç¬¬ä¸€é˜¶æ®µï¼šé…ç½®ç®¡ç†ä¸åŸºç¡€è®¾æ–½æ­å»º
    
    åŠŸèƒ½ï¼š
    1. åŠ è½½é…ç½®æ–‡ä»¶
    2. éªŒè¯æ‰€æœ‰å…³é”®è·¯å¾„
    3. åˆ›å»ºå®éªŒè¾“å‡ºç›®å½•
    4. å†»ç»“é…ç½®ï¼ˆåªè¯»ä¿æŠ¤ï¼‰
    
    Returns:
        ConfigManager: é…ç½®å¯¹è±¡
    """
    print_section(1, "é…ç½®ç®¡ç†ä¸åŸºç¡€è®¾æ–½æ­å»º")
    
    try:
        # å®šä½é…ç½®æ–‡ä»¶
        config_path = Path(__file__).parent / 'config.yaml'
        
        if not config_path.exists():
            print_error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return None
        
        print(f"ğŸ“‚ é…ç½®æ–‡ä»¶: {config_path}")
        
        # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨ï¼ˆåŒ…å«è·¯å¾„éªŒè¯å’Œå®éªŒè¾“å‡ºç›®å½•åˆ›å»ºï¼‰
        config = ConfigManager(str(config_path))
        
        print_success("é…ç½®åŠ è½½æˆåŠŸ")
        print(f"  é¡¹ç›®å: {config.get('project_name')}")
        print(f"  å®éªŒID: {config.get('experiment_id')}")
        print(f"  è¾“å‡ºç›®å½•: {config.get_experiment_output_dir()}")
        
        return config
        
    except Exception as e:
        print_error(f"é…ç½®é˜¶æ®µå¤±è´¥: {e}")
        return None


def phase_2_data_ingestion(config):
    """
    ç¬¬äºŒé˜¶æ®µï¼šæ•°æ®æ¸…æ´—ä¸æ—¶ç©ºç´¢å¼•æ„å»º
    
    åŠŸèƒ½ï¼š
    1. åˆå§‹åŒ–æ ‡ç­¾ç¼–ç å™¨ï¼ˆCSV â†’ ç±»åˆ«æ˜ å°„ï¼‰
    2. åˆå§‹åŒ–æ …æ ¼çˆ¬è™«ï¼ˆå½±åƒæ–‡ä»¶æ‰«æä¸æ—¶é—´è§£æï¼‰
    3. æ‰§è¡Œæ•°æ®éªŒè¯å’Œæ¸…å•ç”Ÿæˆ
    
    Args:
        config: ConfigManagerå¯¹è±¡
    
    Returns:
        Tuple[LabelEncoder, RasterCrawler, RasterCrawler]: æ ‡ç­¾ç¼–ç å™¨å’Œä¸¤ä¸ªæ …æ ¼çˆ¬è™«
    """
    print_section(2, "æ•°æ®æ¸…æ´—ä¸æ—¶ç©ºç´¢å¼•æ„å»º (Data Ingestion & Indexing)")
    
    try:
        # æ­¥éª¤ 1: åˆå§‹åŒ–æ ‡ç­¾ç¼–ç å™¨
        print("\n  ğŸ“ åˆå§‹åŒ–æ ‡ç­¾ç¼–ç å™¨...")
        try:
            encoder = LabelEncoder(config=config)
            stats = encoder.get_statistics()
            hierarchical_map = encoder.get_hierarchical_map()
            
            # è‡ªåŠ¨æ£€æµ‹ç±»åˆ«æ•°
            num_detailed_classes = len(stats['detailed_class_distribution'])
            num_major_classes = len(stats['major_class_distribution'])
            
            print_success(f"æ ‡ç­¾ç¼–ç å®Œæˆ ({stats['total_samples']} ä¸ªæ ·æœ¬)")
            print(f"    è¯¦ç»†ç±»åˆ«: {num_detailed_classes} ä¸ª")
            print(f"      {stats['detailed_class_distribution']}")
            print(f"    å¤§ç±»æ•°: {num_major_classes} ä¸ª")
            print(f"      {stats['major_class_distribution']}")
            
            # åˆ†æå±‚çº§ç»“æ„
            print(f"\n  ğŸ“Š å±‚çº§ç»“æ„åˆ†æ:")
            for major_id, major_info in sorted(hierarchical_map.items()):
                major_name = major_info.get('name', f'Major_{major_id}')
                num_detail = len(major_info.get('detail_classes', {}))
                skip_msg = " âš¡(ä»…1ä¸ªå°ç±»)" if num_detail == 1 else ""
                print(f"    å¤§ç±» {major_id}: {major_name} - {num_detail} ä¸ªå°ç±»{skip_msg}")
        except Exception as e:
            print_error(f"æ ‡ç­¾ç¼–ç å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None, None, None, None, None, None
        
        # æ­¥éª¤ 2: åˆå§‹åŒ–æ …æ ¼çˆ¬è™« - åŠ¨æ€å½±åƒ
        print("\n  ğŸ“š åˆå§‹åŒ–æ …æ ¼çˆ¬è™« (åŠ¨æ€å½±åƒ)...")
        dynamic_crawler = None
        dynamic_channels = None
        try:
            filename_pattern = config.get('data_specs.raster_crawler.filename_pattern')
            dynamic_crawler = RasterCrawler(
                config=config,
                raster_dir=config.get_resolved_path('dynamic_images_dir'),
                filename_pattern=filename_pattern,
                file_extensions=tuple(config.get(
                    'data_specs.raster_crawler.file_extensions',
                    ['.tif', '.tiff', '.jp2']
                )),
            )
            print_success(f"åŠ¨æ€å½±åƒçˆ¬è™«åˆå§‹åŒ– ({dynamic_crawler.get_raster_count()} ä¸ªæ–‡ä»¶)")
            
            # è‡ªåŠ¨æ£€æµ‹æ³¢æ®µæ•°
            channel_info = dynamic_crawler.detect_num_channels(sample_size=5)
            dynamic_channels = channel_info['most_common']
            print(f"    è‡ªåŠ¨æ£€æµ‹: {dynamic_channels} ä¸ªæ³¢æ®µ")
            if 'warning' in channel_info:
                print_warning(f"    {channel_info['warning']}")
            
            # è‡ªåŠ¨æ£€æµ‹åæ ‡ç³»
            print(f"    æ£€æµ‹åæ ‡ç³»...")
            crs_info = dynamic_crawler.detect_crs()
            if crs_info['most_common_crs']:
                print(f"    åæ ‡ç³»: {crs_info['most_common_crs']}")
                if not crs_info['is_consistent']:
                    print_warning(f"    {crs_info['warning']}")
                if crs_info.get('recommendation'):
                    print(f"    ğŸ’¡ {crs_info['recommendation']}")

        except Exception as e:
            print_warning(f"åŠ¨æ€å½±åƒçˆ¬è™«åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # æ­¥éª¤ 3: åˆå§‹åŒ–æ …æ ¼çˆ¬è™« - é™æ€å½±åƒ
        print("\n  ğŸ“š åˆå§‹åŒ–æ …æ ¼çˆ¬è™« (é™æ€å½±åƒ)...")
        static_crawler = None
        static_channels = None
        try:
            static_crawler = RasterCrawler(
                config=config,
                raster_dir=config.get_resolved_path('static_images_dir'),
                filename_pattern=config.get('data_specs.raster_crawler.filename_pattern'),
                file_extensions=tuple(config.get(
                    'data_specs.raster_crawler.file_extensions',
                    ['.tif', '.tiff', '.jp2']
                )),
            )
            print_success(f"é™æ€å½±åƒçˆ¬è™«åˆå§‹åŒ– ({static_crawler.get_raster_count()} ä¸ªæ–‡ä»¶)")
            
            # è‡ªåŠ¨æ£€æµ‹æ³¢æ®µæ•°
            channel_info = static_crawler.detect_num_channels(sample_size=5)
            static_channels = channel_info['most_common']
            print(f"    è‡ªåŠ¨æ£€æµ‹: {static_channels} ä¸ªæ³¢æ®µ")
            if 'warning' in channel_info:
                print_warning(f"    {channel_info['warning']}")
            
            # è‡ªåŠ¨æ£€æµ‹åæ ‡ç³»
            print(f"    æ£€æµ‹åæ ‡ç³»...")
            crs_info = static_crawler.detect_crs()
            if crs_info['most_common_crs']:
                print(f"    åæ ‡ç³»: {crs_info['most_common_crs']}")
                if not crs_info['is_consistent']:
                    print_warning(f"    {crs_info['warning']}")
                if crs_info.get('recommendation'):
                    print(f"    ğŸ’¡ {crs_info['recommendation']}")

            
            # è‡ªåŠ¨æ£€æµ‹æ³¢æ®µæ•°
            channel_info = static_crawler.detect_num_channels(sample_size=5)
            static_channels = channel_info['most_common']
            print(f"    è‡ªåŠ¨æ£€æµ‹: {static_channels} ä¸ªæ³¢æ®µ")
            if 'warning' in channel_info:
                print_warning(f"    {channel_info['warning']}")
        except Exception as e:
            print_warning(f"é™æ€å½±åƒçˆ¬è™«åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # æ­¥éª¤ 4: æ•°æ®é¢„å¤„ç†å’ŒéªŒè¯
        print("\n  ğŸ” æ‰§è¡Œæ•°æ®éªŒè¯...")
        try:
            preprocessor = DataPreprocessor(
                config=config,
                encoder=encoder,
                dynamic_crawler=dynamic_crawler,
                static_crawler=static_crawler,
            )
            preprocessor.run()
            print_success("æ•°æ®éªŒè¯å®Œæˆ")
        except Exception as e:
            print_error(f"æ•°æ®éªŒè¯å¤±è´¥: {e}")
            return encoder, dynamic_crawler, static_crawler, hierarchical_map, dynamic_channels, static_channels
        
        return encoder, dynamic_crawler, static_crawler, hierarchical_map, dynamic_channels, static_channels
        
    except Exception as e:
        print_error(f"ç¬¬äºŒé˜¶æ®µå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None, None, None, None


def phase_3_statistical_analysis(config, dynamic_crawler, static_crawler):
    """
    ç¬¬ä¸‰é˜¶æ®µï¼šåœ¨çº¿ç»Ÿè®¡ä¸æ•°æ®å½’ä¸€åŒ–
    
    åŠŸèƒ½ï¼š
    1. é‡‡æ ·æ …æ ¼æ–‡ä»¶
    2. ä½¿ç”¨ Welford å¢é‡ç®—æ³•è®¡ç®—ç»Ÿè®¡é‡
    3. ä¿å­˜å½’ä¸€åŒ–å‚æ•°
    
    Args:
        config: ConfigManagerå¯¹è±¡
        dynamic_crawler: åŠ¨æ€å½±åƒçˆ¬è™«
        static_crawler: é™æ€å½±åƒçˆ¬è™«
    
    Returns:
        bool: æ˜¯å¦æˆåŠŸ
    """
    print_section(3, "åœ¨çº¿ç»Ÿè®¡ä¸æ•°æ®å½’ä¸€åŒ– (Statistical Analysis & Normalization)")
    
    try:
        if not dynamic_crawler and not static_crawler:
            print_warning("æœªæ‰¾åˆ°ä»»ä½•å½±åƒçˆ¬è™«ï¼Œè·³è¿‡ç»Ÿè®¡é˜¶æ®µ")
            return True
        
        # åˆå§‹åŒ–ç»Ÿè®¡è®¡ç®—å™¨
        print("\n  ğŸ“Š åˆå§‹åŒ–ç»Ÿè®¡è®¡ç®—å™¨...")
        calculator = StatsCalculator(
            config=config,
            dynamic_channel_names=['Band_0'],  # å®é™…é€šé“æ•°ä¼šè‡ªåŠ¨æ£€æµ‹
            static_channel_names=['Band_0'],
        )
        print_success("ç»Ÿè®¡è®¡ç®—å™¨å·²åˆå§‹åŒ–")
        
        # è·å–æ …æ ¼åˆ—è¡¨
        dynamic_rasters = dynamic_crawler.get_all_rasters() if dynamic_crawler else None
        static_rasters = static_crawler.get_all_rasters() if static_crawler else None
        
        if not dynamic_rasters and not static_rasters:
            print_warning("æœªæ‰¾åˆ°ä»»ä½•å½±åƒæ–‡ä»¶")
            return True
        
        # è®¡ç®—ç»Ÿè®¡é‡
        print("\n  ğŸ§® è®¡ç®—å…¨å±€ç»Ÿè®¡é‡ (Welfordå¢é‡ç®—æ³•)...")
        try:
            calculator.compute_global_stats(
                dynamic_rasters=dynamic_rasters,
                static_rasters=static_rasters,
                sampling_rate=0.6,  # é‡‡æ · 60%
            )
            print_success("ç»Ÿè®¡é‡è®¡ç®—å®Œæˆ")
            
            # è·å–å¹¶æ˜¾ç¤ºç»Ÿè®¡å‚æ•°
            params = calculator.get_normalization_params()
            if 'dynamic' in params:
                print(f"\n  ğŸ“Š åŠ¨æ€å½±åƒç»Ÿè®¡:")
                print(f"     Mean: {params['dynamic']['mean']}")
                print(f"     Std:  {params['dynamic']['std']}")
            if 'static' in params:
                print(f"\n  ğŸ“Š é™æ€å½±åƒç»Ÿè®¡:")
                print(f"     Mean: {params['static']['mean']}")
                print(f"     Std:  {params['static']['std']}")
        except Exception as e:
            print_error(f"ç»Ÿè®¡é‡è®¡ç®—å¤±è´¥: {e}")
            return False
        
        # ä¿å­˜ç»Ÿè®¡é‡
        print("\n  ğŸ’¾ ä¿å­˜å½’ä¸€åŒ–å‚æ•°...")
        try:
            calculator.save_stats('normalization_stats.json')
            print_success("å½’ä¸€åŒ–å‚æ•°å·²ä¿å­˜")
        except Exception as e:
            print_error(f"ä¿å­˜ç»Ÿè®¡é‡å¤±è´¥: {e}")
            return False
        
        return True
        
    except Exception as e:
        print_error(f"ç¬¬ä¸‰é˜¶æ®µå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def phase_4_dataset_construction(config, encoder, dynamic_crawler, static_crawler):
    """
    ç¬¬å››é˜¶æ®µï¼šè‡ªå®šä¹‰æ—¶ç©ºæ•°æ®é›†æ„å»º (Custom Dataset Implementation)
    
    åŠŸèƒ½ï¼š
    1. ç»§æ‰¿ torch.utils.data.Dataset å®ç°ç‚¹-æ—¶åºå¯¹é½
    2. åˆ©ç”¨ rasterio çª—å£è¯»å–å®ç°é«˜æ•ˆæ•°æ®åŠ è½½
    3. æ„å»ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†åˆ’åˆ†
    4. éªŒè¯æ•°æ®é›†å®Œæ•´æ€§å’Œæ€§èƒ½
    
    å…³é”®ç‰¹æ€§ï¼š
    âœ“ å³æ—¶çª—å£è¯»å–ï¼ˆOn-the-fly Windowed Readingï¼‰
    âœ“ æ ‡å‡†æ—¶é—´è½´å¯¹é½ï¼ˆæŒ‰æœˆä»½èšåˆï¼‰
    âœ“ çµæ´»çš„ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥
    âœ“ è‡ªåŠ¨å½’ä¸€åŒ–
    âœ“ é«˜æ•ˆçš„Ræ ‘ç©ºé—´ç´¢å¼•
    
    Args:
        config: ConfigManagerå¯¹è±¡
        encoder: LabelEncoderå¯¹è±¡
        dynamic_crawler: åŠ¨æ€å½±åƒçˆ¬è™«
        static_crawler: é™æ€å½±åƒçˆ¬è™«
    
    Returns:
        Tuple[PointTimeSeriesDataset, PointTimeSeriesDataset, PointTimeSeriesDataset]: 
            è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æ•°æ®é›†
    """
    print_section(4, "è‡ªå®šä¹‰æ—¶ç©ºæ•°æ®é›†æ„å»º (Custom Dataset Implementation)")
    
    try:
        # æ­¥éª¤ 1: ç¡®è®¤å½’ä¸€åŒ–å‚æ•°æ–‡ä»¶
        output_dir = config.get_experiment_output_dir()
        stats_file = Path(output_dir) / 'normalization_stats.json'
        
        if not stats_file.exists():
            print_warning(f"å½’ä¸€åŒ–å‚æ•°æ–‡ä»¶ä¸å­˜åœ¨: {stats_file}")
            print_warning("æ•°æ®å°†ä»¥åŸå§‹å€¼è¿”å›ï¼Œä¸è¿›è¡Œå½’ä¸€åŒ–")
        else:
            print(f"âœ“ å½’ä¸€åŒ–å‚æ•°æ–‡ä»¶: {stats_file}")
        
        # æ­¥éª¤ 2: åˆå§‹åŒ–è®­ç»ƒé›†
        print("\n  ğŸ“ åˆå§‹åŒ–è®­ç»ƒé›†...")
        try:
            train_dataset = PointTimeSeriesDataset(
                config=config,
                encoder=encoder,
                dynamic_crawler=dynamic_crawler,
                static_crawler=static_crawler,
                stats_file=str(stats_file) if stats_file.exists() else None,
                split='train',
                split_ratio=config.get('train.split_ratio', (0.7, 0.15, 0.15)),
                seed=config.get('train.seed', 42),
                cache_metadata=True,
                missing_value_strategy=config.get('data_specs.temporal.missing_value_strategy', 'zero_padding'),
                normalization_method=config.get('data_specs.temporal.normalization_method', 'zscore'),
                verbose=False,
            )
            train_stats = train_dataset.get_statistics()
            print_success(f"è®­ç»ƒé›†åˆå§‹åŒ–å®Œæˆ ({len(train_dataset)} ä¸ªæ ·æœ¬)")
            print(f"  ç±»åˆ«åˆ†å¸ƒ: {train_stats['label_distribution']}")
        except Exception as e:
            print_error(f"è®­ç»ƒé›†åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None, None, None
        
        # æ­¥éª¤ 3: åˆå§‹åŒ–éªŒè¯é›†
        print("\n  ğŸ“Š åˆå§‹åŒ–éªŒè¯é›†...")
        try:
            val_dataset = PointTimeSeriesDataset(
                config=config,
                encoder=encoder,
                dynamic_crawler=dynamic_crawler,
                static_crawler=static_crawler,
                stats_file=str(stats_file) if stats_file.exists() else None,
                split='val',
                split_ratio=config.get('train.split_ratio', (0.7, 0.15, 0.15)),
                seed=config.get('train.seed', 42),
                cache_metadata=True,
                verbose=False,
            )
            val_stats = val_dataset.get_statistics()
            print_success(f"éªŒè¯é›†åˆå§‹åŒ–å®Œæˆ ({len(val_dataset)} ä¸ªæ ·æœ¬)")
        except Exception as e:
            print_error(f"éªŒè¯é›†åˆå§‹åŒ–å¤±è´¥: {e}")
            return None, None, None
        
        # æ­¥éª¤ 4: åˆå§‹åŒ–æµ‹è¯•é›†
        print("\n  ğŸ§ª åˆå§‹åŒ–æµ‹è¯•é›†...")
        try:
            test_dataset = PointTimeSeriesDataset(
                config=config,
                encoder=encoder,
                dynamic_crawler=dynamic_crawler,
                static_crawler=static_crawler,
                stats_file=str(stats_file) if stats_file.exists() else None,
                split='test',
                split_ratio=config.get('train.split_ratio', (0.7, 0.15, 0.15)),
                seed=config.get('train.seed', 42),
                cache_metadata=True,
                verbose=False,
            )
            test_stats = test_dataset.get_statistics()
            print_success(f"æµ‹è¯•é›†åˆå§‹åŒ–å®Œæˆ ({len(test_dataset)} ä¸ªæ ·æœ¬)")
        except Exception as e:
            print_error(f"æµ‹è¯•é›†åˆå§‹åŒ–å¤±è´¥: {e}")
            return None, None, None
        
        # æ­¥éª¤ 5: æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ˆå¿«é€Ÿæ£€æŸ¥ï¼‰
        print("\n  âš¡ æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ˆæŠ½æ ·10ä¸ªæ ·æœ¬ï¼‰...")
        try:
            import time
            times = []
            for i in range(min(10, len(train_dataset))):
                start = time.time()
                _ = train_dataset[i]
                times.append(time.time() - start)
            
            avg_time = sum(times) / len(times)
            max_time = max(times)
            
            print(f"  å¹³å‡è€—æ—¶: {avg_time:.4f}s")
            print(f"  æœ€å¤§è€—æ—¶: {max_time:.4f}s")
            
            if avg_time < 0.1:
                print_success(f"æ€§èƒ½æ»¡è¶³è¦æ±‚ (< 0.1s)")
            elif avg_time < 0.5:
                print_warning(f"æ€§èƒ½ç•¥æ…¢ï¼Œä½†å¯ä»¥æ¥å— (< 0.5s)")
            else:
                print_warning(f"æ€§èƒ½å¯èƒ½æˆä¸ºç“¶é¢ˆ (> 0.5s)ï¼Œè€ƒè™‘ä¼˜åŒ–TIFFå­˜å‚¨æ ¼å¼")
        except Exception as e:
            print_warning(f"æ€§èƒ½æµ‹è¯•å¼‚å¸¸: {e}")
        
        # æ­¥éª¤ 6: ä¿å­˜æ•°æ®é›†å…ƒæ•°æ®
        print("\n  ğŸ’¾ ä¿å­˜æ•°æ®é›†å…ƒæ•°æ®...")
        try:
            dataset_info = {
                'train': {
                    'size': len(train_dataset),
                    'statistics': train_stats,
                },
                'val': {
                    'size': len(val_dataset),
                    'statistics': val_stats,
                },
                'test': {
                    'size': len(test_dataset),
                    'statistics': test_stats,
                },
                'configuration': {
                    'patch_size': config.get('data_specs.spatial.patch_size', 64),
                    'missing_value_strategy': config.get('data_specs.temporal.missing_value_strategy', 'zero_padding'),
                    'normalization_method': config.get('data_specs.temporal.normalization_method', 'zscore'),
                }
            }
            
            dataset_info_file = Path(output_dir) / 'dataset_info.json'
            with open(dataset_info_file, 'w', encoding='utf-8') as f:
                json.dump(dataset_info, f, ensure_ascii=False, indent=2)
            
            print_success(f"æ•°æ®é›†å…ƒæ•°æ®å·²ä¿å­˜: {dataset_info_file}")
        except Exception as e:
            print_warning(f"ä¿å­˜æ•°æ®é›†å…ƒæ•°æ®å¤±è´¥: {e}")
        
        print_success("ç¬¬å››é˜¶æ®µå®Œæˆ")
        return train_dataset, val_dataset, test_dataset
        
    except Exception as e:
        print_error(f"ç¬¬å››é˜¶æ®µå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None


def phase_5_model_architecture(config, hierarchical_map, dynamic_channels, static_channels):
    """
    ç¬¬äº”é˜¶æ®µï¼šæ¨¡å‹æ¶æ„è®¾è®¡ (Model Architecture)
    
    åŠŸèƒ½ï¼š
    1. æ„å»ºåˆ†å±‚åˆ†ç±»åŒæµç½‘ç»œ
    2. æ˜¾ç¤ºæ¨¡å‹æ‘˜è¦ä¸å‚æ•°ç»Ÿè®¡
    3. éªŒè¯æ¨¡å‹å¯ä»¥æ­£å¸¸å‰å‘ä¼ æ’­
    
    Args:
        config: ConfigManagerå¯¹è±¡
        hierarchical_map: åˆ†å±‚æ˜ å°„å­—å…¸
        dynamic_channels: åŠ¨æ€å½±åƒé€šé“æ•°
        static_channels: é™æ€å½±åƒé€šé“æ•°
    
    Returns:
        æ¨¡å‹å®ä¾‹ï¼Œæˆ–Noneå¦‚æœæ„å»ºå¤±è´¥
    """
    print_section(5, "æ¨¡å‹æ¶æ„è®¾è®¡ (Model Architecture)")
    
    try:
        # å¯¼å…¥æ¨¡å‹
        from model_architecture import HierarchicalDualStreamNetwork
        
        # æ­¥éª¤ 1: åˆå§‹åŒ–æ¨¡å‹
        print("\n  ğŸ—ï¸  æ„å»ºåˆ†å±‚åˆ†ç±»åŒæµç½‘ç»œ...")
        try:
            model = HierarchicalDualStreamNetwork(
                in_channels_dynamic=dynamic_channels if dynamic_channels else 4,
                in_channels_static=static_channels if static_channels else 1,
                hierarchical_map=hierarchical_map,
                patch_size=config.get('data_specs.spatial.patch_size', 64),
                temporal_steps=12,  # å›ºå®šä¸º12ä¸ªæœˆ
                hidden_dim=config.get('model.hidden_dim', 64),
                dropout=config.get('model.dropout', 0.2),
            )
            print_success("æ¨¡å‹æ„å»ºæˆåŠŸ")
        except Exception as e:
            print_error(f"æ¨¡å‹æ„å»ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
        
        # æ­¥éª¤ 2: æ˜¾ç¤ºæ¨¡å‹æ‘˜è¦
        print("\n  ğŸ“Š æ¨¡å‹æ‘˜è¦:")
        try:
            summary = model.get_model_summary()
            print(f"    æ¨¡å‹åç§°: {summary['model_name']}")
            print(f"    æ€»å‚æ•°æ•°: {summary['total_parameters']:,}")
            print(f"    å¯è®­ç»ƒå‚æ•°: {summary['trainable_parameters']:,}")
            print(f"\n    æ¨¡å‹é…ç½®:")
            for key, value in summary['configuration'].items():
                print(f"      - {key}: {value}")
        except Exception as e:
            print_warning(f"è·å–æ¨¡å‹æ‘˜è¦å¤±è´¥: {e}")
        
        # æ­¥éª¤ 3: æ¨¡å‹å‰å‘ä¼ æ’­æµ‹è¯•
        print("\n  âš¡ æ‰§è¡Œå‰å‘ä¼ æ’­æµ‹è¯•...")
        try:
            import torch
            
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            model = model.to(device)
            
            # åˆ›å»ºè™šæ‹Ÿè¾“å…¥
            batch_size = 4
            dynamic_dummy = torch.randn(
                batch_size,
                12,  # æ—¶é—´æ­¥
                dynamic_channels if dynamic_channels else 4,
                64,  # patch_size
                64,
                device=device
            )
            static_dummy = torch.randn(
                batch_size,
                static_channels if static_channels else 1,
                64,
                64,
                device=device
            )
            
            # å‰å‘ä¼ æ’­
            with torch.no_grad():
                output = model(dynamic_dummy, static_dummy, return_aux=True)
            
            major_logits = output['major_logits']
            detail_logits = output['detail_logits']
            major_preds = output['major_preds']
            detail_preds = output['detail_preds']
            
            print_success(f"å‰å‘ä¼ æ’­æˆåŠŸ")
            print(f"    å¤§ç±»è¾“å‡ºå½¢çŠ¶: {tuple(major_logits.shape)}")
            print(f"    å°ç±»è¾“å‡ºå½¢çŠ¶: {tuple(detail_logits.shape)}")
            print(f"    å¤§ç±»é¢„æµ‹: {major_preds}")
            print(f"    å°ç±»é¢„æµ‹: {detail_preds}")
            
            # æ˜¾ç¤ºæ³¨æ„åŠ›æƒé‡
            if 'auxiliary' in output:
                attn_weights = output['auxiliary']['dynamic_attention_weights']
                print(f"\n  ğŸ“ˆ æ—¶é—´æ³¨æ„åŠ›æƒé‡ (å¹³å‡å€¼):")
                months = ['1æœˆ', '2æœˆ', '3æœˆ', '4æœˆ', '5æœˆ', '6æœˆ',
                         '7æœˆ', '8æœˆ', '9æœˆ', '10æœˆ', '11æœˆ', '12æœˆ']
                avg_weights = attn_weights.mean(dim=0).cpu().numpy()
                for i, (month, weight) in enumerate(zip(months, avg_weights)):
                    bar = 'â–ˆ' * int(weight * 50)
                    print(f"    {month}: {weight:.4f} {bar}")
        
        except Exception as e:
            print_error(f"å‰å‘ä¼ æ’­æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
        
        # æ­¥éª¤ 4: ä¿å­˜æ¨¡å‹æ¶æ„ä¿¡æ¯
        print("\n  ğŸ’¾ ä¿å­˜æ¨¡å‹æ¶æ„ä¿¡æ¯...")
        try:
            output_dir = config.get_experiment_output_dir()
            
            model_info = {
                'architecture': 'HierarchicalDualStreamNetwork',
                'summary': summary,
                'configuration': {
                    'in_channels_dynamic': dynamic_channels if dynamic_channels else 4,
                    'in_channels_static': static_channels if static_channels else 1,
                    'hierarchical_map': hierarchical_map,
                    'patch_size': 64,
                    'temporal_steps': 12,
                    'hidden_dim': config.get('model.hidden_dim', 64),
                    'dropout': config.get('model.dropout', 0.2),
                },
                'device': str(device),
                'attention_weights': {
                    'months': ['January', 'February', 'March', 'April', 'May', 'June',
                              'July', 'August', 'September', 'October', 'November', 'December'],
                    'average_weights': avg_weights.tolist() if 'avg_weights' in locals() else None,
                }
            }
            
            model_info_file = Path(output_dir) / 'model_architecture.json'
            with open(model_info_file, 'w', encoding='utf-8') as f:
                json.dump(model_info, f, ensure_ascii=False, indent=2)
            
            print_success(f"æ¨¡å‹æ¶æ„ä¿¡æ¯å·²ä¿å­˜: {model_info_file}")
        
        except Exception as e:
            print_warning(f"ä¿å­˜æ¨¡å‹æ¶æ„ä¿¡æ¯å¤±è´¥: {e}")
        
        print_success("ç¬¬äº”é˜¶æ®µå®Œæˆ")
        return model
        
    except ImportError as e:
        print_error(f"å¯¼å…¥æ¨¡å‹æ¨¡å—å¤±è´¥: {e}")
        print_warning("è¯·ç¡®ä¿å·²å®‰è£… torch å’Œå…¶ä»–ä¾èµ–")
        return None
    except Exception as e:
        print_error(f"ç¬¬äº”é˜¶æ®µå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def phase_6_training_loop(config, model, train_dataset, val_dataset, test_dataset, output_dir, hierarchical_map):
    """
    ç¬¬å…­é˜¶æ®µï¼šè®­ç»ƒå¾ªç¯ä¸æ—¥å¿—ç³»ç»Ÿ (Training Loop & Logging)
    
    åŠŸèƒ½ï¼š
    1. åˆå§‹åŒ– Trainer ç±»ï¼ˆæ”¯æŒåˆ†å±‚åˆ†ç±»ï¼‰
    2. æ‰§è¡Œå®Œæ•´è®­ç»ƒå¾ªç¯
    3. è¿›è¡ŒéªŒè¯å’Œæµ‹è¯•
    4. ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š
    
    Args:
        config: ConfigManagerå¯¹è±¡
        model: åˆ†å±‚åˆ†ç±»ç¥ç»ç½‘ç»œæ¨¡å‹
        train_dataset: è®­ç»ƒæ•°æ®é›†
        val_dataset: éªŒè¯æ•°æ®é›†
        test_dataset: æµ‹è¯•æ•°æ®é›†
        output_dir: è¾“å‡ºç›®å½•
        hierarchical_map: åˆ†å±‚æ˜ å°„å­—å…¸
    
    Returns:
        bool: è®­ç»ƒæ˜¯å¦æˆåŠŸ
    """
    print_section(6, "è®­ç»ƒå¾ªç¯ä¸æ—¥å¿—ç³»ç»Ÿ (Training Loop & Logging)")
    
    try:
        import torch
        from torch.utils.data import DataLoader
        
        # æ­¥éª¤ 1: æ£€æŸ¥è®­ç»ƒé…ç½®
        print("\n  âš™ï¸  æ£€æŸ¥è®­ç»ƒé…ç½®...")
        try:
            epochs = config.get('train.epochs', 50)
            batch_size = config.get('train.batch_size', 32)
            lr = config.get('train.lr', 1e-3)
            weight_decay = config.get('train.weight_decay', 1e-4)
            patience = config.get('train.patience', 10)
            
            print(f"    Epochs: {epochs}")
            print(f"    Batch Size: {batch_size}")
            print(f"    Learning Rate: {lr}")
            print(f"    Weight Decay: {weight_decay}")
            print(f"    Patience (Early Stopping): {patience}")
            print_success("è®­ç»ƒé…ç½®æ£€æŸ¥å®Œæˆ")
        except Exception as e:
            print_error(f"é…ç½®æ£€æŸ¥å¤±è´¥: {e}")
            return False
        
        # æ­¥éª¤ 2: åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        print("\n  ğŸ“¦ åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨...")
        try:
            # æ³¨æ„ï¼šå°† num_workers è®¾ç½®ä¸º 0 é¿å…å¤šè¿›ç¨‹åºåˆ—åŒ–é—®é¢˜
            train_loader = DataLoader(
                train_dataset,
                batch_size=batch_size,
                shuffle=True,
                num_workers=0,  # å¼ºåˆ¶å•è¿›ç¨‹æ¨¡å¼
                pin_memory=True if torch.cuda.is_available() else False,
            )
            val_loader = DataLoader(
                val_dataset,
                batch_size=batch_size,
                shuffle=False,
                num_workers=0,  # å¼ºåˆ¶å•è¿›ç¨‹æ¨¡å¼
                pin_memory=True if torch.cuda.is_available() else False,
            )
            test_loader = DataLoader(
                test_dataset,
                batch_size=batch_size,
                shuffle=False,
                num_workers=0,  # å¼ºåˆ¶å•è¿›ç¨‹æ¨¡å¼
                pin_memory=True if torch.cuda.is_available() else False,
            )
            print_success(f"æ•°æ®åŠ è½½å™¨åˆå§‹åŒ–å®Œæˆ")
            print(f"  è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}")
            print(f"  éªŒè¯æ‰¹æ¬¡: {len(val_loader)}")
            print(f"  æµ‹è¯•æ‰¹æ¬¡: {len(test_loader)}")
        except Exception as e:
            print_error(f"æ•°æ®åŠ è½½å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
        
        # æ­¥éª¤ 3: åˆå§‹åŒ– Trainer
        print("\n  ğŸ“ åˆå§‹åŒ– Trainer...")
        try:
            # ç¡®å®šè®¡ç®—è®¾å¤‡
            use_cuda = torch.cuda.is_available()
            device = 'cuda' if use_cuda else 'cpu'
            
            if use_cuda:
                print(f"    ğŸ–¥ï¸  GPU å¯ç”¨: {torch.cuda.get_device_name()}")
            else:
                print(f"    âš ï¸  GPU ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPUï¼ˆé€Ÿåº¦ä¼šå¾ˆæ…¢ï¼‰")
            
            # ä½¿ç”¨åˆ†å±‚æ˜ å°„åˆå§‹åŒ–è®­ç»ƒå™¨
            trainer = Trainer(
                model=model,
                train_dataloader=train_loader,
                val_dataloader=val_loader,
                hierarchical_map=hierarchical_map,  # âœ… ä¼ é€’åˆ†å±‚æ˜ å°„
                device=device,
                output_dir=str(output_dir),
                verbose=True,
            )
            print_success("Trainer åˆå§‹åŒ–å®Œæˆ")
            print(f"  å¤§ç±»æ•°: {len(hierarchical_map)}")
            print(f"  è®¾å¤‡: {trainer.device}")
            print(f"  è¾“å‡ºç›®å½•: {trainer.output_dir}")
        except Exception as e:
            print_error(f"Trainer åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
        
        # æ­¥éª¤ 4: æ‰§è¡Œè®­ç»ƒ
        print("\n  ğŸš€ å¼€å§‹è®­ç»ƒ...")
        try:
            trainer.train(
                num_epochs=epochs,
                learning_rate=lr,
                weight_decay=weight_decay,
                patience=patience,
            )
            print_success("è®­ç»ƒå®Œæˆ")
        except Exception as e:
            print_error(f"è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return False
        
        # æ­¥éª¤ 5: åŠ è½½æœ€ä½³æ¨¡å‹å¹¶è¿›è¡Œæµ‹è¯•
        print("\n  ğŸ§ª åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæµ‹è¯•...")
        try:
            best_model_path = Path(output_dir) / 'best_model.pth'
            if best_model_path.exists():
                trainer.load_checkpoint(str(best_model_path))
                print_success(f"æœ€ä½³æ¨¡å‹å·²åŠ è½½: {best_model_path}")
            else:
                print_warning("æœ€ä½³æ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨å½“å‰æ¨¡å‹è¿›è¡Œæµ‹è¯•")
            
            # æ‰§è¡Œæµ‹è¯•
            test_metrics = trainer.test(test_loader)
            print_success("æµ‹è¯•å®Œæˆ")
            print(f"\n  æµ‹è¯•é›†ç»“æœ:")
            print(f"    å¤§ç±»å‡†ç¡®ç‡: {test_metrics.get('major_accuracy', 0):.4f}")
            print(f"    å°ç±»å‡†ç¡®ç‡: {test_metrics.get('detail_accuracy', 0):.4f}")
            print(f"    å±‚çº§å‡†ç¡®ç‡: {test_metrics.get('hierarchical_accuracy', 0):.4f}")
            print(f"    å¤§ç±»F1-Score: {test_metrics.get('major_f1', 0):.4f}")
            print(f"    å°ç±»F1-Score: {test_metrics.get('detail_f1', 0):.4f}")
        except Exception as e:
            print_warning(f"æµ‹è¯•è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
        
        # æ­¥éª¤ 6: ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š
        print("\n  ğŸ“Š ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š...")
        try:
            report_file = Path(output_dir) / 'training_report.json'
            training_info = {
                'status': 'completed',
                'epochs': epochs,
                'batch_size': batch_size,
                'learning_rate': lr,
                'weight_decay': weight_decay,
                'patience': patience,
                'model_name': 'HierarchicalDualStreamNetwork',
                'device': str(trainer.device),
                'train_samples': len(train_dataset),
                'val_samples': len(val_dataset),
                'test_samples': len(test_dataset),
                'test_metrics': test_metrics if 'test_metrics' in locals() else {},
                'hierarchical_map': hierarchical_map,
            }
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(training_info, f, ensure_ascii=False, indent=2)
            
            print_success(f"è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        except Exception as e:
            print_warning(f"ç”Ÿæˆè®­ç»ƒæŠ¥å‘Šå¤±è´¥: {e}")
        
        print_success("ç¬¬å…­é˜¶æ®µå®Œæˆ")
        return True
        
    except Exception as e:
        print_error(f"ç¬¬å…­é˜¶æ®µå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    setup_logging()
    
    print_header("åœ°ç†ç©ºé—´æ•°æ®å¤„ç†æµæ°´çº¿")
    
    try:
        # =====================================================================
        # ç¬¬ä¸€é˜¶æ®µï¼šé…ç½®ç®¡ç†ä¸åŸºç¡€è®¾æ–½æ­å»º
        # =====================================================================
        config = phase_1_configuration()
        if config is None:
            return 1
        
        # =====================================================================
        # ç¬¬äºŒé˜¶æ®µï¼šæ•°æ®æ¸…æ´—ä¸æ—¶ç©ºç´¢å¼•æ„å»º
        # =====================================================================
        encoder, dynamic_crawler, static_crawler, hierarchical_map, dynamic_channels, static_channels = phase_2_data_ingestion(config)
        
        if encoder is None:
            return 1
        
        # =====================================================================
        # ç¬¬ä¸‰é˜¶æ®µï¼šåœ¨çº¿ç»Ÿè®¡ä¸æ•°æ®å½’ä¸€åŒ–
        # =====================================================================
        if not phase_3_statistical_analysis(config, dynamic_crawler, static_crawler):
            return 1
        
        # =====================================================================
        # ç¬¬å››é˜¶æ®µï¼šè‡ªå®šä¹‰æ—¶ç©ºæ•°æ®é›†æ„å»º
        # =====================================================================
        train_dataset, val_dataset, test_dataset = phase_4_dataset_construction(
            config, encoder, dynamic_crawler, static_crawler
        )
        
        if train_dataset is None:
            return 1
        
        # =====================================================================
        # ç¬¬äº”é˜¶æ®µï¼šæ¨¡å‹æ¶æ„è®¾è®¡
        # =====================================================================
        model = phase_5_model_architecture(
            config, hierarchical_map, dynamic_channels, static_channels
        )
        
        if model is None:
            return 1
        
        # =====================================================================
        # ç¬¬å…­é˜¶æ®µï¼šè®­ç»ƒå¾ªç¯ä¸æ—¥å¿—ç³»ç»Ÿ
        # =====================================================================
        output_dir = config.get_experiment_output_dir()
        success = phase_6_training_loop(
            config, model, train_dataset, val_dataset, test_dataset, output_dir, hierarchical_map
        )
        
        if not success:
            return 1
        
        # =====================================================================
        # å®Œæˆ
        # =====================================================================
        print_header("âœ… å®Œæ•´æµæ°´çº¿æ‰§è¡ŒæˆåŠŸï¼")
        
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶ä½ç½®: {output_dir}")
        print(f"\nğŸ“‹ ç”Ÿæˆæ–‡ä»¶æ¸…å•:")
        print(f"  â”œâ”€â”€ config_used.yaml                # é…ç½®å‰¯æœ¬")
        print(f"  â”œâ”€â”€ data_inventory.csv              # æ•°æ®æ¸…å•")
        print(f"  â”œâ”€â”€ verification_report.json        # éªŒè¯æŠ¥å‘Š")
        print(f"  â”œâ”€â”€ data_summary.txt                # æ‘˜è¦")
        print(f"  â”œâ”€â”€ detailed_labels_map.json        # è¯¦ç»†ç±»åˆ«æ˜ å°„")
        print(f"  â”œâ”€â”€ major_labels_map.json           # å¤§ç±»æ˜ å°„")
        print(f"  â”œâ”€â”€ hierarchical_labels_map.json    # å±‚çº§æ˜ å°„")
        print(f"  â”œâ”€â”€ labels_geodata.geojson          # GeoJSONæ ‡ç­¾")
        print(f"  â”œâ”€â”€ rasters_metadata.json           # æ …æ ¼å…ƒæ•°æ®")
        print(f"  â”œâ”€â”€ rasters_summary.json            # æ …æ ¼æ±‡æ€»")
        print(f"  â”œâ”€â”€ normalization_stats.json        # å½’ä¸€åŒ–å‚æ•°")
        print(f"  â”œâ”€â”€ dataset_info.json               # æ•°æ®é›†å…ƒæ•°æ®")
        print(f"  â”œâ”€â”€ model_architecture.json         # æ¨¡å‹æ¶æ„ä¿¡æ¯")
        print(f"  â”œâ”€â”€ detected_parameters.json        # è‡ªåŠ¨æ£€æµ‹å‚æ•°")
        print(f"  â”œâ”€â”€ training_report.json            # è®­ç»ƒæŠ¥å‘Š")
        print(f"  â”œâ”€â”€ best_model.pth                  # æœ€ä½³æ¨¡å‹æƒé‡")
        print(f"  â”œâ”€â”€ last_model.pth                  # æœ€åä¸€ä¸ªæ¨¡å‹")
        print(f"  â”œâ”€â”€ training_log.txt                # è®­ç»ƒæ—¥å¿—")
        print(f"  â””â”€â”€ metrics_curves.png              # æŒ‡æ ‡æ›²çº¿å›¾")
        
        # ä¿å­˜è‡ªåŠ¨æ£€æµ‹åˆ°çš„å‚æ•°
        print(f"\nğŸ“Š è‡ªåŠ¨æ£€æµ‹å‚æ•°:")
        detected_params = {
            'num_major_classes': len(hierarchical_map),
            'hierarchical_map': hierarchical_map,
            'dynamic_channels': dynamic_channels,
            'static_channels': static_channels,
        }
        print(f"  â”œâ”€â”€ å¤§ç±»æ•°: {len(hierarchical_map)}")
        for major_id, major_info in sorted(hierarchical_map.items()):
            num_detail = len(major_info.get('detail_classes', {}))
            print(f"  â”‚  â”œâ”€â”€ {major_info.get('name', f'Major_{major_id}')}: {num_detail} ä¸ªå°ç±»")
        print(f"  â”œâ”€â”€ åŠ¨æ€å½±åƒæ³¢æ®µæ•°: {dynamic_channels}")
        print(f"  â””â”€â”€ é™æ€å½±åƒæ³¢æ®µæ•°: {static_channels}")
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        detected_file = Path(output_dir) / 'detected_parameters.json'
        with open(detected_file, 'w', encoding='utf-8') as f:
            json.dump(detected_params, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… è‡ªåŠ¨æ£€æµ‹å‚æ•°å·²ä¿å­˜: {detected_file}")
        
        # æ‰“å°æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
        print(f"  â”œâ”€â”€ è®­ç»ƒé›†: {len(train_dataset)} ä¸ªæ ·æœ¬")
        print(f"  â”œâ”€â”€ éªŒè¯é›†: {len(val_dataset)} ä¸ªæ ·æœ¬")
        print(f"  â””â”€â”€ æµ‹è¯•é›†: {len(test_dataset)} ä¸ªæ ·æœ¬")
        
        print("\n" + "=" * 80)
        print("ğŸ‰ å…­é˜¶æ®µå®Œæ•´æµæ°´çº¿æ‰§è¡ŒæˆåŠŸï¼")
        print("=" * 80 + "\n")
        
        return 0
        
    except Exception as e:
        print_error(f"æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    sys.exit(main())
