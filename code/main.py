#!/usr/bin/env python3
"""
main.py: â€œå…ˆå¤§ç±»ï¼Œåå°ç±»â€ åˆ†å±‚è®­ç»ƒæµæ°´çº¿ (é…ç½®åˆ†ç¦»ç‰ˆ)

æ›´æ–°å†…å®¹ï¼š
1. çœŸæ­£ä» config.yaml è¯»å–è®­ç»ƒè¶…å‚æ•°ã€‚
2. æ”¯æŒå¤§ç±» (major_model) å’Œå°ç±» (detail_model) ä½¿ç”¨ä¸åŒçš„è¶…å‚æ•° (epochs, lr, batch_size ç­‰)ã€‚
3. ä¿æŒäº†ä¹‹å‰çš„è‡ªåŠ¨å½’ä¸€åŒ–å’Œå¤šè¿›ç¨‹ DataLoader ä¼˜åŒ–ã€‚
"""

import sys
import json
import logging
import multiprocessing
from pathlib import Path
import torch
from torch.utils.data import DataLoader, Subset
import numpy as np

# å¯¼å…¥æœ¬åœ°æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent))
from config_manager import ConfigManager
from label_encoder import LabelEncoder
from raster_crawler import RasterCrawler
from point_timeseries_dataset import PointTimeSeriesDataset, collate_fn
from model_architecture import DualStreamSpatio_TemporalFusionNetwork
from trainer import Trainer
from stats_calculator import StatsCalculator

def setup_logging():
    logging.basicConfig(level=logging.INFO, format='%(message)s')

def get_subset_indices(dataset, filter_func):
    """
    è¾…åŠ©å‡½æ•°ï¼šéå†æ•°æ®é›†ï¼Œè¿”å›æ»¡è¶³ filter_func æ¡ä»¶çš„å±€éƒ¨ç´¢å¼•åˆ—è¡¨ã€‚
    """
    indices = []
    df = dataset.encoder.get_dataframe()
    for local_idx, global_idx in enumerate(dataset.indices):
        row = df.iloc[global_idx]
        if filter_func(row):
            indices.append(local_idx) 
    return indices

def main():
    setup_logging()
    print("="*60)
    print("ğŸš€ å¯åŠ¨åˆ†å±‚è®­ç»ƒæµæ°´çº¿ (é…ç½®åˆ†ç¦» & è‡ªåŠ¨å‚æ•°ç‰ˆ)")
    print("="*60)

    # 1. åŠ è½½é…ç½®
    config = ConfigManager(str(Path(__file__).parent / 'config.yaml'))
    output_dir = config.get_experiment_output_dir()
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è·å–åˆ†ç¦»çš„è¶…å‚æ•°é…ç½®
    # å¦‚æœé…ç½®æ–‡ä»¶é‡Œæ²¡å†™ï¼Œè¿™é‡Œæä¾›äº†é»˜è®¤çš„ fallback å€¼ï¼Œé˜²æ­¢æŠ¥é”™
    major_cfg = config.get('train.major_model', {
        'epochs': 30, 'batch_size': 32, 'learning_rate': 1e-3, 'weight_decay': 1e-4, 'patience': 10
    })
    detail_cfg = config.get('train.detail_model', {
        'epochs': 40, 'batch_size': 16, 'learning_rate': 1e-3, 'weight_decay': 1e-4, 'patience': 10, 'min_samples': 5
    })
    
    common_cfg = {
        'num_workers': config.get('train.num_workers', min(8, multiprocessing.cpu_count())),
        'pin_memory': True if torch.cuda.is_available() else False
    }

    print("\nğŸ“‹ è®­ç»ƒé…ç½®åŠ è½½:")
    print(f"   [å¤§ç±»å‚æ•°] Epochs: {major_cfg['epochs']}, BS: {major_cfg['batch_size']}, LR: {major_cfg['learning_rate']}")
    print(f"   [å°ç±»å‚æ•°] Epochs: {detail_cfg['epochs']}, BS: {detail_cfg['batch_size']}, LR: {detail_cfg['learning_rate']}")
    print(f"   [ç³»ç»Ÿå‚æ•°] Workers: {common_cfg['num_workers']}, Pin Memory: {common_cfg['pin_memory']}")

    # 2. åˆå§‹åŒ–ç»„ä»¶
    encoder = LabelEncoder(config=config)
    dynamic_crawler = RasterCrawler(config=config, raster_dir=config.get_resolved_path('dynamic_images_dir'), filename_pattern=config.get('data_specs.raster_crawler.filename_pattern'), file_extensions=['.tif'])
    static_crawler = RasterCrawler(config=config, raster_dir=config.get_resolved_path('static_images_dir'), filename_pattern=config.get('data_specs.raster_crawler.filename_pattern'), file_extensions=['.tif'])
    
    # 3. è‡ªåŠ¨å½’ä¸€åŒ–è®¡ç®— (å¦‚æœä¸å­˜åœ¨)
    stats_file = output_dir / 'normalization_stats.json'
    if not stats_file.exists():
        print("\nğŸ“Š æœªæ£€æµ‹åˆ°ç»Ÿè®¡æ–‡ä»¶ï¼Œå¼€å§‹è®¡ç®—å…¨å±€ç»Ÿè®¡é‡...")
        calculator = StatsCalculator(config=config, dynamic_channel_names=None, static_channel_names=None)
        d_rasters = dynamic_crawler.get_all_rasters()
        s_rasters = static_crawler.get_all_rasters()
        calculator.compute_global_stats(dynamic_rasters=d_rasters, static_rasters=s_rasters, sampling_rate=0.2)
        calculator.save_stats('normalization_stats.json')
    else:
        print(f"\nâœ… æ£€æµ‹åˆ°ç»Ÿè®¡æ–‡ä»¶: {stats_file.name}ï¼Œè·³è¿‡è®¡ç®—ã€‚")

    # è·å–é€šé“æ•°
    dyn_ch = dynamic_crawler.detect_num_channels()['most_common']
    sta_ch = static_crawler.detect_num_channels()['most_common']
    
    # 4. åˆå§‹åŒ–å…¨é‡æ•°æ®é›†
    print("\nğŸ“¦ åˆå§‹åŒ–å…¨é‡æ•°æ®é›†...")
    full_train_dataset = PointTimeSeriesDataset(config, encoder, dynamic_crawler, static_crawler, split='train', cache_metadata=True, verbose=False)
    full_val_dataset = PointTimeSeriesDataset(config, encoder, dynamic_crawler, static_crawler, split='val', cache_metadata=True, verbose=False)
    
    major_map = encoder.get_major_labels_map()
    hierarchical_map = encoder.get_hierarchical_map()

    # =========================================================================
    # é˜¶æ®µ A: è®­ç»ƒå¤§ç±»æ¨¡å‹ (ä½¿ç”¨ major_cfg)
    # =========================================================================
    print("\n" + "="*60)
    print("ğŸ—ï¸  [é˜¶æ®µ A] è®­ç»ƒå¤§ç±»åˆ†ç±»æ¨¡å‹ (Major Model)")
    print("="*60)
    
    major_model_dir = output_dir / "major_model"
    major_model = DualStreamSpatio_TemporalFusionNetwork(
        in_channels_dynamic=dyn_ch,
        in_channels_static=sta_ch,
        num_classes=len(major_map)
    )
    
    # æ„é€ å¤§ç±»ä¸“ç”¨çš„ DataLoader
    major_loader_args = {
        'batch_size': major_cfg['batch_size'],
        'num_workers': common_cfg['num_workers'],
        'pin_memory': common_cfg['pin_memory'],
        'collate_fn': collate_fn
    }
    
    major_trainer = Trainer(
        model=major_model,
        train_dataloader=DataLoader(full_train_dataset, shuffle=True, **major_loader_args),
        val_dataloader=DataLoader(full_val_dataset, shuffle=False, **major_loader_args),
        num_classes=len(major_map),
        target_key='major_label', 
        output_dir=major_model_dir
    )
    
    major_trainer.train(
        num_epochs=major_cfg['epochs'],
        lr=major_cfg['learning_rate'],
        weight_decay=major_cfg['weight_decay'],
        patience=major_cfg['patience']
    )
    print(f"âœ… å¤§ç±»æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œä¿å­˜äº: {major_model_dir}")

    # =========================================================================
    # é˜¶æ®µ B: è®­ç»ƒå„ä¸ªå°ç±»æ¨¡å‹ (ä½¿ç”¨ detail_cfg)
    # =========================================================================
    print("\n" + "="*60)
    print("ğŸ—ï¸  [é˜¶æ®µ B] è®­ç»ƒå„åˆ†æ”¯å°ç±»æ¨¡å‹ (Detail Models)")
    print("="*60)

    # æ„é€ å°ç±»ä¸“ç”¨çš„ DataLoader å‚æ•°
    detail_loader_args = {
        'batch_size': detail_cfg['batch_size'], # ä½¿ç”¨å°ç±»é…ç½®çš„ BatchSize
        'num_workers': common_cfg['num_workers'],
        'pin_memory': common_cfg['pin_memory'],
        'collate_fn': collate_fn
    }

    for major_name, major_id in major_map.items():
        print(f"\nğŸ‘‰ æ­£åœ¨å¤„ç†å¤§ç±»: {major_name} (ID: {major_id})")
        
        # è·å–å°ç±»ä¿¡æ¯
        sub_info = hierarchical_map[major_name]
        detail_classes_map = sub_info['detail_classes']
        num_sub_classes = len(detail_classes_map)
        
        if num_sub_classes <= 1:
            print(f"   âš ï¸ è¯¥å¤§ç±»ä»…æœ‰ {num_sub_classes} ä¸ªå°ç±»ï¼Œè·³è¿‡è®­ç»ƒã€‚")
            continue
            
        print(f"   åŒ…å«å°ç±»: {list(detail_classes_map.keys())} (å…± {num_sub_classes} ä¸ª)")

        # æ„å»ºæ˜ å°„
        sorted_details = sorted(detail_classes_map.items(), key=lambda x: x[1])
        global_to_local = {gid: lidx for lidx, (_, gid) in enumerate(sorted_details)}
        local_to_global = {lidx: gid for lidx, (_, gid) in enumerate(sorted_details)}
            
        # ç­›é€‰æ•°æ®å­é›†
        train_indices = get_subset_indices(full_train_dataset, lambda row: row['major_label'] == major_id)
        val_indices = get_subset_indices(full_val_dataset, lambda row: row['major_label'] == major_id)
        
        print(f"   æ ·æœ¬æ•°é‡: è®­ç»ƒé›† {len(train_indices)} | éªŒè¯é›† {len(val_indices)}")
        
        # ä½¿ç”¨é…ç½®ä¸­çš„æœ€å°æ ·æœ¬æ•°é™åˆ¶
        min_samples = detail_cfg.get('min_samples', 5)
        if len(train_indices) < min_samples:
            print(f"   âš ï¸ æ ·æœ¬è¿‡å°‘ (<{min_samples})ï¼Œè·³è¿‡è®­ç»ƒã€‚")
            continue

        train_subset = Subset(full_train_dataset, train_indices)
        val_subset = Subset(full_val_dataset, val_indices)
        
        # åˆå§‹åŒ–å­æ¨¡å‹
        sub_model_dir = output_dir / f"detail_model_{major_id}_{major_name}"
        sub_model = DualStreamSpatio_TemporalFusionNetwork(
            in_channels_dynamic=dyn_ch,
            in_channels_static=sta_ch,
            num_classes=num_sub_classes
        )
        
        # è®­ç»ƒå­æ¨¡å‹
        sub_trainer = Trainer(
            model=sub_model,
            train_dataloader=DataLoader(train_subset, shuffle=True, **detail_loader_args),
            val_dataloader=DataLoader(val_subset, shuffle=False, **detail_loader_args),
            num_classes=num_sub_classes,
            target_key='detail_label',
            label_mapping=global_to_local,
            output_dir=sub_model_dir
        )
        
        sub_trainer.train(
            num_epochs=detail_cfg['epochs'],
            lr=detail_cfg['learning_rate'],
            weight_decay=detail_cfg['weight_decay'],
            patience=detail_cfg['patience']
        )
        
        # ä¿å­˜æ˜ å°„
        mapping_info = {
            'major_class': major_name,
            'major_id': major_id,
            'local_to_global_map': local_to_global, 
            'global_to_local_map': global_to_local
        }
        with open(sub_model_dir / 'class_mapping.json', 'w', encoding='utf-8') as f:
            json.dump(mapping_info, f, ensure_ascii=False, indent=2)
            
        print(f"   âœ… {major_name} å°ç±»æ¨¡å‹è®­ç»ƒå®Œæˆã€‚")

    print("\n" + "="*60)
    print("ğŸ‰ æ‰€æœ‰æ¨¡å‹è®­ç»ƒç»“æŸï¼")
    print("="*60)

if __name__ == '__main__':
    try:
        multiprocessing.set_start_method('spawn', force=True)
    except RuntimeError:
        pass
    main()