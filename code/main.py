#!/usr/bin/env python3
"""
main.py: â€œå…ˆå¤§ç±»ï¼Œåå°ç±»â€ åˆ†å±‚è®­ç»ƒæµæ°´çº¿

é€»è¾‘æµç¨‹ï¼š
1. å‡†å¤‡å…¨é‡æ•°æ®ã€‚
2. ã€é˜¶æ®µAã€‘è®­ç»ƒâ€œå¤§ç±»åˆ†ç±»å™¨â€ (Major Class Model)
   - æ•°æ®ï¼šå…¨é‡æ•°æ®
   - æ ‡ç­¾ï¼šmajor_label
   - è¾“å‡ºï¼šmajor_model.pth
3. ã€é˜¶æ®µBã€‘è®­ç»ƒå¤šä¸ªâ€œå°ç±»åˆ†ç±»å™¨â€ (Detail Class Models)
   - å¾ªç¯éå†æ¯ä¸ªå¤§ç±» ID
   - ç­›é€‰æ•°æ®ï¼šä»…ä¿ç•™å±äºå½“å‰å¤§ç±»çš„æ ·æœ¬
   - æ ‡ç­¾ï¼šdetail_label (éœ€é‡æ˜ å°„ä¸º 0~N)
   - è¾“å‡ºï¼šdetail_model_major_{id}.pth
"""

import sys
import json
import logging
from pathlib import Path
import torch
from torch.utils.data import DataLoader, Subset
import numpy as np

# å¯¼å…¥æœ¬åœ°æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent))
from config_manager import ConfigManager
from label_encoder import LabelEncoder
from raster_crawler import RasterCrawler
from point_timeseries_dataset import PointTimeSeriesDataset, collate_fn
from model_architecture import DualStreamSpatio_TemporalFusionNetwork
from trainer import Trainer

def setup_logging():
    logging.basicConfig(level=logging.INFO, format='%(message)s')

def get_subset_indices(dataset, filter_func):
    """
    è¾…åŠ©å‡½æ•°ï¼šéå†æ•°æ®é›†ï¼Œè¿”å›æ»¡è¶³ filter_func æ¡ä»¶çš„å±€éƒ¨ç´¢å¼•åˆ—è¡¨ã€‚
    
    ä¿®æ­£è¯´æ˜ï¼š
    ä¹‹å‰ç›´æ¥éå† df è¿”å›çš„æ˜¯å…¨å±€ç´¢å¼•ï¼Œä¼šå¯¼è‡´ Subset è¶Šç•Œã€‚
    ç°åœ¨éå† dataset.indices (å½“å‰ split çš„å…¨å±€ç´¢å¼•åˆ—è¡¨)ï¼Œå¹¶è¿”å› local_idx (æšä¸¾ç´¢å¼•)ã€‚
    """
    indices = []
    # print("  ğŸ” æ­£åœ¨ç­›é€‰æ•°æ®å­é›†...") # å‡å°‘åˆ·å±
    
    # è·å–åŸå§‹çš„å®Œæ•´ DataFrame
    df = dataset.encoder.get_dataframe()
    
    # dataset.indices å­˜å‚¨äº†å½“å‰ split (å¦‚è®­ç»ƒé›†) å¯¹åº”åœ¨ DataFrame ä¸­çš„å…¨å±€è¡Œå·
    # æˆ‘ä»¬éœ€è¦è¿”å› dataset å†…éƒ¨çš„å±€éƒ¨ç´¢å¼• (0 ~ len(dataset)-1)
    # enumerate çš„ local_idx å°±æ˜¯æˆ‘ä»¬è¦ä¼ ç»™ Subset çš„ç´¢å¼•
    for local_idx, global_idx in enumerate(dataset.indices):
        # ä½¿ç”¨ iloc é€šè¿‡è¡Œå·è®¿é—®åŸå§‹æ•°æ®
        row = df.iloc[global_idx]
        if filter_func(row):
            indices.append(local_idx) # æ³¨æ„ï¼šè¿™é‡Œå¿…é¡»å­˜ local_idx
            
    return indices

def main():
    setup_logging()
    print("="*60)
    print("ğŸš€ å¯åŠ¨åˆ†å±‚è®­ç»ƒæµæ°´çº¿ (Coarse-to-Fine)")
    print("="*60)

    # 1. é…ç½®ä¸æ•°æ®å‡†å¤‡
    config = ConfigManager(str(Path(__file__).parent / 'config.yaml'))
    output_dir = config.get_experiment_output_dir()
    
    # åˆå§‹åŒ–ç»„ä»¶
    encoder = LabelEncoder(config=config)
    dynamic_crawler = RasterCrawler(config=config, raster_dir=config.get_resolved_path('dynamic_images_dir'), filename_pattern=config.get('data_specs.raster_crawler.filename_pattern'), file_extensions=['.tif'])
    static_crawler = RasterCrawler(config=config, raster_dir=config.get_resolved_path('static_images_dir'), filename_pattern=config.get('data_specs.raster_crawler.filename_pattern'), file_extensions=['.tif'])
    
    # è·å–é€šé“æ•°
    dyn_ch = dynamic_crawler.detect_num_channels()['most_common']
    sta_ch = static_crawler.detect_num_channels()['most_common']
    
    # åˆå§‹åŒ–å…¨é‡æ•°æ®é›†
    print("\nğŸ“¦ åˆå§‹åŒ–å…¨é‡æ•°æ®é›†...")
    full_train_dataset = PointTimeSeriesDataset(config, encoder, dynamic_crawler, static_crawler, split='train', cache_metadata=True, verbose=False)
    full_val_dataset = PointTimeSeriesDataset(config, encoder, dynamic_crawler, static_crawler, split='val', cache_metadata=True, verbose=False)
    
    major_map = encoder.get_major_labels_map()
    hierarchical_map = encoder.get_hierarchical_map()

    # =========================================================================
    # é˜¶æ®µ A: è®­ç»ƒå¤§ç±»æ¨¡å‹ (Major Model)
    # =========================================================================
    print("\n" + "="*60)
    print("ğŸ—ï¸  [é˜¶æ®µ A] è®­ç»ƒå¤§ç±»åˆ†ç±»æ¨¡å‹ (Major Model)")
    print("="*60)
    
    major_model_dir = output_dir / "major_model"
    major_model = DualStreamSpatio_TemporalFusionNetwork(
        in_channels_dynamic=dyn_ch,
        in_channels_static=sta_ch,
        num_classes=len(major_map) # è¾“å‡ºèŠ‚ç‚¹æ•° = å¤§ç±»æ•°
    )
    
    major_trainer = Trainer(
        model=major_model,
        train_dataloader=DataLoader(full_train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn),
        val_dataloader=DataLoader(full_val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn),
        num_classes=len(major_map),
        target_key='major_label', # å‘Šè¯‰ Trainer å– batch['major_label']
        output_dir=major_model_dir
    )
    
    # ä½ å¯ä»¥æ ¹æ®éœ€è¦å–æ¶ˆæ³¨é‡Šè¿™ä¸€è¡Œæ¥è·³è¿‡å¤§ç±»è®­ç»ƒï¼ˆå¦‚æœå·²ç»è®­ç»ƒå¥½äº†ï¼‰
    major_trainer.train(num_epochs=30) 
    print(f"âœ… å¤§ç±»æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œä¿å­˜äº: {major_model_dir}")

    # =========================================================================
    # é˜¶æ®µ B: è®­ç»ƒå„ä¸ªå°ç±»æ¨¡å‹ (Detail Models)
    # =========================================================================
    print("\n" + "="*60)
    print("ğŸ—ï¸  [é˜¶æ®µ B] è®­ç»ƒå„åˆ†æ”¯å°ç±»æ¨¡å‹ (Detail Models)")
    print("="*60)

    for major_name, major_id in major_map.items():
        print(f"\nğŸ‘‰ æ­£åœ¨å¤„ç†å¤§ç±»: {major_name} (ID: {major_id})")
        
        # 1. è·å–è¯¥å¤§ç±»ä¸‹çš„å°ç±»ä¿¡æ¯
        sub_info = hierarchical_map[major_name]
        detail_classes_map = sub_info['detail_classes'] # {å°ç±»å: å…¨å±€ID}
        num_sub_classes = len(detail_classes_map)
        
        if num_sub_classes <= 1:
            print(f"   âš ï¸ è¯¥å¤§ç±»ä»…æœ‰ {num_sub_classes} ä¸ªå°ç±»ï¼Œè·³è¿‡è®­ç»ƒã€‚")
            continue
            
        print(f"   åŒ…å«å°ç±»: {list(detail_classes_map.keys())} (å…± {num_sub_classes} ä¸ª)")

        # 2. æ„å»ºæœ¬åœ°æ˜ å°„ (Local ID Mapping)
        sorted_details = sorted(detail_classes_map.items(), key=lambda x: x[1]) # æŒ‰å…¨å±€IDæ’åº
        
        global_to_local = {}
        local_to_global = {}
        for local_idx, (d_name, global_id) in enumerate(sorted_details):
            global_to_local[global_id] = local_idx
            local_to_global[local_idx] = global_id
            
        # 3. ç­›é€‰æ•°æ®å­é›† (Subset)
        print("   ğŸ” æ­£åœ¨ç­›é€‰æ•°æ®å­é›†...")
        # æ³¨æ„ï¼šè¿™é‡Œè°ƒç”¨çš„æ˜¯ä¿®å¤åçš„ get_subset_indices
        train_indices = get_subset_indices(full_train_dataset, lambda row: row['major_label'] == major_id)
        val_indices = get_subset_indices(full_val_dataset, lambda row: row['major_label'] == major_id)
        
        print(f"   æ ·æœ¬æ•°é‡: è®­ç»ƒé›† {len(train_indices)} | éªŒè¯é›† {len(val_indices)}")
        
        if len(train_indices) < 5:
            print("   âš ï¸ æ ·æœ¬è¿‡å°‘ï¼Œè·³è¿‡è®­ç»ƒã€‚")
            continue

        train_subset = Subset(full_train_dataset, train_indices)
        val_subset = Subset(full_val_dataset, val_indices)
        
        # 4. åˆå§‹åŒ–å­æ¨¡å‹
        sub_model_dir = output_dir / f"detail_model_{major_id}_{major_name}"
        sub_model = DualStreamSpatio_TemporalFusionNetwork(
            in_channels_dynamic=dyn_ch,
            in_channels_static=sta_ch,
            num_classes=num_sub_classes # è¾“å‡ºèŠ‚ç‚¹æ•° = æœ¬åœ°å°ç±»æ•°
        )
        
        # 5. è®­ç»ƒå­æ¨¡å‹
        sub_trainer = Trainer(
            model=sub_model,
            train_dataloader=DataLoader(train_subset, batch_size=16, shuffle=True, collate_fn=collate_fn), # å­é›†å¯èƒ½è¾ƒå°ï¼ŒBatchSizeå‡å°
            val_dataloader=DataLoader(val_subset, batch_size=16, shuffle=False, collate_fn=collate_fn),
            num_classes=num_sub_classes,
            target_key='detail_label', # å–å°ç±»æ ‡ç­¾
            label_mapping=global_to_local, # ä¼ å…¥æ˜ å°„è¡¨ï¼ŒTrainerä¼šè‡ªåŠ¨å°†å…¨å±€IDè½¬ä¸ºæœ¬åœ°0~N
            output_dir=sub_model_dir
        )
        
        sub_trainer.train(num_epochs=40)
        
        # 6. ä¿å­˜å­æ¨¡å‹çš„æ˜ å°„å…³ç³»ï¼Œä»¥ä¾¿æ¨ç†æ—¶ä½¿ç”¨
        mapping_info = {
            'major_class': major_name,
            'major_id': major_id,
            'local_to_global_map': local_to_global, # æ¨ç†è¾“å‡º 0 -> å¯¹åº”çš„å…¨å±€ID
            'global_to_local_map': global_to_local
        }
        with open(sub_model_dir / 'class_mapping.json', 'w', encoding='utf-8') as f:
            json.dump(mapping_info, f, ensure_ascii=False, indent=2)
            
        print(f"   âœ… {major_name} å°ç±»æ¨¡å‹è®­ç»ƒå®Œæˆã€‚")

    print("\n" + "="*60)
    print("ğŸ‰ æ‰€æœ‰æ¨¡å‹è®­ç»ƒç»“æŸï¼")
    print("="*60)

if __name__ == '__main__':
    main()