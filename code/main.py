#!/usr/bin/env python3
"""
main.py: â€œå…ˆå¤§ç±»ï¼Œåå°ç±»â€ åˆ†å±‚è®­ç»ƒæµæ°´çº¿ (ä¿®å¤é€šé“æ£€æµ‹ç‰ˆ)
"""

import sys
import json
import logging
import multiprocessing
from pathlib import Path
import torch
from torch.utils.data import DataLoader, Subset
import numpy as np

# å¯¼å…¥æœ¬åœ°æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent))
from config_manager import ConfigManager
from label_encoder import LabelEncoder
from raster_crawler import RasterCrawler
from point_timeseries_dataset import PointTimeSeriesDataset, collate_fn
from model_architecture import DualStreamSpatio_TemporalFusionNetwork
from trainer import Trainer
from stats_calculator import StatsCalculator

def setup_logging():
    logging.basicConfig(level=logging.INFO, format='%(message)s')

def get_subset_indices(dataset, filter_func):
    """è¾…åŠ©å‡½æ•°ï¼šéå†æ•°æ®é›†ï¼Œè¿”å›æ»¡è¶³æ¡ä»¶çš„å±€éƒ¨ç´¢å¼•åˆ—è¡¨"""
    indices = []
    df = dataset.points_df 
    for local_idx, global_idx in enumerate(dataset.indices):
        row = df.iloc[global_idx]
        if filter_func(row):
            indices.append(local_idx) 
    return indices

def main():
    setup_logging()
    print("="*60)
    print("ğŸš€ å¯åŠ¨åˆ†å±‚è®­ç»ƒæµæ°´çº¿ (Auto-Channel Detect)")
    print("="*60)

    # 1. åŠ è½½é…ç½®
    config = ConfigManager(str(Path(__file__).parent / 'config.yaml'))
    output_dir = config.get_experiment_output_dir()
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è·å–è¶…å‚æ•°
    major_cfg = config.get('train.major_model', {
        'epochs': 30, 'batch_size': 32, 'learning_rate': 1e-3, 'weight_decay': 1e-4, 'patience': 10
    })
    detail_cfg = config.get('train.detail_model', {
        'epochs': 40, 'batch_size': 16, 'learning_rate': 1e-3, 'weight_decay': 1e-4, 'patience': 10, 'min_samples': 5
    })
    common_cfg = {
        'num_workers': config.get('train.num_workers', 0),
        'pin_memory': True if torch.cuda.is_available() else False
    }

    # 2. åˆå§‹åŒ–ç»„ä»¶
    encoder = LabelEncoder(config=config)
    
    # 3. è‡ªåŠ¨å½’ä¸€åŒ–è®¡ç®— (ä»…å½“æ–‡ä»¶ä¸å­˜åœ¨æ—¶)
    stats_file = output_dir / 'normalization_stats.json'
    if not stats_file.exists():
        # æ£€æŸ¥æ˜¯å¦åœ¨ä¹‹å‰çš„è¿è¡Œç›®å½•ä¸­æœ‰ï¼ˆå¯é€‰ä¼˜åŒ–ï¼Œè¿™é‡Œç›´æ¥ä»tiffè®¡ç®—æ›´ç¨³ï¼‰
        print("\nğŸ“Š æ­£åœ¨è®¡ç®—å…¨å±€ç»Ÿè®¡é‡ (åŠ¨æ€+é™æ€)...")
        dyn_crawler = RasterCrawler(config=config)
        static_crawler = RasterCrawler(
            config=config, 
            raster_dir=config.get_resolved_path('static_images_dir'),
            filename_pattern='.*',
            file_extensions=('.tif', '.tiff')
        )
        calculator = StatsCalculator(config=config)
        calculator.compute_all_stats(dyn_crawler, static_crawler, sampling_rate=0.2) 
        print("âœ… ç»Ÿè®¡é‡è®¡ç®—å®Œæˆã€‚")
        del dyn_crawler, static_crawler, calculator

    # 4. åŠ è½½æ•°æ®é›†
    print("\nğŸ“¦ åŠ è½½é¢„å¤„ç†æ•°æ®é›†...")
    try:
        full_train_dataset = PointTimeSeriesDataset(config, encoder, split='train')
        full_val_dataset = PointTimeSeriesDataset(config, encoder, split='val')
    except FileNotFoundError as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ: python code/preprocess_dataset.py")
        sys.exit(1)
    
    # 5. [å…³é”®ä¿®æ”¹] ç›´æ¥ä»æ•°æ®é›†è·å–é€šé“å‚æ•°ï¼Œä¸å†ä¾èµ–å¯èƒ½ä¸¢å¤±çš„jsonæ–‡ä»¶
    dyn_ch = full_train_dataset.num_channels
    sta_ch = full_train_dataset.num_static_channels
    
    print(f"   åŠ¨æ€é€šé“æ•°: {dyn_ch}")
    print(f"   é™æ€é€šé“æ•°: {sta_ch}")
    
    if sta_ch == 0:
        print("âš ï¸ è­¦å‘Šï¼šæ£€æµ‹åˆ°é™æ€é€šé“æ•°ä¸º 0ï¼Œè¯·æ£€æŸ¥ preprocess_dataset.py æ˜¯å¦æ­£ç¡®è¯»å–äº†é™æ€æ•°æ®ã€‚")
        # å¦‚æœç¡®å®æ˜¯0ï¼Œä¸ºäº†é˜²æ­¢æ¨¡å‹æŠ¥é”™ï¼Œå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œä½†è¿™é‡Œå…ˆè®©å®ƒè·‘ï¼Œçœ‹æ˜¯å¦æŠ¥é”™

    major_map = encoder.get_major_labels_map()
    hierarchical_map = encoder.get_hierarchical_map()

    # =========================================================================
    # é˜¶æ®µ A: è®­ç»ƒå¤§ç±»æ¨¡å‹ (Major Model)
    # =========================================================================
    print("\n" + "="*60)
    print("ğŸ—ï¸  [é˜¶æ®µ A] è®­ç»ƒå¤§ç±»åˆ†ç±»æ¨¡å‹")
    print("="*60)
    
    major_model_dir = output_dir / "major_model"
    major_model = DualStreamSpatio_TemporalFusionNetwork(
        in_channels_dynamic=dyn_ch,
        in_channels_static=sta_ch,
        num_classes=len(major_map)
    )
    
    major_trainer = Trainer(
        model=major_model,
        train_dataloader=DataLoader(full_train_dataset, shuffle=True, batch_size=major_cfg['batch_size'], collate_fn=collate_fn, **common_cfg),
        val_dataloader=DataLoader(full_val_dataset, shuffle=False, batch_size=major_cfg['batch_size'], collate_fn=collate_fn, **common_cfg),
        num_classes=len(major_map),
        target_key='major_label',
        output_dir=major_model_dir
    )
    
    major_trainer.train(
        num_epochs=major_cfg['epochs'],
        learning_rate=major_cfg['learning_rate'],
        weight_decay=major_cfg['weight_decay'],
        patience=major_cfg['patience']
    )
    print(f"âœ… å¤§ç±»æ¨¡å‹ä¿å­˜äº: {major_model_dir}")

    # =========================================================================
    # é˜¶æ®µ B: è®­ç»ƒå°ç±»æ¨¡å‹ (Detail Models)
    # =========================================================================
    print("\n" + "="*60)
    print("ğŸ—ï¸  [é˜¶æ®µ B] è®­ç»ƒå„åˆ†æ”¯å°ç±»æ¨¡å‹")
    print("="*60)

    for major_name, major_id in major_map.items():
        print(f"\nğŸ‘‰ å¤„ç†å¤§ç±»: {major_name} (ID: {major_id})")
        
        sub_info = hierarchical_map[major_name]
        detail_classes_map = sub_info['detail_classes']
        num_sub_classes = len(detail_classes_map)
        
        if num_sub_classes <= 1:
            print(f"   âš ï¸ è¯¥å¤§ç±»ä»…æœ‰ {num_sub_classes} ä¸ªå°ç±»ï¼Œè·³è¿‡ã€‚")
            continue

        sorted_details = sorted(detail_classes_map.items(), key=lambda x: x[1])
        global_to_local = {gid: lidx for lidx, (_, gid) in enumerate(sorted_details)}
        local_to_global = {lidx: gid for lidx, (_, gid) in enumerate(sorted_details)}
            
        train_indices = get_subset_indices(full_train_dataset, lambda row: row['major_label'] == major_id)
        val_indices = get_subset_indices(full_val_dataset, lambda row: row['major_label'] == major_id)
        
        print(f"   æ ·æœ¬æ•°: Train {len(train_indices)} | Val {len(val_indices)}")
        
        if len(train_indices) < detail_cfg.get('min_samples', 5):
            print("   âš ï¸ æ ·æœ¬ä¸è¶³ï¼Œè·³è¿‡ã€‚")
            continue

        train_subset = Subset(full_train_dataset, train_indices)
        val_subset = Subset(full_val_dataset, val_indices)
        
        sub_model_dir = output_dir / f"detail_model_{major_id}_{major_name}"
        sub_model = DualStreamSpatio_TemporalFusionNetwork(
            in_channels_dynamic=dyn_ch,
            in_channels_static=sta_ch,
            num_classes=num_sub_classes
        )
        
        sub_trainer = Trainer(
            model=sub_model,
            train_dataloader=DataLoader(train_subset, shuffle=True, batch_size=detail_cfg['batch_size'], collate_fn=collate_fn, **common_cfg),
            val_dataloader=DataLoader(val_subset, shuffle=False, batch_size=detail_cfg['batch_size'], collate_fn=collate_fn, **common_cfg),
            num_classes=num_sub_classes,
            target_key='detail_label',
            label_mapping=global_to_local,
            output_dir=sub_model_dir
        )
        
        sub_trainer.train(
            num_epochs=detail_cfg['epochs'],
            learning_rate=detail_cfg['learning_rate'],
            weight_decay=detail_cfg['weight_decay'],
            patience=detail_cfg['patience']
        )
        
        with open(sub_model_dir / 'class_mapping.json', 'w', encoding='utf-8') as f:
            json.dump({
                'major_class': major_name,
                'major_id': major_id,
                'local_to_global_map': local_to_global, 
                'global_to_local_map': global_to_local
            }, f, ensure_ascii=False, indent=2)
            
        print(f"   âœ… {major_name} å°ç±»æ¨¡å‹å®Œæˆã€‚")

    print("\n" + "="*60)
    print("ğŸ‰ è®­ç»ƒæµæ°´çº¿ç»“æŸï¼")
    print("="*60)

if __name__ == '__main__':
    try:
        multiprocessing.set_start_method('spawn', force=True)
    except RuntimeError:
        pass
    main()